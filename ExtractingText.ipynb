{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:15.252558Z",
     "start_time": "2021-02-02T22:21:15.250748Z"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:15.694193Z",
     "start_time": "2021-02-02T22:21:15.683759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } p, ul {max-width:␣\n",
       ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
       ",→display: flex; justify-content: center;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Markdown as md\n",
    "display(HTML(\"\"\"<style>.container { width:80% !important; } p, ul {max-width:␣\n",
    ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
    ",→display: flex; justify-content: center;}</style>\"\"\"))\n",
    "import pandas     as pd\n",
    "import numpy      as np\n",
    "import re\n",
    "from   pyocr      import pyocr\n",
    "from   pyocr      import builders\n",
    "import io\n",
    "import codecs\n",
    "import os\n",
    "import shutil\n",
    "from   PIL        import Image as PI\n",
    "from   wand.image import Image\n",
    "from   difflib    import SequenceMatcher  \n",
    "from   bs4        import BeautifulSoup as bs\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlock and remove all temp magick files\n",
    "def removeMagickFiles():\n",
    "    \n",
    "    # This doesn't work because imageMagick doesn't release the lock on its temp files \n",
    "    \n",
    "    # Get files from temp folder\n",
    "    tempFolder = tempfile.gettempdir()\n",
    "    files      = getMagickFiles(tempFolder)\n",
    "\n",
    "    # Iterate through them \n",
    "    for (path, filename) in files:\n",
    "\n",
    "        # Get extact file path\n",
    "        file = os.path.join(path, filename)\n",
    "\n",
    "        # Delete it\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all temp magickfiles\n",
    "def getMagickFiles(tempFolder):\n",
    "\n",
    "    # This doesn't work because imageMagick doesn't release the lock on its temp files \n",
    "        \n",
    "    # Create empty List\n",
    "    files = list()\n",
    "    \n",
    "    # Iterate through temp folder and sub folders\n",
    "    for (dirpath, dirnames, filenames) in os.walk(tempFolder):\n",
    "        \n",
    "        # Add magickFiles to filelist\n",
    "        files += [(dirpath, file) for file in filenames if file.startswith('magick')]\n",
    "\n",
    "    # Return the list\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T03:50:37.721148Z",
     "start_time": "2021-01-26T03:50:37.714533Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertPDFtoTextArray(folder, file):\n",
    "    \"\"\"\n",
    "    Takes a PDF document, converts to an image and \n",
    "    then extracts text from each page of that image into an array of UTF-8 encoded text.\n",
    "\n",
    "    \"\"\"\n",
    "    pdf_as_image = Image(filename=folder+'/'+file, resolution=600)\n",
    "    pdf_as_jpeg  = pdf_as_image.convert('jpeg')\n",
    "    del pdf_as_image\n",
    "    \n",
    "    tool         = pyocr.get_available_tools()[0]\n",
    "    pages_text   = []\n",
    "    \n",
    "    for img in pdf_as_jpeg.sequence:\n",
    "        img_page  = Image(image=img)\n",
    "        req_image = img_page.make_blob('jpeg')\n",
    "        txt       = tool.image_to_string(\n",
    "            \n",
    "            PI.open(io.BytesIO(req_image)),\n",
    "            builder = pyocr.builders.TextBuilder()\n",
    "            \n",
    "        )\n",
    "        pages_text.append(txt.encode('utf-8', 'ignore'))\n",
    "        \n",
    "    filename = re.sub('.pdf','', file)\n",
    "    np.save(folder+'/'+filename, pages_text)\n",
    "    \n",
    "    removeMagickFiles \n",
    "    \n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T03:50:38.933506Z",
     "start_time": "2021-01-26T03:50:38.929331Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertFolderofPDFstotextArrays(folder):\n",
    "    filelist = os.listdir(folder)\n",
    "    filelist = [x for x in filelist if x.endswith('pdf')]\n",
    "    numFiles = len(filelist)\n",
    "    for i, file in enumerate(filelist):\n",
    "        print('Converting file',i+1, 'of', numFiles)\n",
    "        convertPDFtoTextArray(folder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:53.929587Z",
     "start_time": "2021-02-02T22:21:53.916276Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes a array of text strings (pages) and cleans docuemnt of headers and footers then cleans up flattened text (i.e. removes ) \n",
    "def cleanAndSaveText(folder, \n",
    "                     file, \n",
    "                     footkeeplist=[], \n",
    "                     footdroplist=[], \n",
    "                     headkeeplist=[], \n",
    "                     headdroplist=[],\n",
    "                     footthreshold=3, \n",
    "                     headthreshold=3, \n",
    "                     hiddenFooters=True, \n",
    "                     hiddenHeaders=True):\n",
    "    \n",
    "    data     = convertTextArrayToText(folder, file, footkeeplist,footdroplist, headkeeplist, headdroplist,\n",
    "                                  footthreshold, headthreshold, hiddenFooters, hiddenHeaders)\n",
    "    txtfile  = re.sub('npy', 'txt', file)\n",
    "    f        = io.open(folder+'/'+txtfile, encoding='utf-8', mode='w')\n",
    "    doc_text = ' '.join(data) \n",
    "\n",
    "    #remove breaks from words that wrap over two lines\n",
    "    doc_text = re.sub('-\\n', '', doc_text) \n",
    "\n",
    "    #replace single returns with spaces\n",
    "    doc_text = re.sub('\\n', ' ', doc_text) \n",
    "\n",
    "    #replace double spaces with single spaces\n",
    "    doc_text = re.sub('  ', ' ', doc_text) \n",
    "\n",
    "    f.write(doc_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:59.807045Z",
     "start_time": "2021-02-02T22:21:59.795867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use for full process from PDF doc to text file \n",
    "def convertOnePDFtoText(folder, \n",
    "                        file, \n",
    "                        footkeeplist=[], \n",
    "                        headkeeplist=[], \n",
    "                        footthreshold=3, \n",
    "                        headthreshold=3, \n",
    "                        hiddenFooters=True, \n",
    "                        hiddenHeaders=True):\n",
    "    \n",
    "    pages_of_text = convertPDFtoTextArray(folder, file)\n",
    "    pages_of_text = decodeText(pages_of_text)\n",
    "    pages_of_text = removeFooters(pages_of_text, footkeeplist, footthreshold, hiddenFooters)\n",
    "    pages_of_text = removeHeaders(pages_of_text, headkeeplist, headthreshold, hiddenHeaders)\n",
    "    return pages_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:02.979193Z",
     "start_time": "2021-02-02T22:22:02.973736Z"
    }
   },
   "outputs": [],
   "source": [
    "# use for turning an already prcessed PDF (i.e. converted to npy file) into text file without headers or footers\n",
    "def convertTextArrayToText(folder, \n",
    "                           file, \n",
    "                           footkeeplist=[], \n",
    "                           footdroplist=[], \n",
    "                           headkeeplist=[],\n",
    "                           headdroplist=[], \n",
    "                           footthreshold=3, \n",
    "                           headthreshold=3, \n",
    "                           hiddenFooters=True, \n",
    "                           hiddenHeaders=True):\n",
    "    \n",
    "    pages_of_text = np.load(folder+'/'+file)\n",
    "    pages_of_text = decodeText(pages_of_text)\n",
    "    pages_of_text = removeFooters(pages_of_text, footkeeplist, footdroplist, footthreshold, hiddenFooters)\n",
    "    pages_of_text = removeHeaders(pages_of_text, headkeeplist, headdroplist, headthreshold, hiddenHeaders)\n",
    "    return pages_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:03.948227Z",
     "start_time": "2021-02-02T22:22:03.945233Z"
    }
   },
   "outputs": [],
   "source": [
    "def decodeText(encoded_text):\n",
    "    decoded_text = [page.decode('utf-8') for page in encoded_text]\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:04.698421Z",
     "start_time": "2021-02-02T22:22:04.685277Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeFooters(text_with_footers, keeplist=[], droplist=[], thresh=2.9, hidden=True):\n",
    "    possible_footers = findFooters(text_with_footers)\n",
    "    footer_scores    = scoreHeadersOrFooters (possible_footers)\n",
    "    probableFooters  = sortScores(footer_scores, threshold=thresh, type='footers')\n",
    "    new_text         = text_with_footers.copy()\n",
    "\n",
    "    set_of_footers   = set([x[0][0] for x in probableFooters])\n",
    "    set_of_footers   = [x for x in set_of_footers if x not in keeplist]\n",
    "    set_of_footers   = list(set(set_of_footers + droplist))\n",
    "    new_text         = deleteFooters(new_text, probableFooters)\n",
    "    \n",
    "    if hidden ==True:\n",
    "        new_text   = removeHeadersOrFootersHiddeninText (new_text,set_of_footers)    \n",
    "    \n",
    "    return new_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:05.385316Z",
     "start_time": "2021-02-02T22:22:05.365257Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findFooters(pages):\n",
    "    footer_candidates=[]\n",
    "    for num, page in enumerate(pages):\n",
    "        \n",
    "        lines = []\n",
    "        start = -1\n",
    "        end   = -1\n",
    "        doublereturn = False\n",
    "        \n",
    "        for i in range (0,5):\n",
    "\n",
    "            while end == start:\n",
    "                start = page.rfind('\\n', max(0,end-200), end)\n",
    "                if start == -1:\n",
    "                    break\n",
    "                elif start == end-1:\n",
    "                    end          = start\n",
    "                    doublereturn = True\n",
    "                    \n",
    "            if start == -1:\n",
    "                line = ''\n",
    "                \n",
    "            if end == -1:\n",
    "                line = re.sub('\\d', '@', page[start+1:])\n",
    "            elif (doublereturn):\n",
    "                line = re.sub('\\d', '@', page[start+1:end])\n",
    "            else: line = re.sub('\\d', '@', page[start+1:end])\n",
    "\n",
    "            lines.append([line, start, end])\n",
    "            end = start\n",
    "\n",
    "        footer_candidates.append(lines)\n",
    "\n",
    "    return footer_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:06.074191Z",
     "start_time": "2021-02-02T22:22:06.061321Z"
    }
   },
   "outputs": [],
   "source": [
    "def scoreHeadersOrFooters(candidates):\n",
    "    scores  =[]\n",
    "    numpages= len(candidates)\n",
    "    WIN     = 8 #range of pages back and forth to compare\n",
    "    weights = [1,0.75, 0.5, 0.5, 0.5]\n",
    "    \n",
    "    for j in range(0, numpages):\n",
    "        \n",
    "        first = max(0, j-WIN)\n",
    "        last  = min(j+WIN, numpages-1)\n",
    "        pageScores = []\n",
    "    \n",
    "        for i in range (0,5):\n",
    "            similaritySum = 0\n",
    "            \n",
    "            for k in range(first, last):\n",
    "                \n",
    "                if j != k:\n",
    "                    similarity = SequenceMatcher(None, candidates[j][i][0],candidates[k][i][0]).ratio()\n",
    "                    similaritySum += similarity\n",
    "                similaritySum = weights[i]*similaritySum\n",
    "            pageScores.append([candidates[j][i], j, i,similaritySum])\n",
    "        scores.append(pageScores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:06.683515Z",
     "start_time": "2021-02-02T22:22:06.672747Z"
    }
   },
   "outputs": [],
   "source": [
    "def sortScores(scores, threshold=2.5, type = 'headers/footers'):\n",
    "    deleteList = []\n",
    "    \n",
    "    for page in scores:\n",
    "        \n",
    "        for line in page:\n",
    "            if line[3] > threshold:\n",
    "                deleteList.append(line)\n",
    "    print('Will delete these', type, ':', deleteList, '\\n')\n",
    "    return deleteList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:07.232276Z",
     "start_time": "2021-02-02T22:22:07.224760Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeHeaders(text_with_headers, keeplist=[], droplist=[],thresh=2.9, hidden=True):\n",
    "    possible_headers = findHeaders(text_with_headers)\n",
    "    header_scores    = scoreHeadersOrFooters (possible_headers)\n",
    "    probable_headers = sortScores(header_scores, threshold=thresh, type='headers')\n",
    "    new_text         = text_with_headers.copy()\n",
    "\n",
    "    set_of_headers   = set([x[0][0] for x in probable_headers])\n",
    "    set_of_headers   = [x for x in set_of_headers if x not in keeplist]\n",
    "    set_of_headers   = list(set(set_of_headers + droplist))\n",
    "    \n",
    "    new_text         = deleteHeaders(new_text, probable_headers)\n",
    "    \n",
    "    if hidden ==True:\n",
    "        new_text = removeHeadersOrFootersHiddeninText (new_text,set_of_headers)\n",
    "    \n",
    "    return new_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:07.965679Z",
     "start_time": "2021-02-02T22:22:07.957115Z"
    }
   },
   "outputs": [],
   "source": [
    "def deleteFooters(text_pages, footers_to_delete, keeplist=[]):\n",
    "    \n",
    "    for line in footers_to_delete:\n",
    "        if line[0][0] not in keeplist:\n",
    "            pagenum = line[1]\n",
    "            start   = line[0][1]\n",
    "            while text_pages[pagenum][start]=='\\n':\n",
    "                start -= 1\n",
    "            end     = line[0][2]\n",
    "#         print(pagenum, start, end)\n",
    "        if end == -1:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start]\n",
    "        else:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start] + text_pages[pagenum][end:]\n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:08.598188Z",
     "start_time": "2021-02-02T22:22:08.592209Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeHeadersOrFootersHiddeninText(text_pages, possibleHiddenItems):\n",
    "    print(possibleHiddenItems, '\\n')\n",
    "    \n",
    "    for option in possibleHiddenItems:\n",
    "        \n",
    "        for i, page in enumerate(text_pages):\n",
    "            option2 = option\n",
    "            if page.find(option2) != -1:\n",
    "                if page.find(option2) == page.rfind(option2):\n",
    "                    start = page.find(option2)\n",
    "                    end   = start + len(option2)\n",
    "                    while start > 0 and page[start-1]== '\\n':\n",
    "                        start -= 1\n",
    "                    while end + 1 <= len(page) - 1 and page[end+1] == '\\n':\n",
    "                        end += 1\n",
    "                    print(option2, 'deleted from page', i, 'position', page.find(option2))\n",
    "                    text_pages[i] = text_pages[i][:start]+' '+text_pages[i][end:]\n",
    "                    \n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:09.181687Z",
     "start_time": "2021-02-02T22:22:09.176219Z"
    }
   },
   "outputs": [],
   "source": [
    "def findHeaders(pages):\n",
    "    header_candidates = []\n",
    "    for num, page in enumerate(pages):\n",
    "#         print('page:', num)\n",
    "        lines = []\n",
    "        start = 0\n",
    "        end   = 0\n",
    "        \n",
    "        for i in range (0,5):\n",
    "            \n",
    "            while end - start < 1:\n",
    "                end   = page.find('\\n', start, start+200)\n",
    "                if end == -1:\n",
    "                    break\n",
    "                elif start == end:\n",
    "                    start = end + 1\n",
    "                    \n",
    "            if end == - 1:\n",
    "                line = ''\n",
    "            else:\n",
    "                line = re.sub('\\d', '@', page[start:end])\n",
    "            lines.append([line, start, end])\n",
    "            start = end + 1\n",
    "            \n",
    "        header_candidates.append(lines)\n",
    "\n",
    "    return header_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:09.881984Z",
     "start_time": "2021-02-02T22:22:09.876878Z"
    }
   },
   "outputs": [],
   "source": [
    "def deleteHeaders(text_pages, headers_to_delete, keeplist=[]):\n",
    "    for line in headers_to_delete:\n",
    "        if line[0][0] not in keeplist:\n",
    "            pagenum = line[1]\n",
    "            start   = line[0][1]\n",
    "            end     = line[0][2]+1\n",
    "            while text_pages[pagenum][end] == '\\n':\n",
    "                end += 1\n",
    "                \n",
    "        if start != 0:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start]+text_pages[pagenum][end:]\n",
    "        else:\n",
    "            text_pages[pagenum] = text_pages[pagenum][end:]\n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T07:45:11.700207Z",
     "start_time": "2021-01-21T07:45:11.691164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the text from a PDF\n",
    "# Must be passed the path to it's folder and the filename\n",
    "def extractTextFromPDF(file, tool):\n",
    "\n",
    "    # Note: The imageMagick functions read the pdf pages to memory. If there is not enough memory allocated to the stack, it writes them to the temp\n",
    "    #       directory. Unfortunately it also doesn't release or delete the files. So when iterating through lots of PDFs with lots of pages the \n",
    "    #       C:\\ drive runs out of memory. To get around this we have to use the with x as y functionality of python\n",
    "    #       In order to do it with many pdfs, as you will see below, it gets a bit convoluted\n",
    "    #       This still doesn't entirely work, but it helps\n",
    "    \n",
    "    # Get the PDF as a JPEG image\n",
    "    pdf_as_image = Image(filename = file , resolution=600)     \n",
    "    with pdf_as_image.convert('jpeg') as pdf_as_jpeg:\n",
    "\n",
    "        # Create the text object we will append text to\n",
    "        pages_text = []\n",
    "\n",
    "        # Iterate through the pdf images\n",
    "        for img in pdf_as_jpeg.sequence:\n",
    "            with Image(image = img) as img_page:\n",
    "\n",
    "                req_image = img_page.make_blob('jpeg')\n",
    "\n",
    "                # Create txt\n",
    "                with PI.open(io.BytesIO(req_image)) as im:\n",
    "                    txt = tool.image_to_string(\n",
    "                        im,\n",
    "                        builder = builders.TextBuilder()\n",
    "                    )\n",
    "                pages_text.append(txt.encode('utf-8', 'ignore'))\n",
    "                del txt\n",
    "                del img\n",
    "\n",
    "    #flatten array of pages into one string/byte for whole document\n",
    "    doc_text = b' '.join(pages_text) \n",
    "    del pages_text\n",
    "    \n",
    "    #remove breaks from words that wrap over two lines\n",
    "    doc_text = re.sub(b'-\\n', b'', doc_text) \n",
    "    \n",
    "    #replace single returns with spaces\n",
    "    doc_text = re.sub(b'\\n', b' ', doc_text) \n",
    "    \n",
    "    #replace double spaces with single spaces\n",
    "    doc_text = re.sub(b'  ', b' ', doc_text) \n",
    "    \n",
    "    #replace double returns with single returns\n",
    "    doc_text = re.sub(b'\\n\\n', b'\\n', doc_text) \n",
    "    \n",
    "    # Return the text\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops all characters up to the first '-' in the TagTog file name \n",
    "def dropGarbage(fileName):\n",
    "    \n",
    "    # Temp variable\n",
    "    temp = fileName\n",
    "    \n",
    "    # Get buffer for post index starting point\n",
    "    if temp[temp.index('-') + 1] == '_':\n",
    "        buffer = 2\n",
    "        \n",
    "    else:\n",
    "        buffer = 1\n",
    "        \n",
    "    # Return from the starting point post index\n",
    "    return temp[temp.index('-') + buffer:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:18:35.738180Z",
     "start_time": "2021-01-18T07:12:20.579122Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set root Directories\n",
      "Begin iterating through PDFs\n",
      "Current dir: ExtractedText\\\n",
      "Check and create destination directory\n",
      "Iterate through files\n",
      "Current dir: ExtractedText\\OldSet\n",
      "Check and create destination directory\n",
      "Iterate through files\n",
      "\n",
      "Source file: PDFs\\OldSet\\a59fulELeqIRtdP_zyI1F_BVRa.y-s42003_018_0260_y.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\s42003_018_0260_y.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n",
      "\n",
      "Source file: PDFs\\OldSet\\a8lxxCHk3HXTleqg2.Wf4RxRevaS-summer_activity_patterns_for_mosses_and_lichens_in_maritime_antarctica.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\summer_activity_patterns_for_mosses_and_lichens_in_maritime_antarctica.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n",
      "\n",
      "Source file: PDFs\\OldSet\\aDXuKg2KPEH_fR5A797NwmzPvSxy-fmicb_10_01018.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\fmicb_10_01018.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n",
      "\n",
      "Source file: PDFs\\OldSet\\agkER8jS.DZk0OdrF0PjgoJawxX8-source.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\source.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n",
      "\n",
      "Source file: PDFs\\OldSet\\aODfaqaUXF66h3uPZ4aupXSRnsqq-Fraser2018_Article_EvidenceOfPlantAndAnimalCommun.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\Fraser2018_Article_EvidenceOfPlantAndAnimalCommun.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n",
      "\n",
      "Source file: PDFs\\OldSet\\awkaYNS0KsDlin9nBwoxLbhE6rr0-Archer2017_Article_EndolithicMicrobialDiversityIn.txt.ann.pdf\n",
      "Dest   file: ExtractedText\\OldSet\\Archer2017_Article_EndolithicMicrobialDiversityIn.txt.ann.pdf\n",
      "Check and delete previous entries for same file\n",
      "Extract text from PDF\n",
      "Clean up the extracted text\n",
      "Write out the text file\n"
     ]
    }
   ],
   "source": [
    "# Set root directories\n",
    "print('Set root Directories')\n",
    "root_src_dir = 'PDFs\\\\'\n",
    "root_dst_dir = 'ExtractedText\\\\'\n",
    "\n",
    "# Get the installed OCR tools\n",
    "try:\n",
    "\n",
    "    tools = pyocr.get_available_tools()\n",
    "    if len(tools) == 0:\n",
    "        raise UserWarning(\"No OCR tool found\")\n",
    "\n",
    "    tool  = tools[0]\n",
    "\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Iterate through files in the PDF folder\n",
    "print('Begin iterating through PDFs')\n",
    "for src_dir, dirs, files in os.walk(root_src_dir):\n",
    "    \n",
    "    # Set the root directory\n",
    "    dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
    "    print('Current dir: ' + dst_dir)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    print('Check and create destination directory')\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "        \n",
    "    # Iterate through the files    \n",
    "    print('Iterate through files')\n",
    "    for file in files:\n",
    "        print('')\n",
    "        \n",
    "        # Create file paths\n",
    "        src_file = os.path.join(src_dir, file)\n",
    "        dst_file = os.path.join(dst_dir, dropGarbage(file))\n",
    "        print('Source file: ' + src_file)\n",
    "        print('Dest   file: ' + dst_file)\n",
    "          \n",
    "        # Delete previous files\n",
    "        print('Check and delete previous entries for same file')\n",
    "        if os.path.exists(dst_file):\n",
    "            \n",
    "            # Unless they are exactly the same file\n",
    "            if os.path.samefile(src_file, dst_file):\n",
    "                continue\n",
    "                \n",
    "            os.remove(dst_file)\n",
    "            \n",
    "        # Extract the text from the PDF\n",
    "        print('Extract text from PDF')\n",
    "        extractedText = extractTextFromPDF(src_file, tool)\n",
    "        \n",
    "        # Clean up the text as much as possible\n",
    "        print('Clean up the extracted text')\n",
    "        soup          = bs(extractedText)\n",
    "        raw           = soup.get_text()\n",
    "        txtfile       = re.sub('pdf', 'txt', dst_file)\n",
    "        \n",
    "        # Write out the .txt file\n",
    "        print('Write out the text file')\n",
    "        f = io.open(txtfile, \n",
    "                    encoding = 'utf-8', \n",
    "                    mode = 'w')\n",
    "        f.write(raw)\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "414px",
    "left": "1342px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
