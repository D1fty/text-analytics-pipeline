{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pipeline - Manaaki Whenua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Defaults\" data-toc-modified-id=\"Imports-and-Defaults-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Defaults</a></span></li><li><span><a href=\"#Extracting-Text-from-PDFs-(Revised-methodology)\" data-toc-modified-id=\"Extracting-Text-from-PDFs-(Revised-methodology)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Extracting Text from PDFs (Revised methodology)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Turn-PDF-into-Raw-Text\" data-toc-modified-id=\"Turn-PDF-into-Raw-Text-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Turn PDF into Raw Text</a></span></li><li><span><a href=\"#Convert-Array-of-Raw-Text-into-Cleaned-and-Flattened-Text-File\" data-toc-modified-id=\"Convert-Array-of-Raw-Text-into-Cleaned-and-Flattened-Text-File-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Convert Array of Raw Text into Cleaned and Flattened Text File</a></span></li><li><span><a href=\"#Converting-Individual-Documents\" data-toc-modified-id=\"Converting-Individual-Documents-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Converting Individual Documents</a></span><ul class=\"toc-item\"><li><span><a href=\"#Original-Group-of-PDFs\" data-toc-modified-id=\"Original-Group-of-PDFs-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Original Group of PDFs</a></span></li><li><span><a href=\"#Second-Group-of-PDFs\" data-toc-modified-id=\"Second-Group-of-PDFs-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Second Group of PDFs</a></span></li></ul></li></ul></li><li><span><a href=\"#Initial-Process---Now-replaced-with-new-method\" data-toc-modified-id=\"Initial-Process---Now-replaced-with-new-method-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Initial Process - Now replaced with new method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracting-Text-from-PDF\" data-toc-modified-id=\"Extracting-Text-from-PDF-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Extracting Text from PDF</a></span></li><li><span><a href=\"#Further-Preprocessing\" data-toc-modified-id=\"Further-Preprocessing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Further Preprocessing</a></span></li><li><span><a href=\"#Extracting-Text-From-.pdf-Files-in-a-Folder-and-Saving-New-.txt-Files\" data-toc-modified-id=\"Extracting-Text-From-.pdf-Files-in-a-Folder-and-Saving-New-.txt-Files-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Extracting Text From .pdf Files in a Folder and Saving New .txt Files</a></span></li></ul></li><li><span><a href=\"#Sandbox\" data-toc-modified-id=\"Sandbox-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sandbox</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:13.374787Z",
     "start_time": "2021-02-02T22:21:13.325096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } p, ul {max-width:␣\n",
       ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
       ",→display: flex; justify-content: center;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Markdown as md\n",
    "display(HTML(\"\"\"<style>.container { width:80% !important; } p, ul {max-width:␣\n",
    ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
    ",→display: flex; justify-content: center;}</style>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:15.252558Z",
     "start_time": "2021-02-02T22:21:15.250748Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install Pillow\n",
    "# pip install wand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:15.694193Z",
     "start_time": "2021-02-02T22:21:15.683759Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:17.986443Z",
     "start_time": "2021-02-02T22:21:17.977447Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyocr\n",
    "import pyocr.builders\n",
    "import io\n",
    "import codecs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:18.412101Z",
     "start_time": "2021-02-02T22:21:18.403001Z"
    }
   },
   "outputs": [],
   "source": [
    "import deleteMagick #executable script that deletes leftover magick* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:18.816403Z",
     "start_time": "2021-02-02T22:21:18.809833Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image as PI\n",
    "from wand.image import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from PDFs (Revised methodology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn PDF into Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T03:50:37.721148Z",
     "start_time": "2021-01-26T03:50:37.714533Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertPDFtoTextArray(folder, file):\n",
    "    \"\"\"\n",
    "    Takes a PDF document, converts to an image and \n",
    "    then extracts text from each page of that image into an array of UTF-8 encoded text.\n",
    "\n",
    "    \"\"\"\n",
    "#     print(\"beginning\")\n",
    "    pdf_as_image = Image(filename=folder+'/'+file, resolution=600)\n",
    "    pdf_as_jpeg = pdf_as_image.convert('jpeg')\n",
    "    del pdf_as_image\n",
    "    tool = pyocr.get_available_tools()[0]\n",
    "    pages_text = []\n",
    "    for img in pdf_as_jpeg.sequence:\n",
    "        img_page = Image(image=img)\n",
    "        req_image = img_page.make_blob('jpeg')\n",
    "        txt = tool.image_to_string(\n",
    "            PI.open(io.BytesIO(req_image)),\n",
    "            builder=pyocr.builders.TextBuilder()\n",
    "        )\n",
    "        pages_text.append(txt.encode('utf-8', 'ignore'))\n",
    "    filename=re.sub('.pdf','', file)\n",
    "    np.save(folder+'/'+filename, pages_text)\n",
    "    \n",
    "    deleteMagick.clean() #ensure ImageMagick files are removed from temp folder \n",
    "    \n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T03:50:38.933506Z",
     "start_time": "2021-01-26T03:50:38.929331Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertFolderofPDFstotextArrays(folder):\n",
    "    filelist = os.listdir(folder)\n",
    "    filelist = [x for x in filelist if x.endswith('pdf')]\n",
    "    numFiles = len(filelist)\n",
    "    for i, file in enumerate(filelist):\n",
    "        print('Converting file',i+1, 'of', numFiles)\n",
    "        convertPDFtoTextArray(folder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T00:26:35.069384Z",
     "start_time": "2021-01-24T00:26:35.067131Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertFolderofPDFstotextArrays('smallPDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T00:26:24.786040Z",
     "start_time": "2021-01-24T00:26:24.783564Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertFolderofPDFstotextArrays('FraserSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T00:26:32.157587Z",
     "start_time": "2021-01-24T00:26:32.155381Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertFolderofPDFstotextArrays('PDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T00:23:50.978273Z",
     "start_time": "2021-01-24T00:23:50.159968Z"
    }
   },
   "outputs": [],
   "source": [
    "deleteMagick.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Array of Raw Text into Cleaned and Flattened Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:05:29.418911Z",
     "start_time": "2021-01-26T04:05:29.416104Z"
    }
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:53.929587Z",
     "start_time": "2021-02-02T22:21:53.916276Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes a array of text strings (pages) and cleans docuemnt of headers and footers then cleans up flattened text (i.e. removes ) \n",
    "\n",
    "def cleanAndSaveText(folder, file, footkeeplist=[], footdroplist=[], headkeeplist=[], headdroplist=[],\n",
    "                           footthreshold=3, headthreshold=3, hiddenFooters=True, hiddenHeaders=True):\n",
    "    data = convertTextArrayToText(folder, file, footkeeplist,footdroplist, headkeeplist, headdroplist,\n",
    "                                  footthreshold, headthreshold, hiddenFooters, hiddenHeaders)\n",
    "    txtfile = re.sub('npy', 'txt', file)\n",
    "\n",
    "    f = io.open(folder+'/'+txtfile, \n",
    "                encoding='utf-8', \n",
    "                mode='w')\n",
    "\n",
    "#     doc_text = '!!!PAGE BREAK!!!'.join(data) \n",
    "    doc_text = ' '.join(data) \n",
    "\n",
    "    #remove breaks from words that wrap over two lines\n",
    "    doc_text = re.sub('-\\n', '', doc_text) \n",
    "\n",
    "#     #replace double returns with placeholder\n",
    "#     doc_text = re.sub('\\n\\n', '<DOUBLERETURN>', doc_text) \n",
    "\n",
    "    #replace single returns with spaces\n",
    "    doc_text = re.sub('\\n', ' ', doc_text) \n",
    "\n",
    "    #replace double spaces with single spaces\n",
    "    doc_text = re.sub('  ', ' ', doc_text) \n",
    "\n",
    "#     #replace placeholder with double returns\n",
    "#     doc_text = re.sub('<DOUBLERETURN>','\\n\\n', doc_text) \n",
    "    \n",
    "#     #replace double returns with single returns\n",
    "#     while doc_text.find('\\n\\n')!=-1:\n",
    "#         doc_text = re.sub('\\n\\n', '\\n', doc_text)\n",
    "\n",
    "#     doc_text = doc_text.decode()\n",
    "    f.write(doc_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T03:55:38.481738Z",
     "start_time": "2021-01-26T03:55:38.473640Z"
    }
   },
   "outputs": [],
   "source": [
    "# # takes a array of text strings (pages) and cleans docuemnt of headers and footers then cleans up flattened text (i.e. removes ) \n",
    "\n",
    "# def cleanAndSaveText(folder, file, footkeeplist=[], footdroplist=[], headkeeplist=[], headdroplist=[],\n",
    "#                            footthreshold=3, headthreshold=3, hiddenFooters=True, hiddenHeaders=True):\n",
    "#     data = convertTextArrayToText(folder, file, footkeeplist,footdroplist, headkeeplist, headdroplist,\n",
    "#                                   footthreshold, headthreshold, hiddenFooters, hiddenHeaders)\n",
    "#     txtfile = re.sub('npy', 'txt', file)\n",
    "\n",
    "#     f = io.open(folder+'/'+txtfile, \n",
    "#                 encoding='utf-8', \n",
    "#                 mode='w')\n",
    "\n",
    "# #     doc_text = '!!!PAGE BREAK!!!'.join(data) \n",
    "#     doc_text = ' '.join(data) \n",
    "\n",
    "#     #remove breaks from words that wrap over two lines\n",
    "#     doc_text = re.sub('-\\n', '', doc_text) \n",
    "\n",
    "#     #replace double returns with placeholder\n",
    "#     doc_text = re.sub('\\n\\n', '<DOUBLERETURN>', doc_text) \n",
    "\n",
    "#     #replace single returns with spaces\n",
    "#     doc_text = re.sub('\\n', ' ', doc_text) \n",
    "\n",
    "#     #replace double spaces with single spaces\n",
    "#     doc_text = re.sub('  ', ' ', doc_text) \n",
    "\n",
    "#     #replace placeholder with double returns\n",
    "#     doc_text = re.sub('<DOUBLERETURN>','\\n\\n', doc_text) \n",
    "    \n",
    "#     #replace double returns with single returns\n",
    "#     while doc_text.find('\\n\\n')!=-1:\n",
    "#         doc_text = re.sub('\\n\\n', '\\n', doc_text)\n",
    "\n",
    "# #     doc_text = doc_text.decode()\n",
    "#     f.write(doc_text)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:21:59.807045Z",
     "start_time": "2021-02-02T22:21:59.795867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use for full process from PDF doc to text file \n",
    "\n",
    "def convertOnePDFtoText(folder, file, footkeeplist=[], headkeeplist=[], \n",
    "                        footthreshold=3, headthreshold=3, hiddenFooters=True, hiddenHeaders=True):\n",
    "    pages_of_text = convertPDFtoTextArray(folder, file)\n",
    "#     pages_of_text = np.load(folder+'/'+file)\n",
    "    pages_of_text = decodeText(pages_of_text)\n",
    "    pages_of_text = removeFooters(pages_of_text, footkeeplist, footthreshold, hiddenFooters)\n",
    "    pages_of_text = removeHeaders(pages_of_text, headkeeplist, headthreshold, hiddenHeaders)\n",
    "    return pages_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:02.979193Z",
     "start_time": "2021-02-02T22:22:02.973736Z"
    }
   },
   "outputs": [],
   "source": [
    "# use for turning an already prcessed PDF (i.e. converted to npy file) into text file without headers or footers\n",
    "\n",
    "def convertTextArrayToText(folder, file, footkeeplist=[], footdroplist=[], headkeeplist=[],headdroplist=[], \n",
    "                           footthreshold=3, headthreshold=3, hiddenFooters=True, hiddenHeaders=True):\n",
    "#     pages_of_text = convertPDFtoTextArray(folder, file)\n",
    "    pages_of_text = np.load(folder+'/'+file)\n",
    "    pages_of_text = decodeText(pages_of_text)\n",
    "    pages_of_text = removeFooters(pages_of_text, footkeeplist, footdroplist, footthreshold, hiddenFooters)\n",
    "    pages_of_text = removeHeaders(pages_of_text, headkeeplist, headdroplist, headthreshold, hiddenHeaders)\n",
    "    return pages_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:03.948227Z",
     "start_time": "2021-02-02T22:22:03.945233Z"
    }
   },
   "outputs": [],
   "source": [
    "def decodeText(encoded_text):\n",
    "    decoded_text = [page.decode('utf-8') for page in encoded_text]\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:04.698421Z",
     "start_time": "2021-02-02T22:22:04.685277Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeFooters(text_with_footers, keeplist=[], droplist=[], thresh=2.9, hidden=True):\n",
    "    possible_footers = findFooters(text_with_footers)\n",
    "    footer_scores = scoreHeadersOrFooters (possible_footers)\n",
    "    probableFooters = sortScores(footer_scores, threshold=thresh, type='footers')\n",
    "    new_text = text_with_footers.copy()\n",
    "    \n",
    "#     while len(probableFooters)>0: #not empty\n",
    "#         set_of_footers = set([x[0][0] for x in probableFooters])\n",
    "#         new_text = deleteFooters(new_text, probableFooters)\n",
    "#         new_text = removeFootersHiddeninText (new_text,set_of_footers)\n",
    "#         possible_footers = findFooters(new_text)\n",
    "#         footer_scores = scoreHeadersOrFooters (possible_footers)\n",
    "#         probableFooters = sortScores(footer_scores, threshold=4)\n",
    "\n",
    "    set_of_footers = set([x[0][0] for x in probableFooters])\n",
    "    set_of_footers = [x for x in set_of_footers if x not in keeplist]\n",
    "    set_of_footers = list(set(set_of_footers + droplist))\n",
    "    new_text = deleteFooters(new_text, probableFooters)\n",
    "    if hidden ==True:\n",
    "        new_text = removeHeadersOrFootersHiddeninText (new_text,set_of_footers)    \n",
    "    \n",
    "    return new_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:05.385316Z",
     "start_time": "2021-02-02T22:22:05.365257Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findFooters(pages):\n",
    "    footer_candidates=[]\n",
    "    for num, page in enumerate(pages):\n",
    "#         print('page:', num)\n",
    "        lines = []\n",
    "        start=-1\n",
    "        end=-1\n",
    "        doublereturn=False\n",
    "        for i in range (0,5):\n",
    "#             print(i)\n",
    "            while end==start:\n",
    "                start = page.rfind('\\n', max(0,end-200), end)\n",
    "                if start == -1:\n",
    "#                     print('not enough')\n",
    "                    break\n",
    "                elif start == end-1:\n",
    "                    end = start\n",
    "                    doublereturn=True\n",
    "#                     print('looping', start, end)\n",
    "#             print(start, end)\n",
    "            if start ==-1:\n",
    "                line = ''\n",
    "            if end ==-1:\n",
    "                line = re.sub('\\d', '@', page[start+1:])\n",
    "            elif (doublereturn):\n",
    "                line = re.sub('\\d', '@', page[start+1:end])\n",
    "            else: line = re.sub('\\d', '@', page[start+1:end])\n",
    "\n",
    "\n",
    "#             print(line)\n",
    "            lines.append([line, start, end])\n",
    "            end = start\n",
    "#         print(lines)\n",
    "        footer_candidates.append(lines)\n",
    "\n",
    "    return footer_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:06.074191Z",
     "start_time": "2021-02-02T22:22:06.061321Z"
    }
   },
   "outputs": [],
   "source": [
    "def scoreHeadersOrFooters(candidates):\n",
    "    scores=[]\n",
    "    numpages= len(candidates)\n",
    "    WIN = 8 #range of pages back and forth to compare\n",
    "    weights = [1,0.75, 0.5, 0.5, 0.5]\n",
    "    for j in range(0, numpages):\n",
    "#         print('j=', j)\n",
    "        first = max(0, j-WIN)\n",
    "        last = min(j+WIN, numpages-1)\n",
    "#         for i in range (0,5):\n",
    "        pageScores = []\n",
    "        for i in range (0,5):\n",
    "            similaritySum=0\n",
    "            for k in range(first, last):\n",
    "                if j!=k:\n",
    "                    similarity = SequenceMatcher(None, candidates[j][i][0],candidates[k][i][0]).ratio()\n",
    "    #                 print(similarity)\n",
    "                    similaritySum+=similarity\n",
    "                similaritySum=weights[i]*similaritySum\n",
    "            pageScores.append([candidates[j][i], j, i,similaritySum])\n",
    "#             print(candidates[j][i], similaritySum)\n",
    "        scores.append(pageScores)\n",
    "#             print('k=',k)\n",
    "#         print (first, last)\n",
    "#     print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:06.683515Z",
     "start_time": "2021-02-02T22:22:06.672747Z"
    }
   },
   "outputs": [],
   "source": [
    "def sortScores(scores, threshold=2.5, type = 'headers/footers'):\n",
    "    deleteList = []\n",
    "    for page in scores:\n",
    "        for line in page:\n",
    "            if line[3] >threshold:\n",
    "                deleteList.append(line)\n",
    "    print('Will delete these', type, ':', deleteList, '\\n')\n",
    "    return deleteList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:07.232276Z",
     "start_time": "2021-02-02T22:22:07.224760Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeHeaders(text_with_headers, keeplist=[], droplist=[],thresh=2.9, hidden=True):\n",
    "    possible_headers = findHeaders(text_with_headers)\n",
    "    header_scores = scoreHeadersOrFooters (possible_headers)\n",
    "    probable_headers = sortScores(header_scores, threshold=thresh, type='headers')\n",
    "    new_text = text_with_headers.copy()\n",
    "    \n",
    "#     while len(probable_headers)>0: #not empty\n",
    "#         set_of_headers = set([x[0][0] for x in probable_headers])\n",
    "#         new_text = deleteFooters(new_text, probable_headers)\n",
    "#         new_text = removeFootersHiddeninText (new_text,set_of_headers)\n",
    "#         possible_headers = findHeaders(new_text)\n",
    "#         header_scores = scoreHeadersOrFooters (possible_headers)\n",
    "#         probable_headers = sortScores(header_scores, threshold=4)\n",
    "\n",
    "    set_of_headers = set([x[0][0] for x in probable_headers])\n",
    "    set_of_headers = [x for x in set_of_headers if x not in keeplist]\n",
    "    set_of_headers = list(set(set_of_headers + droplist))\n",
    "    new_text = deleteHeaders(new_text, probable_headers)\n",
    "    if hidden ==True:\n",
    "        new_text = removeHeadersOrFootersHiddeninText (new_text,set_of_headers)\n",
    "#         possible_headers = findHeaders(new_text)\n",
    "#         header_scores = scoreHeadersOrFooters (possible_headers)\n",
    "#         probable_headers = sortScores(header_scores, threshold=4)\n",
    "    \n",
    "    \n",
    "    return new_text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:07.965679Z",
     "start_time": "2021-02-02T22:22:07.957115Z"
    }
   },
   "outputs": [],
   "source": [
    "def deleteFooters(text_pages, footers_to_delete, keeplist=[]):\n",
    "    for line in footers_to_delete:\n",
    "        if line[0][0] not in keeplist:\n",
    "            pagenum = line[1]\n",
    "            start = line[0][1]\n",
    "            while text_pages[pagenum][start]=='\\n':\n",
    "                start-=1\n",
    "            end = line[0][2]\n",
    "#         print(pagenum, start, end)\n",
    "        if end == -1:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start]\n",
    "        else:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start] + text_pages[pagenum][end:]\n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:08.598188Z",
     "start_time": "2021-02-02T22:22:08.592209Z"
    }
   },
   "outputs": [],
   "source": [
    "def removeHeadersOrFootersHiddeninText(text_pages, possibleHiddenItems):\n",
    "# , headerOrfooter):\n",
    "    print(possibleHiddenItems, '\\n')\n",
    "    for option in possibleHiddenItems:\n",
    "        for i, page in enumerate(text_pages):\n",
    "            option2 = option\n",
    "#             print('looking for', option2, 'on page', i)\n",
    "\n",
    "            if page.find(option2) !=-1:\n",
    "#                 print('found:', option2, i, page.find(option2))\n",
    "#                 print(page.find(option2), page.rfind(option2))\n",
    "                if page.find(option2) == page.rfind(option2):\n",
    "                    start=page.find(option2)\n",
    "                    end=start+len(option2)\n",
    "#                     print(start, end, page[start:end])\n",
    "                    while start > 0  and page[start-1]== '\\n':\n",
    "                        start -=1\n",
    "                    while end+1 <= len(page)-1 and page[end+1]=='\\n':\n",
    "                        end+=1\n",
    "                    print(option2, 'deleted from page', i, 'position', page.find(option2))\n",
    "                    text_pages[i] = text_pages[i][:start]+' '+text_pages[i][end:]\n",
    "#                     text_pages[i] = re.sub(option2,' ',page)\n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:09.181687Z",
     "start_time": "2021-02-02T22:22:09.176219Z"
    }
   },
   "outputs": [],
   "source": [
    "def findHeaders(pages):\n",
    "    header_candidates = []\n",
    "    for num, page in enumerate(pages):\n",
    "#         print('page:', num)\n",
    "        lines=[]\n",
    "        start=0\n",
    "        end=0\n",
    "        for i in range (0,5):\n",
    "            while end-start<1:\n",
    "                end = page.find('\\n', start, start+200)\n",
    "                if end ==-1:\n",
    "                    break\n",
    "                elif start == end:\n",
    "                    start = end+1\n",
    "#                 print('looping', start, end)\n",
    "        #     print(start, end)\n",
    "#             print(page[start:end])\n",
    "            if end ==-1:\n",
    "                line =''\n",
    "            else:\n",
    "                line = re.sub('\\d', '@', page[start:end])\n",
    "            lines.append([line, start, end])\n",
    "            start = end+1\n",
    "#         print(lines)\n",
    "        header_candidates.append(lines)\n",
    "#     print(len(header_candidates))\n",
    "\n",
    "    return header_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:09.881984Z",
     "start_time": "2021-02-02T22:22:09.876878Z"
    }
   },
   "outputs": [],
   "source": [
    "def deleteHeaders(text_pages, headers_to_delete, keeplist=[]):\n",
    "    for line in headers_to_delete:\n",
    "        if line[0][0] not in keeplist:\n",
    "            pagenum = line[1]\n",
    "            start = line[0][1]\n",
    "            end = line[0][2]+1\n",
    "            while text_pages[pagenum][end]=='\\n':\n",
    "                end+=1\n",
    "#         print(pagenum, start, end)\n",
    "        if start != 0:\n",
    "            text_pages[pagenum] = text_pages[pagenum][:start]+text_pages[pagenum][end:]\n",
    "        else:\n",
    "            text_pages[pagenum] = text_pages[pagenum][end:]\n",
    "    return text_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Individual Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Group of PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:16:53.449989Z",
     "start_time": "2021-02-03T01:16:53.445943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thesis (2).npy',\n",
       " 'thesis (3).npy',\n",
       " '10.1007@s00300-017-2230-0.npy',\n",
       " 'fevo-07-00076.npy',\n",
       " 'greenslade2018.npy',\n",
       " 'gen-2015-0194suppla.npy',\n",
       " 'gen-2015-0194.npy',\n",
       " 'thesis.npy',\n",
       " 'thesis (1).npy']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npyfiles  = [x for x in os.listdir('PDFs') if x.endswith('npy')]\n",
    "npyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:17:03.085046Z",
     "start_time": "2021-02-03T01:17:03.081081Z"
    }
   },
   "outputs": [],
   "source": [
    "fevofootdroplist=['Frontiers in Ecology and Evolution | www.frontiersin.org',\n",
    "                  'Frontiers in Ecology and Evolution | www.frontiersin.or']\n",
    "fevoheaddroplist = ['Spatial Diversity of Antarctic Springtails',\n",
    "                    ' | g 1', ' | g ', \n",
    "                   ]\n",
    "gen2015headdroplist = ['The 6th International Barcode of Life Conference Downloaded from cdnsciencepub.com by 151.210.131.92 on 11/30/20 For personal use only.',\n",
    "                      'Genome Vol. 59, 2016', 'Beet et al.', 'The 6th International Barcode of Life Conference Downloaded from cdnscience',\n",
    "                      'pub.com by 151.210.131.92 on 11/30/20','For personal use only.']\n",
    "thesis2footkeeplist=['Cape Bird, Antarctica','Miers Valley, Antarctica','Cape Crozier, Antarctica',\n",
    "                     'Granite Harbour, Antarctica','Marble Point, Antarctica','Antarctica']\n",
    "thesis2headkeeplist=['TABLE OF CONTENTS','TABLE OF UNITS AND ABBREVIATIONS','LIST OF TABLES',\n",
    "                     'LIST OF FIGURES',  'LIST OF APPENDICES','CHAPTER @', 'Taxon','REFERENCES @@']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:31:58.929993Z",
     "start_time": "2021-02-03T01:31:58.921546Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[0], footkeeplist=thesis2footkeeplist, headkeeplist=thesis2headkeeplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:32:15.931768Z",
     "start_time": "2021-02-03T01:32:15.929609Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[1], footthreshold=9, hiddenHeaders=False) #some line breaks remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:32:27.477721Z",
     "start_time": "2021-02-03T01:32:27.475169Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:39:31.099282Z",
     "start_time": "2021-02-03T01:39:31.097012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[3], footdroplist=fevofootdroplist, headdroplist=fevoheaddroplist) \n",
    "# #some manual cleaning needed after running this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:40:12.433690Z",
     "start_time": "2021-02-03T01:40:12.431411Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:40:23.728114Z",
     "start_time": "2021-02-03T01:40:23.725615Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:40:49.895612Z",
     "start_time": "2021-02-03T01:40:49.892565Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[6], headdroplist=gen2015headdroplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:40:59.108424Z",
     "start_time": "2021-02-03T01:40:59.106291Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[7], headthreshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:41:10.948020Z",
     "start_time": "2021-02-03T01:41:10.945928Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('PDFs', npyfiles[8], footthreshold=5.5, hiddenFooters=False, headthreshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Group of PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:22:58.555311Z",
     "start_time": "2021-02-02T22:22:58.544598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archer2017_Article_EndolithicMicrobialDiversityIn.npy',\n",
       " 'Fraser2018_Article_EvidenceOfPlantAndAnimalCommun.npy',\n",
       " 'source.npy',\n",
       " 'fmicb-10-01018.npy',\n",
       " 's42003-018-0260-y.npy',\n",
       " 'summer-activity-patterns-for-mosses-and-lichens-in-maritime-antarctica.npy',\n",
       " 'fmicb-07-01642.npy']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npyfiles2  = [x for x in os.listdir('FraserSet') if x.endswith('npy')]\n",
    "npyfiles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:23:21.394320Z",
     "start_time": "2021-02-02T22:23:21.391298Z"
    }
   },
   "outputs": [],
   "source": [
    "archerheaddroplist = ['Polar Biol (2017) 40:997—1006']\n",
    "\n",
    "Fraser2018headdroplist = ['Polar Biol (2018) 41:417-421']\n",
    "\n",
    "sourceheaddroplist = ['T. C. Hawes', 'Downloaded by [University of Cambridge] at 05:22 08 April 2016',\n",
    "                      'LIST OF FIGURES',  'LIST OF APPENDICES','CHAPTER @', 'Taxon','REFERENCES @@']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:26:19.725881Z",
     "start_time": "2021-02-02T22:26:19.723587Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[0], headdroplist=archerheaddroplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T22:26:30.603786Z",
     "start_time": "2021-02-02T22:26:30.601403Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[1], headthreshold=2, footthreshold=1, headdroplist=Fraser2018headdroplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T00:58:46.984208Z",
     "start_time": "2021-02-03T00:58:46.982060Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[2], headdroplist=sourceheaddroplist, headthreshold=1)\n",
    "# #some additional headers deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T00:59:24.552419Z",
     "start_time": "2021-02-03T00:59:24.550116Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[3], headthreshold=2, footthreshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:14:28.940279Z",
     "start_time": "2021-02-03T01:14:28.938083Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:14:56.764723Z",
     "start_time": "2021-02-03T01:14:56.762599Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[5], footthreshold=2, \n",
    "#                  headthreshold=2, headdroplist=['BURKHARD SCHROETER et al.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T01:15:21.495542Z",
     "start_time": "2021-02-03T01:15:21.493091Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanAndSaveText('FraserSet', npyfiles2[6], headthreshold=2, footthreshold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Process - Now replaced with new method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial process had some flaws. Headers and footer were not removed and this caused issues with tokenisation and named entity recognition. Additionally the beautiful soup tool used to clean text removed entire pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T07:45:11.700207Z",
     "start_time": "2021-01-21T07:45:11.691164Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractTextFromPDF(folder, file):\n",
    "    \"\"\"\n",
    "    Takes a PDF document, converts to an image and \n",
    "    then extracts text from each page of that image into an array.\n",
    "    The elements of the array (pages) are merged into one document \n",
    "    and line breaks removed ready for the next step.\n",
    "    \"\"\"\n",
    "#     print(\"beginning\")\n",
    "    pdf_as_image = Image(filename=folder+'/'+file, resolution=600)\n",
    "    print('number of pages:', len(pdf_as_image.sequence))\n",
    "    pdf_as_jpeg = pdf_as_image.convert('jpeg')\n",
    "    print('number of pages:', len(pdf_as_jpeg.sequence))\n",
    "    del pdf_as_image\n",
    "    tool = pyocr.get_available_tools()[0]\n",
    "#     req_image = []\n",
    "    pages_text = []\n",
    "#     final_text = ''\n",
    "#     print('pages: ')\n",
    "    for img in pdf_as_jpeg.sequence:\n",
    "        img_page = Image(image=img)\n",
    "        req_image = img_page.make_blob('jpeg')\n",
    "        txt = tool.image_to_string(\n",
    "            PI.open(io.BytesIO(req_image)),\n",
    "            #lang=lang,\n",
    "            builder=pyocr.builders.TextBuilder()\n",
    "        )\n",
    "        pages_text.append(txt.encode('utf-8', 'ignore'))\n",
    "\n",
    "#     del img\n",
    "    #flatten array of pages into one string/byte for whole document\n",
    "    doc_text = b' '.join(pages_text) \n",
    "    del pages_text\n",
    "    #remove breaks from words that wrap over two lines\n",
    "    doc_text = re.sub(b'-\\n', b'', doc_text) \n",
    "    #replace single returns with spaces\n",
    "    doc_text = re.sub(b'\\n', b' ', doc_text) \n",
    "    \n",
    "    #replace double spaces with single spaces\n",
    "    doc_text = re.sub(b'  ', b' ', doc_text) \n",
    "    \n",
    "    #replace double returns with single returns\n",
    "    doc_text = re.sub(b'\\n\\n', b'\\n', doc_text) \n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T07:48:20.968860Z",
     "start_time": "2021-01-21T07:45:20.059617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages: 13\n",
      "number of pages: 13\n",
      "1\n",
      "number of pages: 1\n",
      "2\n",
      "number of pages: 2\n",
      "3\n",
      "number of pages: 3\n",
      "4\n",
      "number of pages: 4\n",
      "5\n",
      "number of pages: 5\n",
      "6\n",
      "number of pages: 6\n",
      "7\n",
      "number of pages: 7\n",
      "8\n",
      "number of pages: 8\n",
      "9\n",
      "number of pages: 9\n",
      "10\n",
      "number of pages: 10\n",
      "11\n",
      "number of pages: 11\n",
      "12\n",
      "number of pages: 12\n",
      "13\n",
      "number of pages: 13\n",
      "number of pages: 13\n"
     ]
    }
   ],
   "source": [
    "document = extractTextFromPDF('PDFs',\"10.1007@s00300-017-2230-0.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T08:22:59.597611Z",
     "start_time": "2021-01-21T08:22:59.593934Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Polar Biology https://doi.org/10.1007/s00300-017-2230-0 ORIGINAL PAPER @® CrossMark Collembola of Barrientos Island, Antarctica: first census and assessment of environmental factors determining springtail distribution Natalia Enriquez' - Pablo Tejedo? - Javier Benayas? - Belén Albertos? - Maria José Luciafiez' Received: 21 October 2016 / Revised: 13 August 2017 / Accepted: 3 December 2017 © Springer-Verlag GmbH Germany, part of Springer Nature 2017 Abstract Barrientos Island is a small islet in the South Shetland archipelago frequently visited by Antarctic tourists. Collembola were recently used in another study developed in this site to assess the environmental conditions of two paths used by visitors, showing the importance of this soil faunal community. This motivated the realization of the first comprehensive census of Collembola from Barrientos Island. Fifty-six samples were recorded over three seasons, 2011-2013, from eight different substrate types. During the last campaign,\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:1000].decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T04:37:40.354821Z",
     "start_time": "2021-01-21T04:37:40.350160Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup to strip further unwanted characters from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T04:37:41.822982Z",
     "start_time": "2021-01-21T04:37:41.516029Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = bs(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T04:37:43.289255Z",
     "start_time": "2021-01-21T04:37:43.270671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Polar Biology\\nhttps://doi.org/10.1007/s00300-017-2230-0\\n\\nORIGINAL PAPER\\n\\n@® CrossMark\\n\\nCollembola of Barrientos Island, Antarctica: first census\\nand assessment of environmental factors determining springtail\\ndistribution\\n\\nNatalia Enriquez' - Pablo Tejedo? - Javier Benayas? - Belén Albertos? - Maria José Luciafiez'\\n\\nReceived: 21 October 2016 / Revised: 13 August 2017 / Accepted: 3 December 2017\\n© Springer-Verlag GmbH Germany, part of Springer Nature 2017\\n\\nAbstract\\n\\nBarrientos Island is a small islet in the South Shetland archipelago frequently visited by Antarctic tourists. Collembola were\\nrecently used in another study developed in this site to assess the environmental conditions of two paths used by visitors,\\nshowing the importance of this soil faunal community. This motivated the realization of the first comprehensive census of\\nCollembola from Barrientos Island. Fifty-six samples were recorded over three seasons, 2011-2013, from eight different\\nsubstrate types. During the last campai\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = soup.get_text()\n",
    "raw[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T02:21:20.659102Z",
     "start_time": "2021-01-13T02:21:20.654652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T02:21:20.664493Z",
     "start_time": "2021-01-13T02:21:20.661156Z"
    }
   },
   "outputs": [],
   "source": [
    "end = raw.rfind('References')\n",
    "raw = raw[:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T02:21:20.670766Z",
     "start_time": "2021-01-13T02:21:20.666847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27828"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T02:21:20.678234Z",
     "start_time": "2021-01-13T02:21:20.672956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text From .pdf Files in a Folder and Saving New .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T20:52:37.659886Z",
     "start_time": "2021-01-18T20:52:37.653949Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertFolderofPDFsToText (folder):\n",
    "    \"\"\"\n",
    "    Runs through a given folder and converts PDFs to cleaned text files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : string\n",
    "        location of PDF files to convert. (also target location for text files)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    n/a\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filelist = os.listdir(folder)\n",
    "    filelist = [x for x in filelist if x.endswith('pdf')]\n",
    "    numFiles = len(filelist)\n",
    "    for i, file in enumerate(filelist):\n",
    "        txtfile = re.sub('pdf', 'txt', file)\n",
    "        print('Converting file',i+1, 'of', numFiles)\n",
    "#         print('creating ', txtfile)\n",
    "        f = io.open(folder+'/'+txtfile, \n",
    "                    encoding='utf-8', \n",
    "                    mode='w')\n",
    "#         print('loading ', file)\n",
    "#         print('extracting text from ', file)\n",
    "        extractedText = extractTextFromPDF(folder, file)\n",
    "        soup = bs(extractedText)\n",
    "        raw = soup.get_text()\n",
    "#         print('saving ', txtfile)\n",
    "        f.write(raw)\n",
    "#         print('closing ', txtfile, '\\n')\n",
    "        f.close()\n",
    "        deleteMagick.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T20:52:49.648522Z",
     "start_time": "2021-01-18T20:52:49.643419Z"
    }
   },
   "outputs": [],
   "source": [
    "def convertPDFtoText (folder, file):\n",
    "    \"\"\"\n",
    "    Converts one PDFs to cleaned text file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : string\n",
    "        location of PDF files to convert. (also target location for text files)\n",
    "    file : string\n",
    "        name of file to convert\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    n/a\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    txtfile = re.sub('pdf', 'txt', file)\n",
    "    f = io.open(folder+'/'+txtfile, \n",
    "                    encoding='utf-8', \n",
    "                    mode='w')\n",
    "    extractedText = extractTextFromPDF(folder, file)\n",
    "    \n",
    "#     soup = bs(extractedText)\n",
    "#     raw = soup.get_text()\n",
    "    f.write(raw)\n",
    "    f.close()\n",
    "    deleteMagick.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:18:35.738180Z",
     "start_time": "2021-01-18T07:12:20.579122Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file 1 of 3\n",
      "Converting file 2 of 3\n",
      "Converting file 3 of 3\n"
     ]
    }
   ],
   "source": [
    "convertFolderofPDFsToText('smallPDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T07:37:31.192427Z",
     "start_time": "2021-01-18T07:18:35.754337Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file 1 of 7\n",
      "Converting file 2 of 7\n",
      "Converting file 3 of 7\n",
      "Converting file 4 of 7\n",
      "Converting file 5 of 7\n",
      "Converting file 6 of 7\n",
      "Converting file 7 of 7\n"
     ]
    }
   ],
   "source": [
    "convertFolderofPDFsToText('FraserSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T21:06:50.875160Z",
     "start_time": "2021-01-18T21:02:46.048035Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'fevo-07-00076.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T21:36:57.121359Z",
     "start_time": "2021-01-18T21:09:31.590544Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'thesis (3).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T17:59:12.836814Z",
     "start_time": "2021-01-18T17:59:12.828418Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertFolderofPDFsToText('PDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T03:40:30.821752Z",
     "start_time": "2021-01-13T03:38:21.020914Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'gen-2015-0194.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T20:57:19.355453Z",
     "start_time": "2021-01-18T20:53:18.779Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'thesis.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T20:57:19.360888Z",
     "start_time": "2021-01-18T20:53:24.491Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'thesis (1).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T20:57:19.365066Z",
     "start_time": "2021-01-18T20:53:27.573Z"
    }
   },
   "outputs": [],
   "source": [
    "# convertPDFtoText('PDFs', 'thesis (2).pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T03:41:13.460801Z",
     "start_time": "2021-01-25T03:41:13.452485Z"
    }
   },
   "outputs": [],
   "source": [
    "def saveArraysAsText(folder):\n",
    "    filelist = os.listdir(folder)\n",
    "    filelist = [x for x in filelist if x.endswith('npy')]\n",
    "    numFiles = len(filelist)\n",
    "    for i, file in enumerate(filelist):\n",
    "        data = np.load(folder+'/'+file)\n",
    "        txtfile = re.sub('npy', 'txt', file)\n",
    "        print('Converting file',i+1, 'of', numFiles)\n",
    "#         print('creating ', txtfile)\n",
    "        f = io.open(folder+'/'+txtfile, \n",
    "                    encoding='utf-8', \n",
    "                    mode='w')\n",
    "\n",
    "        doc_text = b'\\n\\n!!!PAGE BREAK!!!\\n\\n'.join(data) \n",
    "\n",
    "        #remove breaks from words that wrap over two lines\n",
    "        doc_text = re.sub(b'-\\n', b'', doc_text) \n",
    "        \n",
    "        #replace double returns with single returns\n",
    "        doc_text = re.sub(b'\\n\\n', b'<DOUBLERETURN>', doc_text) \n",
    "        \n",
    "        #replace single returns with spaces\n",
    "        doc_text = re.sub(b'\\n', b' ', doc_text) \n",
    "\n",
    "        #replace double spaces with single spaces\n",
    "        doc_text = re.sub(b'  ', b' ', doc_text) \n",
    "        \n",
    "        #replace double returns with single returns\n",
    "        doc_text = re.sub(b'<DOUBLERETURN>',b'\\n\\n', doc_text) \n",
    "\n",
    "        doc_text = doc_text.decode()\n",
    "        f.write(doc_text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndSaveArrayAsText(folder, file):\n",
    "    data = np.load(folder+'/'+file)\n",
    "    txtfile = re.sub('npy', 'txt', file)\n",
    "    print('Converting file',i+1, 'of', numFiles)\n",
    "    f = io.open(folder+'/'+txtfile, \n",
    "                encoding='utf-8', \n",
    "                mode='w')\n",
    "\n",
    "    doc_text = b'\\n\\n!!!PAGE BREAK!!!\\n\\n'.join(data) \n",
    "\n",
    "    #remove breaks from words that wrap over two lines\n",
    "    doc_text = re.sub(b'-\\n', b'', doc_text) \n",
    "\n",
    "    #replace double returns with single returns\n",
    "    doc_text = re.sub(b'\\n\\n', b'<DOUBLERETURN>', doc_text) \n",
    "\n",
    "    #replace single returns with spaces\n",
    "    doc_text = re.sub(b'\\n', b' ', doc_text) \n",
    "\n",
    "    #replace double spaces with single spaces\n",
    "    doc_text = re.sub(b'  ', b' ', doc_text) \n",
    "\n",
    "    #replace double returns with single returns\n",
    "    doc_text = re.sub(b'<DOUBLERETURN>',b'\\n\\n', doc_text) \n",
    "\n",
    "    doc_text = doc_text.decode()\n",
    "    f.write(doc_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T09:37:22.823155Z",
     "start_time": "2021-01-23T09:37:22.815589Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractTextFromPDF_test(folder, file):\n",
    "    \"\"\"\n",
    "    Takes a PDF document, converts to an image and \n",
    "    then extracts text from each page of that image into an array.\n",
    "    The elements of the array (pages) are merged into one document \n",
    "    and line breaks removed ready for the next step.\n",
    "    \"\"\"\n",
    "#     print(\"beginning\")\n",
    "    pdf_as_image = Image(filename=folder+'/'+file, resolution=600)\n",
    "    pdf_as_jpeg = pdf_as_image.convert('jpeg')\n",
    "    del pdf_as_image\n",
    "    tool = pyocr.get_available_tools()[0]\n",
    "#     req_image = []\n",
    "    pages_text = []\n",
    "#     final_text = ''\n",
    "#     print('pages: ')\n",
    "    for img in pdf_as_jpeg.sequence:\n",
    "        img_page = Image(image=img)\n",
    "        req_image = img_page.make_blob('jpeg')\n",
    "        txt = tool.image_to_string(\n",
    "            PI.open(io.BytesIO(req_image)),\n",
    "            #lang=lang,\n",
    "            builder=pyocr.builders.TextBuilder()\n",
    "        )\n",
    "        pages_text.append(txt.encode('utf-8', 'ignore'))\n",
    "#         print(len(pages_text))\n",
    "#     del img\n",
    "    #flatten array of pages into one string/byte for whole document\n",
    "#     doc_text = b' '.join(pages_text) \n",
    "#     del pages_text\n",
    "#     #remove breaks from words that wrap over two lines\n",
    "#     doc_text = re.sub(b'-\\n', b'', doc_text) \n",
    "#     #replace single returns with spaces\n",
    "#     doc_text = re.sub(b'\\n', b' ', doc_text) \n",
    "    \n",
    "#     #replace double spaces with single spaces\n",
    "#     doc_text = re.sub(b'  ', b' ', doc_text) \n",
    "\n",
    "\n",
    "    \n",
    "#     #replace double returns with single returns\n",
    "#     doc_text = re.sub(b'\\n\\n', b'\\n', doc_text) \n",
    "#     return doc_text\n",
    "    return pages_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "414px",
    "left": "1342px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
