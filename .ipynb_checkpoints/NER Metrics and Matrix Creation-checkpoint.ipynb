{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:30:56.753705Z",
     "start_time": "2021-03-13T07:30:56.743885Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import difflib\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "from spacy.matcher   import Matcher\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of lists of files contained within folders\n",
    "# Pass the root folder as a string eg: 'folder\\\\'\n",
    "def listAllFilePairs(rootFolder):\n",
    "    \n",
    "    # Start with empty list\n",
    "    returnList = list()\n",
    "    \n",
    "    # Walk down from the source folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(rootFolder):\n",
    "        \n",
    "        # If there are files in the folder\n",
    "        if (len(filenames) > 0):\n",
    "            \n",
    "            # New list\n",
    "            files = []\n",
    "            \n",
    "            # Add the list of filenames to the return list\n",
    "            for fileName in filenames:\n",
    "                \n",
    "                # Add to the list of files\n",
    "                files += [os.path.join(dirpath, fileName)]\n",
    "                                       \n",
    "            # Add to the return list\n",
    "            returnList += [files]\n",
    "                                       \n",
    "    # Finally, return the list\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of tuples contains all paths and files of a specific type in a directory and its sub directory\n",
    "# Pass an extension to override .json\n",
    "def listAllFiles(folder, extension = '.json'):\n",
    "    files = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        files += [(dirpath, file) for file in filenames if file.endswith(extension)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:30.601571Z",
     "start_time": "2021-03-13T07:31:30.550444Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEntitiesandRelations(annotationsFile, txtFile):\n",
    "\n",
    "    # Open JSON file with annotations\n",
    "    file                      = open(annotationsFile, encoding = \"utf-8\")\n",
    "    annotationsDoc            = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # Load the annotations\n",
    "    annotations               = json.loads(annotationsDoc)\n",
    "\n",
    "    # Read the matching .txt file\n",
    "    file                      = open(txtFile, encoding = \"utf-8\")\n",
    "    plainText                 = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # tokenize the .txt file\n",
    "    doc                       = nlp_lg(plainText)\n",
    "\n",
    "    # Extract relevant information from JSON files for entities\n",
    "    entities                  = extractEntities(annotations)\n",
    "\n",
    "    # Offset the entities\n",
    "    entities                  = offsetEntities(entities, plainText)\n",
    "\n",
    "    # Add the start token to the dataframe\n",
    "    entities['Start_Token']   = entities.adj_Start.apply(lambda x: get_token_num_for_char(doc, x))\n",
    "\n",
    "    # Add the end token to the dataframe\n",
    "    entities['End_Token']     = [get_token_num_for_end_char(doc, entities.loc[X]['Start_Token'], entities.loc[X]['adj_doctext']) for X in entities.index]\n",
    "\n",
    "    #Create a dictionary for relationship indices\n",
    "    positionDict              = dict(zip(entities.Start_Index, entities.adj_Start))\n",
    "\n",
    "    # Not sure \n",
    "    entities                  = entities[['adj_doctext', 'Start_Token','End_Token','adj_Start', 'adj_End', 'classId']]\n",
    "        \n",
    "    # Note sure\n",
    "    entities.columns          = ['Text', 'Start_Token','End_Token','Start_Index','End_Index','classId']\n",
    "    \n",
    "    # Get rid of the junk rows\n",
    "    if (annotationsFile.split('\\\\')[1] == 'OldSet'):\n",
    "        entities = filterOld(entities)\n",
    "    else:\n",
    "        entities = filterNew(entities)\n",
    "    \n",
    "    # Set that start token\n",
    "    entities['Sentence']      = [doc[entities.loc[X]['Start_Token']].sent for X in entities.index]\n",
    "    \n",
    "    # Find abstract entities\n",
    "    entities                  = findAbstractEntities(entities, doc)\n",
    "    \n",
    "    # Calculate the TFSIF\n",
    "    entities['max_TFISF']     = calculateTFISF(zip(entities.Start_Token, entities.End_Token), doc) \n",
    "    \n",
    "    # I think this is treated as a boolean value, based on what the value os is_sent_start is but I don't know what is_sent_start is\n",
    "    entities['Sent_Start']    = [ # List comprehension\n",
    "                                  1 if   doc[entities.loc[index]['Start_Token']].is_sent_start == True \n",
    "                                  else 0 \n",
    "                                  for index in entities.index\n",
    "                                ]\n",
    "\n",
    "    # Add the name of the document to a document column in the entities dataframe\n",
    "    entities['Document']      = txtFile.split('\\\\')[-1][:-4] \n",
    "    entities['AnnTxtFile']    = txtFile\n",
    "\n",
    "    # Make a lit of columns to downcast\n",
    "    cols_to_downcast          = ['Start_Token', \n",
    "                                 'End_Token', \n",
    "                                 'Start_Index',\n",
    "                                 'End_Index', \n",
    "                                 'inAbstract300', \n",
    "                                 'inAbstract500', \n",
    "                                 'Sent_Start']\n",
    "\n",
    "    # Downcast the columns\n",
    "    entities[cols_to_downcast] = entities[cols_to_downcast].apply(pd.to_numeric, downcast='integer')\n",
    "    \n",
    "    # Create dataframes of annotated species and locations \n",
    "    if (annotationsFile.split('\\\\')[1] == 'OldSet'):\n",
    "        species   = filterSpeciesOld(entities)\n",
    "        locations = filterLocationsOld(entities)\n",
    "    else:\n",
    "        entities  = filterNew(entities)\n",
    "        species   = filterSpeciesNew(entities)\n",
    "        locations = filterLocationsNew(entities)\n",
    "        \n",
    "    species.drop(columns   = ['classId'])  \n",
    "    locations.drop(columns = ['classId'])\n",
    "      \n",
    "    # Extract the relationships from the Annotations file\n",
    "    relationships              = extractRelationships(annotations)\n",
    "        \n",
    "    #Rearrange order of entities so species always appears on left hand side of the pair \n",
    "    relationships[['entity1-type','entity1-start','entity1-end','entity2-type','entity2-start','entity2-end']] = relationships[['entity1-type','entity1-start','entity1-end','entity2-type','entity2-start','entity2-end']].mask(\n",
    "        relationships['entity1-type'] == 'location', relationships[['entity2-type','entity2-start','entity2-end','entity1-type','entity1-start','entity1-end']].values)\n",
    "\n",
    "    # Adjust the relationships indices\n",
    "    relationships = adjustRelationshipEntityIndices(relationships, positionDict)\n",
    "\n",
    "    # Insert names into dataframe\n",
    "    relationships = insertEntityNames(relationships, plainText)\n",
    "\n",
    "    # Insert token positions to the dataframe\n",
    "    relationships = insertTokenNumbers(relationships, doc)\n",
    "\n",
    "    # Add Ground Truth Value for predictions\n",
    "    relationships['Tagged_Relationship'] = 1\n",
    "\n",
    "    # Tidy the whole thing up\n",
    "    relationships.drop(columns = ['entity1-class',\n",
    "                                  'entity2-class',\n",
    "                                  'entity1-type', \n",
    "                                  'entity2-type',\n",
    "                                  'entity1-start',\n",
    "                                  'entity1-end',\n",
    "                                  'entity2-start',\n",
    "                                  'entity2-end'], inplace = True)\n",
    "\n",
    "    relationships.columns      = ['Start_Index_Species', \n",
    "                                  'End_Index_Species',\n",
    "                                  'Start_Index_Location', \n",
    "                                  'End_Index_Location', \n",
    "                                  'Species', \n",
    "                                  'Location', \n",
    "                                  'Start_Token_Species',\n",
    "                                  'End_Token_Species',\n",
    "                                  'Start_Token_Location',\n",
    "                                  'End_Token_Location',\n",
    "                                  'Tagged_Relationship']\n",
    "\n",
    "    relationships               = relationships[['Species', \n",
    "                                   'Start_Token_Species',\n",
    "                                   'End_Token_Species',\n",
    "                                   'Start_Index_Species', \n",
    "                                   'End_Index_Species', \n",
    "                                   'Location',\n",
    "                                   'Start_Token_Location',\n",
    "                                   'End_Token_Location',\n",
    "                                   'Start_Index_Location', \n",
    "                                   'End_Index_Location',\n",
    "                                   'Tagged_Relationship']]\n",
    "\n",
    "    # return three dataframes\n",
    "    return species, locations, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:33.097271Z",
     "start_time": "2021-03-13T07:31:33.091849Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateTFISF(tokens, doc):\n",
    "    results       = []\n",
    "    numSentsinDoc = len(list(doc.sents))\n",
    "    \n",
    "    for startToken,endToken in tokens:\n",
    "        maxtfisf = 0\n",
    "        for tkn in range(startToken,endToken):\n",
    "            if doc[tkn].is_alpha:\n",
    "                tf       = len([1 for word in doc[tkn].sent if doc[tkn].text == str(word)])\n",
    "                isf      = np.log(numSentsinDoc / len([1 for sent in doc.sents if doc[tkn].text in str(sent)]))\n",
    "                tfisf    = tf*isf\n",
    "                maxtfisf = max(maxtfisf, tfisf)\n",
    "        results.append(maxtfisf)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:34.221396Z",
     "start_time": "2021-03-13T07:31:34.217779Z"
    }
   },
   "outputs": [],
   "source": [
    "def findAbstractStart(doc):\n",
    "    matcher = Matcher(nlp_lg.vocab)\n",
    "    pattern = [\n",
    "        [{\"LOWER\": \"abstract\"}]\n",
    "    ]\n",
    "    matcher.add(\"abstract_pattern\", pattern)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    if len(matches) >0:\n",
    "        AbstractToken = matches[0][1]\n",
    "    else: AbstractToken = -1\n",
    "        \n",
    "    return AbstractToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:35.032921Z",
     "start_time": "2021-03-13T07:31:35.027214Z"
    }
   },
   "outputs": [],
   "source": [
    "def inAbstract(tokens, AbstractStart, doc, limit):\n",
    "    if AbstractStart == -1:\n",
    "        return np.zeros(len(list(tokens)))\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    for startToken,endToken in tokens:\n",
    "        matcher = Matcher(nlp_lg.vocab)\n",
    "        pattern = []\n",
    "        \n",
    "        for tkn in range (startToken,endToken):\n",
    "            pattern.append({\"TEXT\": doc[tkn].text})\n",
    "            \n",
    "        patterns = [pattern]\n",
    "        matcher.add(\"species_pattern\", patterns)\n",
    "        matches  = matcher(doc)\n",
    "        idx      = [match[1] for match in matches if match[1] in range(AbstractStart,AbstractStart+limit)]\n",
    "        \n",
    "        if len(idx)>0:\n",
    "            results.append(1)\n",
    "        else: results.append(0)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:36.114104Z",
     "start_time": "2021-03-13T07:31:36.110547Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_num_for_char(doc, start_idx):\n",
    "    for i, token in enumerate(doc):\n",
    "        if start_idx > token.idx:\n",
    "            continue\n",
    "        if start_idx == token.idx:\n",
    "            return i\n",
    "        if start_idx < token.idx:\n",
    "            return i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:37.798619Z",
     "start_time": "2021-03-13T07:31:37.794946Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_num_for_end_char(doc, start_token, text):\n",
    "    end_token = start_token+1\n",
    "    span      = doc[start_token:end_token]\n",
    "    \n",
    "    while len(span.text)< len(text):\n",
    "        end_token += 1\n",
    "        span       = doc[start_token:end_token]\n",
    "        \n",
    "    return end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts entities from the parsed annotations JSON file\n",
    "# Must be passed the annotations in parsed JSON format\n",
    "def extractEntities(annotations):\n",
    "    \n",
    "    # Create data frame from Entities in JSON\n",
    "    entities                  = pd.DataFrame(annotations['entities'], columns = ['classId', 'offsets'])\n",
    "    \n",
    "    # Define columns in the dataframe\n",
    "    entities['Start_Index']   = [X[0].get('start') for X in entities['offsets']]\n",
    "    entities['Text']          = [X[0].get('text') for X in entities['offsets']]\n",
    "    entities['End_Index']     = [len(X[0].get('text'))+X[0].get('start') for X in entities['offsets']]\n",
    "    entities                  = entities[['Text', 'Start_Index', 'End_Index','classId']]\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off set entities \n",
    "def offsetEntities(entities, plainText):\n",
    "\n",
    "    # Offset the entities is a bit convoluted so I heavily commented it\n",
    "    # TagTog outputs its txt files as html. The txt is split into <pre>'s. \n",
    "    # The annotations file's start index keeps track of the character count from the start of the <pre>, \n",
    "    # Not the start of the txt, as is the way we calculate it\n",
    "    # Subsequently, each new pre resets the start index from 0. \n",
    "    # Because we work from the start of the txt file, we have to keep track of the last start/end index and work from there\n",
    "\n",
    "    # First, we need to add the working columns to the dataframe\n",
    "    entities['newStartIndex'] = 0\n",
    "    entities['newEndIndex']   = 0\n",
    "    entities['lastListedSI']  = 0\n",
    "    entities['offsetR']       = 0\n",
    "    entities['offsetL']       = 0\n",
    "    entities['adj_Start']     = 0\n",
    "    entities['adj_End']       = 0\n",
    "    entities['adj_doctext']   = \"\"\n",
    "\n",
    "    # We also need to keep track of the last items Start Index\n",
    "    lastSI                    = 0 # Stored per index\n",
    "    startIndex                = 0 # Calculated per index\n",
    "    lastListedSI              = 0 # Required for calculations\n",
    "\n",
    "    # And the last items End Index\n",
    "    lastEI                    = 0\n",
    "    endIndex                  = 0\n",
    "\n",
    "    # This boolean lets us know if we've gone past the first <pre>\n",
    "    reset                     = False\n",
    "\n",
    "    # Iterate through the entities\n",
    "    for index in entities.index:\n",
    "\n",
    "        # Check if the start index has reset\n",
    "        listedSI = entities.at[index, 'Start_Index']\n",
    "\n",
    "        # Most of the entries for longer pdfs will be outside of the first <pre> so we start with the reset check as\n",
    "        # it will speed up the process\n",
    "        if reset == True:\n",
    "\n",
    "            # If it's reset but we've reached a new <pre> we don't need to do anything fancy\n",
    "            if listedSI < lastListedSI:\n",
    "\n",
    "                # So we add the set startIndex to the last known start index (stored in lastSI)\n",
    "                startIndex = lastSI + listedSI \n",
    "\n",
    "            # Otherwise...\n",
    "            else:\n",
    "\n",
    "                # We need to manipulate the startIndex to find the new accurate start index in the text document\n",
    "                startIndex = listedSI - lastListedSI + lastSI\n",
    "\n",
    "        # If we are still in the first <pre> we need to check for the first entry of the next <pre>\n",
    "        elif listedSI < lastSI:\n",
    "\n",
    "            # We've now entered the next <pre> so we add the listed startIndex to the last known start index (stored in lastSI)\n",
    "            startIndex = lastSI + listedSI \n",
    "\n",
    "            # And set the reset flag as true to indicate we have gone past the first pre\n",
    "            reset      = True\n",
    "\n",
    "        # Otherwise\n",
    "        else:\n",
    "\n",
    "            # We are still in the first <pre> so we just use the accurate index\n",
    "            startIndex = listedSI\n",
    "\n",
    "        # Here we keep track of the index variables needed on the next iteration\n",
    "        lastSI       = startIndex\n",
    "        lastListedSI = listedSI\n",
    "        endIndex     = startIndex + len(entities.at[index, 'Text']) \n",
    "        lastEI       = endIndex\n",
    "\n",
    "        # Now the offset work begins\n",
    "        # Find the offset on the right\n",
    "        offsetR = startIndex - plainText[:endIndex].rfind(entities.loc[index]['Text'])\n",
    "\n",
    "        # Set it to a valid high integer if it went off the text\n",
    "        if offsetR == -1:\n",
    "\n",
    "            entities.at[index, 'offsetR'] = 99999999\n",
    "\n",
    "        else:\n",
    "\n",
    "            entities.at[index, 'offsetR'] = offsetR        \n",
    "\n",
    "        # Find the offset on the left\n",
    "        offsetL = plainText[startIndex:].find(entities.loc[index]['Text'])\n",
    "\n",
    "        # Set it to a valid high integer if it went off the text\n",
    "        if offsetL == -1:\n",
    "\n",
    "            entities.at[index, 'offsetL'] = 99999999\n",
    "\n",
    "        else:\n",
    "            entities.at[index, 'offsetL'] = offsetL\n",
    "\n",
    "        # Check which side we need to offset on and do it\n",
    "        if offsetR < offsetL:\n",
    "\n",
    "            # Adjust it left \n",
    "            adjustedStart = startIndex - offsetR\n",
    "\n",
    "        else: \n",
    "\n",
    "            # Adjust it right\n",
    "            adjustedStart = startIndex + offsetL\n",
    "\n",
    "        # Set the adjusted start\n",
    "        entities.at[index, 'adj_Start']      = adjustedStart\n",
    "\n",
    "        # Set the adjusted end\n",
    "        adjustedEnd                          = endIndex - startIndex + adjustedStart\n",
    "        entities.at[index, 'adj_End']        = adjustedEnd\n",
    "\n",
    "        # Set the adjusted text\n",
    "        entities.at[index, 'adj_doctext']    = plainText[adjustedStart: adjustedEnd]\n",
    "\n",
    "    # Return the data frame\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only the relevant TagTog annotations in the old data\n",
    "# We need to find a way to standardise the annotations both between trees but also between projects\n",
    "def filterNew(entities):\n",
    "    return entities[(entities.classId == 'e_1' )    #Bryophytes\n",
    "                  | (entities.classId == 'e_2' )    #Nematoda\n",
    "                  | (entities.classId == 'e_4' )    #Location\n",
    "                  | (entities.classId == 'e_41')    #Cyanobacteria\n",
    "                  | (entities.classId == 'e_49')    #Rotifers\n",
    "                  | (entities.classId == 'e_75')    #Algae\n",
    "                  | (entities.classId == 'e_38')    #Acari\n",
    "                  | (entities.classId == 'e_55')    #Cyanobacteria\n",
    "                  | (entities.classId == 'e_43')    #Mites\n",
    "                  | (entities.classId == 'e_32')    #Moss\n",
    "                  | (entities.classId == 'e_53')    #Rotier\n",
    "                  | (entities.classId == 'e_54')    #Protist\n",
    "                  | (entities.classId == 'e_57')    #Micro_Molecular\n",
    "                  | (entities.classId == 'e_7' )    #Lichen\n",
    "                  | (entities.classId == 'e_33')    #Nematode\n",
    "                  | (entities.classId == 'e_44')    #SpingTail\n",
    "                  | (entities.classId == 'e_48')    #Tardigrades\n",
    "                  | (entities.classId == 'e_39')    #Collembola\n",
    "                  | (entities.classId == 'e_40')    #Algae\n",
    "                  | (entities.classId == 'e_51')    #Tardigrade\n",
    "                  | (entities.classId == 'e_31')    #Lichen\n",
    "                  | (entities.classId == 'e_50')]   #Protists   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as species in the new documents\n",
    "def filterSpeciesNew(entities):\n",
    "    return entities[(entities.classId != 'e_4' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as locations in the new documents\n",
    "def filterLocationsNew(entities):\n",
    "    return entities[(entities.classId == 'e_4' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as species in the old documents\n",
    "def filterSpeciesOld(entities):\n",
    "    return entities[(entities.classId == 'e_1' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as locations in the old documents\n",
    "def filterLocationsOld(entities):\n",
    "    return entities[(entities.classId == 'e_2' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAbstractEntities(entities, doc):\n",
    "    \n",
    "    # Get the abstract\n",
    "    AbstractStart   = findAbstractStart(doc)\n",
    "\n",
    "    # Make two lists of entities within the abstract\n",
    "    entities['inAbstract300'] = inAbstract(zip(entities.Start_Token, entities.End_Token), AbstractStart, doc, 300) \n",
    "    entities['inAbstract500'] = inAbstract(zip(entities.Start_Token, entities.End_Token), AbstractStart, doc, 500) \n",
    "    \n",
    "    # Return\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from JSON file for relationships\n",
    "# Requires you to pass in the parsed annotations\n",
    "def extractRelationships(annotations):\n",
    "\n",
    "    # Create a data frame using current data\n",
    "    relationships                  = pd.DataFrame(annotations['relations'], columns = ['entities'])\n",
    "\n",
    "    # Add desire columns to the data frame\n",
    "    # for entity one\n",
    "    relationships['entity1-class']  = \"\"\n",
    "    relationships['entity1-type']   = \"\"\n",
    "    relationships['entity1-start']  = 0\n",
    "    relationships['entity1-end']    = 0\n",
    "\n",
    "    # for entity two\n",
    "    relationships['entity2-class']  = \"\"\n",
    "    relationships['entity2-type']   = \"\"\n",
    "    relationships['entity2-start']  = 0\n",
    "    relationships['entity2-end']    = 0\n",
    "\n",
    "    # Iterate through the current data\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Extract entities\n",
    "        entityOne, entityTwo                         = relationships.at[index, 'entities']\n",
    "\n",
    "        # Extract relevant details\n",
    "        # Entity one\n",
    "        throwAway, entityOneClass, entityOneStartEnd = entityOne.split('|')\n",
    "        entityOneStart, entityOneEnd                 = entityOneStartEnd.split(',')\n",
    "\n",
    "        relationships.at[index, 'entity1-class']     = entityOneClass\n",
    "        relationships.at[index, 'entity1-type']      = \"location\" if entityOneClass == 'e_4' else \"species\"\n",
    "        relationships.at[index, 'entity1-start']     = entityOneStart\n",
    "        relationships.at[index, 'entity1-end']       = entityOneEnd\n",
    "\n",
    "        # Entity two\n",
    "        throwAway, entityTwoClass, entityTwoStartEnd = entityTwo.split('|')\n",
    "        entityTwoStart, entityTwoEnd                 = entityTwoStartEnd.split(',')\n",
    "\n",
    "        relationships.at[index, 'entity2-class']     = entityTwoClass \n",
    "        relationships.at[index, 'entity2-type']      = \"location\" if entityTwoClass == 'e_4' else \"species\"\n",
    "        relationships.at[index, 'entity2-start']     = entityTwoStart\n",
    "        relationships.at[index, 'entity2-end']       = entityTwoEnd\n",
    "\n",
    "    # Dopr entities column    \n",
    "    relationships.drop(columns = 'entities', inplace=True)\n",
    "    \n",
    "    # Return\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert actual text of the entities into the relationships dataframe\n",
    "def insertEntityNames(relationships, document):\n",
    "\n",
    "    # Iterate through the relationships\n",
    "    for index in relationships.index:\n",
    "        \n",
    "        # Entity One\n",
    "        relationships.at[index, 'entity1'] = document[relationships.at[index, 'entity1-adjstart']: \n",
    "                                                      relationships.at[index, 'entity1-adjend']] \n",
    "\n",
    "        # Entity Two\n",
    "        relationships.at[index, 'entity2'] = document[relationships.at[index, 'entity2-adjstart']: \n",
    "                                                      relationships.at[index, 'entity2-adjend']] \n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust entity indices to match document\n",
    "def adjustRelationshipEntityIndices(relationships, positionDict):\n",
    "\n",
    "    # Create the desired columns\n",
    "    relationships['entity1-adjstart'] = 0\n",
    "    relationships['entity1-adjend']   = 0\n",
    "    relationships['entity2-adjstart'] = 0\n",
    "    relationships['entity2-adjend']   = 0\n",
    "\n",
    "    # Iterate through relationships\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Get adjusted start for Entity One\n",
    "        relationships.at[index, 'entity1-adjstart'] = positionDict.get(relationships.at[index, 'entity1-start'])\n",
    "\n",
    "        # Calculate adjusted end for Entity Two\n",
    "        entity1End    = relationships.at[index, 'entity1-end']\n",
    "        entity1Start  = relationships.at[index, 'entity1-start']\n",
    "        entity1AdjEnd = relationships.at[index, 'entity1-adjstart']\n",
    "        relationships.at[index, 'entity1-adjend']   =  entity1End - entity1Start + entity1AdjEnd\n",
    "\n",
    "        # Get adjusted start for Entity One\n",
    "        relationships.at[index, 'entity2-adjstart'] = positionDict.get(relationships.at[index, 'entity2-start'])\n",
    "\n",
    "        # Calculate adjusted end for Entity Two\n",
    "        entity2End    = relationships.at[index, 'entity2-end']\n",
    "        entity2Start  = relationships.at[index, 'entity2-start']\n",
    "        entity2AdjEnd = relationships.at[index, 'entity2-adjstart']\n",
    "        relationships.at[index, 'entity2-adjend']   =  entity2End - entity2Start + entity2AdjEnd\n",
    "        \n",
    "    # Return\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add token numbers to the relationships tables\n",
    "def insertTokenNumbers(relationships, doc):\n",
    "\n",
    "    # Add desires columns\n",
    "    relationships['Start_Token_Species'] = 0\n",
    "    relationships['End_Token_Species']   = 0\n",
    "\n",
    "    # Iterate through the relationships\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Get the starting tokens\n",
    "        # Species\n",
    "        tokenNum = relationships.at[index, 'entity1-adjstart']\n",
    "        relationships.at[index, 'Start_Token_Species']  = get_token_num_for_char(doc, tokenNum)\n",
    "\n",
    "        # Location\n",
    "        tokenNum = relationships.at[index, 'entity2-adjstart']\n",
    "        relationships.at[index, 'Start_Token_Location'] = get_token_num_for_char(doc, tokenNum)\n",
    "\n",
    "        # Get end tokens\n",
    "        # Species\n",
    "        relationships.at[index, 'End_Token_Species']    = get_token_num_for_end_char(\n",
    "             doc, relationships.at[index, 'Start_Token_Species'], relationships.at[index, 'entity1'])\n",
    "\n",
    "        # Location\n",
    "        relationships.at[index, 'End_Token_Location']    = get_token_num_for_end_char(\n",
    "         doc, relationships.at[index, 'Start_Token_Location'], relationships.at[index, 'entity2'])\n",
    "        \n",
    "    # Return relationships\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the Extracted CSV folder and find the CSV\n",
    "def findExtractedCSV(txtFile, CSVFolder, fileTag):\n",
    "    \n",
    "    # Check if we are using oldset or new set\n",
    "    # Then get the CSV name\n",
    "    if txtFile.split('\\\\')[1] == 'OldSetTxt':\n",
    "        speciesCSVFileName = re.sub('.txt.txt', fileTag, txtFile)\n",
    "    else:\n",
    "        speciesCSVFileName = re.sub('.pdf.txt', fileTag, txtFile)\n",
    "    speciesCSVFileName     = speciesCSVFileName.split('\\\\')[-1]\n",
    "    \n",
    "    # Iterate through the folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(CSVFolder):\n",
    "        for filename in filenames:\n",
    "            if (filename == speciesCSVFileName):\n",
    "                return True, os.path.join(dirpath, filename)\n",
    "        \n",
    "    # return -1 on fail\n",
    "    return False, speciesCSVFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeciesMetrics(speciesMatchesDF):\n",
    "    Document       = []\n",
    "    Tagged         = []\n",
    "    Extracted      = []\n",
    "    TruePositive   = []\n",
    "    FalsePositive  = []\n",
    "    FalseNegative  = []\n",
    "    Precision      = []\n",
    "    Recall         = []\n",
    "    F1             = []\n",
    "    partialMatches = []\n",
    "\n",
    "    for doc in speciesMatchesDF.Document.unique():\n",
    "        \n",
    "        Document.append(doc)\n",
    "        \n",
    "        ActualResults    = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Text'].notna())])\n",
    "        \n",
    "        PredictedResults = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Found_as'].notna())])\n",
    "        \n",
    "        Tagged.append(ActualResults)\n",
    "        \n",
    "        Extracted.append(PredictedResults)\n",
    "        \n",
    "        TP               = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Found_as'].notna()) & (speciesMatchesDF['Text'].notna())])\n",
    "        \n",
    "        TruePositive.append(TP)\n",
    "        \n",
    "        FalsePositive.append(len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].isna()]))\n",
    "        \n",
    "        FalseNegative.append(len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& speciesMatchesDF['Found_as'].isna() & speciesMatchesDF['Text'].notna()]))\n",
    "        \n",
    "        notTagged        = speciesMatchesDF[(speciesMatchesDF.Document ==doc) & speciesMatchesDF.Text.isna()]\n",
    "        \n",
    "        notExtracted     = speciesMatchesDF[(speciesMatchesDF.Document==doc) & speciesMatchesDF.Found_as.isna()]\n",
    "        \n",
    "        closeMatches     = 0\n",
    "        \n",
    "        for i in notTagged.index:\n",
    "            \n",
    "            for j in notExtracted.index:\n",
    "                \n",
    "                if notTagged.loc[i]['Start_Index']>=notExtracted.loc[j]['Start_Index'] and notTagged.loc[i]['End_Index']<=notExtracted.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "                    \n",
    "        for i in notExtracted.index:\n",
    "            \n",
    "            for j in notTagged.index:\n",
    "                \n",
    "                if notExtracted.loc[i]['Start_Index']>=notTagged.loc[j]['Start_Index'] and notExtracted.loc[i]['End_Index']<=notTagged.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "                    \n",
    "        partialMatches.append(closeMatches)\n",
    "\n",
    "    Document.append('Total')\n",
    "    \n",
    "    ActualResults    = len(speciesMatchesDF[speciesMatchesDF['Text'].notna()])\n",
    "    \n",
    "    PredictedResults = len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna()])\n",
    "    \n",
    "    Tagged.append(ActualResults)\n",
    "    \n",
    "    Extracted.append(PredictedResults)\n",
    "    \n",
    "    TP               =len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].notna()])\n",
    "    \n",
    "    TruePositive.append(TP)\n",
    "    \n",
    "    FalsePositive.append(len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].isna()]))\n",
    "    \n",
    "    FalseNegative.append(len(speciesMatchesDF[speciesMatchesDF['Found_as'].isna() & speciesMatchesDF['Text'].notna()]))\n",
    "    \n",
    "    partialMatches.append(sum(partialMatches))\n",
    "\n",
    "    results                   = pd.DataFrame({'Document': Document, 'Tagged':Tagged, 'Extracted':Extracted, 'True Positives':TruePositive,\n",
    "                                              'False Positives':FalsePositive, 'False Negatives':FalseNegative, 'Partial Matches':partialMatches,\n",
    "                                             })\n",
    "    \n",
    "    results['Precision']      = np.round(results['True Positives']/results['Extracted'], 4)\n",
    "    \n",
    "    results['Recall']         = np.round(results['True Positives']/results['Tagged'], 4)\n",
    "    \n",
    "    results['F1']             = np.round(2* ((results['Precision']*results['Recall'])/(results['Precision']+results['Recall'])), 4)\n",
    "    \n",
    "    results['Adj. Precision'] = np.round((results['True Positives']+results['Partial Matches'])/results['Extracted'], 4)\n",
    "    \n",
    "    results['Adj. Recall']    = np.round((results['True Positives']+results['Partial Matches'])/results['Tagged'], 4)\n",
    "    \n",
    "    results['Adj. F1']        = np.round(2* ((results['Adj. Precision']*results['Adj. Recall'])/(results['Adj. Precision']+results['Adj. Recall'])), 4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocationsMetrics(locationsMatchesDF):\n",
    "    \n",
    "    Document       = []\n",
    "    Tagged         = []\n",
    "    Extracted      = []\n",
    "    TruePositive   = []\n",
    "    FalsePositive  = []\n",
    "    FalseNegative  = []\n",
    "    Precision      = []\n",
    "    Recall         = []\n",
    "    F1             = []\n",
    "    partialMatches = []\n",
    "\n",
    "    for doc in locationsMatchesDF.Document.unique():\n",
    "        \n",
    "        Document.append(doc)\n",
    "        \n",
    "        ActualResults    = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['TaggedLocation'].notna())])\n",
    "        \n",
    "        PredictedResults = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['ExtractedLocation'].notna())])\n",
    "        \n",
    "        Tagged.append(ActualResults)\n",
    "        \n",
    "        Extracted.append(PredictedResults)\n",
    "        \n",
    "        TP               = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['ExtractedLocation'].notna()) & (locationsMatchesDF['TaggedLocation'].notna())])\n",
    "        \n",
    "        TruePositive.append(TP)\n",
    "        \n",
    "        FalsePositive.append(len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].isna()]))\n",
    "        \n",
    "        FalseNegative.append(len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& locationsMatchesDF['ExtractedLocation'].isna() & locationsMatchesDF['TaggedLocation'].notna()]))\n",
    "        \n",
    "        notTagged        = locationsMatchesDF[(locationsMatchesDF.Document ==doc) & locationsMatchesDF.TaggedLocation.isna()]\n",
    "        \n",
    "        notExtracted     = locationsMatchesDF[(locationsMatchesDF.Document==doc) & locationsMatchesDF.ExtractedLocation.isna()]\n",
    "        \n",
    "        closeMatches     = 0\n",
    "        \n",
    "        for i in notTagged.index:\n",
    "        \n",
    "            for j in notExtracted.index:\n",
    "        \n",
    "                if notTagged.loc[i]['Start_Index']>=notExtracted.loc[j]['Start_Index'] and notTagged.loc[i]['End_Index']<=notExtracted.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "        \n",
    "        for i in notExtracted.index:\n",
    "        \n",
    "            for j in notTagged.index:\n",
    "        \n",
    "                if notExtracted.loc[i]['Start_Index']>=notTagged.loc[j]['Start_Index'] and notExtracted.loc[i]['End_Index']<=notTagged.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "        \n",
    "        partialMatches.append(closeMatches)\n",
    "\n",
    "    Document.append('Total')\n",
    "    \n",
    "    ActualResults    = len(locationsMatchesDF[locationsMatchesDF['TaggedLocation'].notna()])\n",
    "    \n",
    "    PredictedResults = len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna()])\n",
    "    \n",
    "    Tagged.append(ActualResults)\n",
    "    \n",
    "    Extracted.append(PredictedResults)\n",
    "    \n",
    "    TP               = len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].notna()])\n",
    "    \n",
    "    TruePositive.append(TP)\n",
    "    \n",
    "    FalsePositive.append(len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].isna()]))\n",
    "    \n",
    "    FalseNegative.append(len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].isna() & locationsMatchesDF['TaggedLocation'].notna()]))\n",
    "    \n",
    "    partialMatches.append(sum(partialMatches))\n",
    "\n",
    "    results                   = pd.DataFrame({'Document': Document, 'Tagged':Tagged, 'Extracted':Extracted, 'True Positives':TruePositive,\n",
    "                                              'False Positives':FalsePositive, 'False Negatives':FalseNegative, 'Partial Matches':partialMatches,\n",
    "                                             })\n",
    "    \n",
    "    results['Precision']      = np.round(results['True Positives']/results['Extracted'], 4)\n",
    "    \n",
    "    results['Recall']         = np.round(results['True Positives']/results['Tagged'], 4)\n",
    "    \n",
    "    results['F1']             = np.round(2* ((results['Precision']*results['Recall'])/(results['Precision']+results['Recall'])), 4)\n",
    "    \n",
    "    results['Adj. Precision'] = np.round((results['True Positives']+results['Partial Matches'])/results['Extracted'], 4)\n",
    "    \n",
    "    results['Adj. Recall']    = np.round((results['True Positives']+results['Partial Matches'])/results['Tagged'], 4)\n",
    "    \n",
    "    results['Adj. F1']        = np.round(2* ((results['Adj. Precision']*results['Adj. Recall'])/(results['Adj. Precision']+results['Adj. Recall'])), 4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_taggedlocations_df(df, gazetters, filePairs):\n",
    "    places           = []\n",
    "\n",
    "    # Gazetter separation\n",
    "    nzGaz            = gazetters[0]\n",
    "    nzGazAntarctica  = gazetters[1]\n",
    "    scarGlobalNames  = gazetters[2]\n",
    "    scarNzNames      = gazetters[3]\n",
    "    geoNamesAnt      = gazetters[4]\n",
    "    geoNamesNZ       = gazetters[5]\n",
    "    geoNamesFiltered = gazetters[6]\n",
    "    \n",
    "    # Create columns on the dataframes\n",
    "    data               = []\n",
    "    columns            = ['id', 'Location']\n",
    "    toConCat           = pd.DataFrame(data=data,columns=columns)\n",
    "    df                 = pd.concat([df,toConCat], axis=1)\n",
    "\n",
    "    # We need to do a stupid iteration here because the list comprehension doesn't grab all the rows\n",
    "    # Iterate through locations\n",
    "    \n",
    "    i = 0\n",
    "    for row in df.index:\n",
    "        \n",
    "        # Give them all a unique id to later be used for indexing\n",
    "        df.at[row, 'id'] = i\n",
    "        i += 1\n",
    "        \n",
    "        # Set some booleans first up\n",
    "        df.at[row, 'inNZ']          = False\n",
    "        df.at[row, 'inAntarctica']  = False\n",
    "        df.at[row, 'exactMatch']    = False\n",
    "            \n",
    "        # Get location and leading four letters\n",
    "        location = str(df.loc[row, 'TaggedLocation'])\n",
    "        \n",
    "        # Add place but drop leading words 'The'/'the' and replace leading words 'Mt.'/'mt' with 'Mount'\n",
    "        if location.startswith('The '):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "            \n",
    "        elif location.startswith('the '):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "            \n",
    "        elif location.startswith('Mt.'):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "        \n",
    "        elif location.startswith('Mt '):\n",
    "            df.at[row, 'Location'] = location[3:]\n",
    "\n",
    "        # Check possible locations in gazetteers for exact matches\n",
    "        if location in nzGazAntarctica:\n",
    "            df.at[row, 'NZGazAnt']    = True\n",
    "            df.at[row, 'inNZ'    ]    = True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "        else:\n",
    "            df.at[row, 'NZGazAnt']    = False\n",
    "            \n",
    "        if location in nzGaz:\n",
    "            df.at[row, 'NZGaz']       = True\n",
    "            df.at[row, 'inNZ' ]       = True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'NZGaz']       = False\n",
    "            \n",
    "        if location in scarNzNames:\n",
    "            df.at[row, 'ScarNZ']      = True\n",
    "        else:\n",
    "            df.at[row, 'ScarNZ']      = False\n",
    "                 \n",
    "        if location in scarGlobalNames:\n",
    "            df.at[row, 'ScarGlobal'  ]= True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'ScarGlobal']  = False\n",
    "\n",
    "        if location in geoNamesNZ:\n",
    "            df.at[row, 'GeoNamesNZ']  = True\n",
    "            df.at[row, 'inNZ'      ]  = True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "        else:\n",
    "            df.at[row, 'GeoNamesNZ']  = False\n",
    "                 \n",
    "        if location in geoNamesAnt:\n",
    "            df.at[row, 'GeoNamesAnt'] = True\n",
    "        else:\n",
    "            df.at[row, 'GeoNamesAnt'] = False\n",
    "                 \n",
    "        if location in geoNamesFiltered:\n",
    "            df.at[row, 'GeoNames']    = True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'GeoNames']    = False\n",
    "\n",
    "    # filter locations by those not found in Antarctica or New Zealand gazeteers \n",
    "    df2                = df[(df['inAntarctica'] == False) & (df['inNZ']== False)].copy()\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # look through these remaining locations (including those found only in GeoNames) for close matches \n",
    "    # eg. McMurdo Dry Valley v McMurdo Dry Valley or for partial matches eg. Ross Sea Region == Ross Sea\n",
    "    for row in df2.index:\n",
    "        \n",
    "        # This section is for close matches\n",
    "        # Set to not found\n",
    "        df2.at[row, 'Close_Match'] = False\n",
    "        \n",
    "        # Then iterate for close matches on the location as a string\n",
    "        location = str(df2.loc[row, 'Location'])\n",
    "            \n",
    "        # In Antarctica\n",
    "        matches = difflib.get_close_matches(location, nzGazAntarctica, cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_NZGazAnt']   = matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "            \n",
    "        # In NZ\n",
    "        matches = difflib.get_close_matches(location, nzGaz,           cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_NZGaz']      = matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "        \n",
    "        # In SCARNZ\n",
    "        matches = difflib.get_close_matches(location, scarNzNames,     cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_ScarNZ']     = matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    " \n",
    "        # In ScarGlobal\n",
    "        matches = difflib.get_close_matches(location, scarGlobalNames, cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_ScarGlobal'] = matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "    \n",
    "        # In GeoNamesNZ\n",
    "        matches = difflib.get_close_matches(location, geoNamesNZ,     cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_GeoNamesNZ'] = matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "            \n",
    "        # In GeoNamesAntarctica\n",
    "        matches = difflib.get_close_matches(location, geoNamesAnt,    cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_GeoNamesAnt'] = matches[0] \n",
    "            df2.at[row, 'Close_Match']             = True\n",
    "    \n",
    "        # This section is for partial matches\n",
    "        # Set the not found boolean\n",
    "        df2.at[row, 'PartialMatch'] = False\n",
    "        \n",
    "        # Get the tokenised document for that row\n",
    "        fileString = str(df2.loc[row, 'Document'])\n",
    "        doc        = getTokenisedDocument(fileString.replace('.txt', ''), filePairs)\n",
    "    \n",
    "        # Get the location, start and end tokens\n",
    "        startToken = df2.loc[row, 'Start_Token']\n",
    "        endToken   = df2.loc[row, 'End_Token']\n",
    "        \n",
    "        # Check the partial matches\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, nzGazAntartica)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_NZGazAnt']   = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc, startToken, endToken, nzGaz)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_NZGaz']      = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "            \n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, scarNzNames)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_ScarNZ']     = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, scarGlobalNames)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_ScarGlobal'] = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "            \n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, geoNamesNZ)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_GeoNamesNZ'] = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, geoNamesAnt)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_GeoNamesAnt'] = result\n",
    "            df2.at[row, 'PartialMatch']            = True\n",
    "            \n",
    "    # merge the filtered dataframe (now with close and partial matches) with the unfiltered dataframe\n",
    "    df_unified                 = df.merge(df2, how = 'left')\n",
    "    \n",
    "    # redo the inNZ,inAntarctica and Found columns to include close and partial matches\n",
    "    df_unified.drop(columns    = ['inNZ', 'inAntarctica'], inplace = True)\n",
    "    \n",
    "    df_unified['inNZ']         = (df_unified.NZGaz \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesNZ \n",
    "                                | df_unified.Close_Match_NZGaz \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesNZ \n",
    "                                | df_unified.PartialMatch_NZGaz \n",
    "                                | df_unified.PartialMatch_NZGazAnt \n",
    "                                | df_unified.PartialMatch_GeoNamesNZ)\n",
    "    \n",
    "    df_unified['inAntarctica'] = (df_unified.ScarGlobal \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesAnt \n",
    "                                | df_unified.Close_Match_ScarGlobal \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesAnt \n",
    "                                | df_unified.PartialMatch_ScarGlobal \n",
    "                                | df_unified.PartialMatch_NZGazAnt \n",
    "                                | df_unified.PartialMatch_GeoNamesAnt)\n",
    "    \n",
    "    df_unified['Found']        = (df_unified.exactMatch \n",
    "                                | df_unified.Close_Match\n",
    "                                | df_unified.PartialMatch)\n",
    "    \n",
    "    return df_unified[df_unified.Found==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the annotated text file and tokenises it\n",
    "def getTokenisedDocument(document, filePairs):\n",
    "\n",
    "    # Find text file in filePairs\n",
    "    document  = filterFilePairs(fileCount, document, filePairs)\n",
    "    \n",
    "    # Filter to .txt\n",
    "    txtFile   = [file for file in filePair if file.endswith('.txt')][0]\n",
    "    \n",
    "    # Open .txt\n",
    "    file      = open(txtFile, encoding = \"utf-8\")\n",
    "    plainText = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    # tokenize the .txt file\n",
    "    doc       = nlp_lg(plainText)\n",
    "    \n",
    "    # Return\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestSubStringMatch(document, fullStart, fullEnd, gazetteer):\n",
    "       \n",
    "    subLength = fullEnd - fullStart - 1\n",
    "\n",
    "    while subLength > 1:\n",
    "\n",
    "        subStart = fullStart\n",
    "\n",
    "        subEnd   = fullStart + subLength\n",
    "\n",
    "        while subEnd <= fullEnd:\n",
    "\n",
    "            subString = document[subStart:subEnd].text\n",
    "\n",
    "            if subString in gazetteer:\n",
    "\n",
    "                return subString\n",
    "\n",
    "            subStart += 1\n",
    "\n",
    "            subEnd   += 1\n",
    "\n",
    "        subLength -= 1\n",
    "            \n",
    "    return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_num_for_char(doc, start_idx):\n",
    "    for i, token in enumerate(doc):\n",
    "        if start_idx > token.idx:\n",
    "            continue\n",
    "        if start_idx == token.idx:\n",
    "            return i\n",
    "        if start_idx < token.idx:\n",
    "            return i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterFilePairs(count, file, filePairs):\n",
    "    \n",
    "    # If no specific file is passed\n",
    "    if file == None:\n",
    "        \n",
    "        # Check if specific count required\n",
    "        if count > 0:\n",
    "\n",
    "            # Filter down to count\n",
    "            filePairs = filePairs[:count]\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # Use a specific file\n",
    "        newFilePair = []\n",
    "        for filePair in filePairs:\n",
    "\n",
    "            # This gets the foler of the filepairs and matches it to the filename you put it\n",
    "            fileName = [file for file in filePair if file.endswith('.txt')][0]\n",
    "            fileName = fileName.split('\\\\')[-1]\n",
    "            if fileName.replace('.txt', '') == file:\n",
    "\n",
    "                newFilePair += filePair\n",
    "                break\n",
    "\n",
    "        # This breaks if not found\n",
    "        if not newFilePair:\n",
    "            print('ERROR: File not found')\n",
    "            print('File requested: specificFile')\n",
    "\n",
    "        else:\n",
    "            filePairs = [newFilePair]\n",
    "            del newFilePair\n",
    "\n",
    "                     \n",
    "    return filePairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:** Gets the results of all the extracted files and annotated files and merges them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:48:26.922127Z",
     "start_time": "2021-03-14T03:48:21.598584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE: Find source files\n",
      "STAGE: Filter files\n",
      "STAGE: Create metrics dataframes\n",
      "STAGE: Iterate files\n",
      "\n",
      "FILE:    Archer2017_Article_EndolithicMicrobialDiversityIn.txt.txt\n",
      "FOLDER:  ExtractedAnnotatedData\\OldSetTxt\n",
      "COMMAND: Get annotated entities and relations\n",
      "RESULT:  OK\n",
      "COMMAND: Find extracted species CSV\n",
      "RESULT:  Found extracted Species CSV for Archer2017_Article_EndolithicMicrobialDiversityIn.txt.txt as below\n",
      "         ExtractedSpecies\\OldSet\\Archer2017_Article_EndolithicMicrobialDiversityIn-Species.csv\n",
      "COMMAND: Parse extraction data\n",
      "RESULT:  OK\n",
      "COMMAND: Merge extracted and annotated species data\n",
      "RESULT:  OK\n",
      "COMMAND: Find extracted locations csv\n",
      "RESULT:  Found extracted locations CSV for Archer2017_Article_EndolithicMicrobialDiversityIn.txt.txt as below\n",
      "         ExtractedLocations\\OldSet\\Archer2017_Article_EndolithicMicrobialDiversityIn-Locations.csv\n",
      "COMMAND: Parse extracted data\n",
      "RESULT:  OK\n",
      "COMMAND: Match extracted and annotated location data\n",
      "RESULT:  OK\n",
      "COMMAND: Append to over all metrics dataframe\n",
      "RESULT:  OK\n",
      "COMMAND: Merge species and locations to possible relationships\n",
      "RESULT:  OK\n",
      "COMMAND: Create dataframe of actual relationships\n",
      "RESULT:  OK\n",
      "COMMAND: Out oneDocRelationships.csv\n",
      "RESULT:  OK\n",
      "COMMAND: Concat all relationship dataframes\n",
      "RESULT:  OK\n",
      "COMMAND: Out allPossibleRelationships.csv\n",
      "RESULT:  OK\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This notebook is really intensive so I've done a lot of print outs in very ordered, specific formatting \n",
    "# to make it easier for the human brain to follow what is happening\n",
    "\n",
    "# The output will clear on success, it is really only there incase you hit errors\n",
    "\n",
    "# Any errors you do hit should only be problems with input data\n",
    "\n",
    "# I have also broken it into two parts to help with running it\n",
    "\n",
    "# God I wish Python supported multi line comments\n",
    "\n",
    "# Working with all the files takes a ball-bustingly long time, so I've built in filters:\n",
    "\n",
    "# To work with a specific file, change this value to the fileName of the PDF \n",
    "# This should match the folder the files are sitting in\n",
    "# Example: 10.1007_BF00238925.pdf\n",
    "specificFile    = 'Archer2017_Article_EndolithicMicrobialDiversityIn'\n",
    "\n",
    "# To work with all files, set this to zero\n",
    "fileCount       = 2\n",
    "\n",
    "# Load source files\n",
    "print('STAGE: Find source files')\n",
    "srcFolder       = 'ExtractedAnnotatedData\\\\'\n",
    "filePairs       = listAllFilePairs(srcFolder)\n",
    "\n",
    "# Filter to file count or file name\n",
    "print('STAGE: Filter files')\n",
    "filePairs       = filterFilePairs(fileCount, specificFile, filePairs)\n",
    "\n",
    "# Create data frames to append stuff to\n",
    "print('STAGE: Create metrics dataframes')\n",
    "allSpecMatches           = pd.DataFrame()\n",
    "allLocMatches            = pd.DataFrame()\n",
    "allRelAnnotated          = pd.DataFrame()\n",
    "allPossibleRelationships = pd.DataFrame()\n",
    "\n",
    "# Iterate through selected files\n",
    "print ('STAGE: Iterate files')\n",
    "for filePair in filePairs:\n",
    "    print('')\n",
    "    \n",
    "    # Split types\n",
    "    annotationsFile = [file for file in filePair if file.endswith('.json')][0]\n",
    "    txtFile         = [file for file in filePair if file.endswith('.txt')][0]\n",
    "    fileName        = txtFile.split('\\\\')[-1]\n",
    "    folder          = '\\\\'.join(txtFile.split('\\\\')[:-2])\n",
    "    print('FILE:    ' + fileName)\n",
    "    print('FOLDER:  ' + folder)\n",
    "    \n",
    "    # Pass files tuple to extract species, locations and relations from the annotations\n",
    "    print('COMMAND: Get annotated entities and relations')\n",
    "    speciesAnnotated, locationsAnnotated, relationsAnnotated = getEntitiesandRelations(annotationsFile, txtFile)\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Load the Extracted Species CSV file\n",
    "    print('COMMAND: Find extracted species CSV')\n",
    "    result, csv = findExtractedCSV(txtFile, 'ExtractedSpecies\\\\', '-Species.csv')\n",
    "    \n",
    "    if result == False:\n",
    "        print('RESULT:  Could not find Extracted Species CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    else:\n",
    "        print('RESULT:  Found extracted Species CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    \n",
    "    # Parse the csv\n",
    "    print('COMMAND: Parse extraction data')\n",
    "    speciesExtracted              = pd.read_csv(csv,index_col=0 )\n",
    "    speciesExtracted              = speciesExtracted[['Found as', 'Full name', 'Start', 'End','PositionOfFirstToken']]\n",
    "    speciesExtracted.columns      = ['Found_as', 'Full_name', 'Start_Token', 'End_Token', 'Start_Index']\n",
    "    speciesExtracted['End_Index'] = [len(speciesExtracted.loc[X]['Found_as'])+speciesExtracted.loc[X]['Start_Index'] for X in speciesExtracted.index]\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Match Species from annotated data to extracted data\n",
    "    print('COMMAND: Merge extracted and annotated species data')\n",
    "    matchesSpecies                = pd.merge(speciesExtracted, speciesAnnotated, how = 'outer')\n",
    "    matchesSpecies['Match']       = matchesSpecies.Found_as == matchesSpecies.Text\n",
    "    matchesSpecies['Document']    = fileName   \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Load the Extracted Locations CSV file\n",
    "    print('COMMAND: Find extracted locations csv')\n",
    "    result, csv = findExtractedCSV(txtFile, 'ExtractedLocations\\\\', '-Locations.csv')\n",
    "    \n",
    "    if result == False:\n",
    "        print('RESULT:  Could not find extracted locations CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    else:\n",
    "        print('RESULT:  Found extracted locations CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "        \n",
    "    # Parse the CSV\n",
    "    print('COMMAND: Parse extracted data')\n",
    "    locationsExtracted              = pd.read_csv(csv,index_col=0 )  \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Match locations from annotated data to extracted data\n",
    "    print('COMMAND: Match extracted and annotated location data')\n",
    "    matchesLocations                = pd.merge(locationsExtracted, locationsAnnotated, how='outer')\n",
    "    matchesLocations.rename(columns = {'Location':'ExtractedLocation','Text':'TaggedLocation'}, inplace=True)\n",
    "    matchesLocations['Document']    = fileName\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Append to over all dataframes\n",
    "    print('COMMAND: Append to over all metrics dataframe')\n",
    "    allSpecMatches                  = pd.concat([allSpecMatches,  matchesSpecies])\n",
    "    allLocMatches                   = pd.concat([allLocMatches,   matchesLocations])\n",
    "    allRelAnnotated                 = pd.concat([allRelAnnotated, relationsAnnotated])\n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Reset indexes\n",
    "    print('COMMAND: Reset indexes')\n",
    "    allSpecMatches.reset_index (drop=True, inplace=True)\n",
    "    allLocMatches.reset_index  (drop=True, inplace=True)\n",
    "    allRelAnnotated.reset_index(drop=True, inplace=True)  \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Start identifying relationships\n",
    "    print('COMMAND: Merge species and locations to possible relationships')\n",
    "    possibleRelationships = pd.merge(speciesAnnotated.assign(key   = 0), \n",
    "                                     locationsAnnotated.assign(key = 0),\n",
    "                                     on       = ['key','Document'], \n",
    "                                     suffixes = ('_Species', \n",
    "                                                 '_Location')).drop('key', axis = 1)\n",
    "\n",
    "    possibleRelationships.rename(columns = {'Text_Species' :'Species', \n",
    "                                            'Text_Location':'Location',\n",
    "                                            'AnnTxtFile_Species':'AnnTxtFile'}, \n",
    "                                 inplace = True) \n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Create a dataframe of the actual relationships in the file\n",
    "    print('COMMAND: Create dataframe of actual relationships')\n",
    "    oneDocRelationships                     = pd.merge(possibleRelationships, relationsAnnotated, how = 'left')\n",
    "    oneDocRelationships.Tagged_Relationship.fillna(0, inplace=True)\n",
    "    oneDocRelationships.Tagged_Relationship = oneDocRelationships.Tagged_Relationship.astype('int8')\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Create the out folder if it's not there\n",
    "    outFolder = 'Metrics\\\\Relationships\\\\DocumentSpecific\\\\' + folder.split('\\\\')[-1]\n",
    "    if not os.path.exists(outFolder):\n",
    "        os.makedirs(outFolder)   \n",
    "    \n",
    "    # Out the single documents relationships\n",
    "    print('COMMAND: Out oneDocRelationships.csv')\n",
    "    oneDocRelationships.to_csv(outFolder + '\\\\' + fileName + '.csv')\n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Concat bot down to all possible relationships\n",
    "    print('COMMAND: Concat all relationship dataframes')\n",
    "    allPossibleRelationships = pd.concat([allPossibleRelationships, oneDocRelationships])\n",
    "    allPossibleRelationships.reset_index(drop=True, inplace=True)\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "# Print to allRelationships CSVs\n",
    "print('COMMAND: Out allPossibleRelationships.csv')\n",
    "allPossibleRelationships.to_csv('Metrics\\\\Relationships\\\\allPossible.csv')\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Clear output on success\n",
    "#clear_output(wait = True)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** Generates the metrics from all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This will also clear the output on success\n",
    "\n",
    "# Get species matches\n",
    "print('COMMAND: Get the species matches' )\n",
    "speciesMetrics = getSpeciesMetrics(allSpecMatches)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get location matches\n",
    "print('COMMAND: Get the location matches')\n",
    "locationMetrics = getLocationsMetrics(allLocMatches)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Format allLocMatches\n",
    "asStrings = ['ExtractedLocation', \n",
    "             'Original Tokenised Text',\n",
    "             'Sentence',\n",
    "             'TaggedLocation', \n",
    "             'Document']\n",
    "\n",
    "asInts    = ['Start_Token',\n",
    "             'End_Token', \n",
    "             'Start_Index', \n",
    "             'End_Index']\n",
    "\n",
    "asBool    = ['NZGazAnt',\n",
    "             'NZGaz', \n",
    "             'ScarNZ', \n",
    "             'ScarGlobal', \n",
    "             'GeoNamesNZ', \n",
    "             'GeoNamesAnt',\n",
    "             'GeoNames',\n",
    "             'exactMatch', \n",
    "             'Close_Match_NZGazAnt', \n",
    "             'Close_Match_NZGaz',\n",
    "             'Close_Match_ScarNZ', \n",
    "             'Close_Match_ScarGlobal',\n",
    "             'Close_Match_GeoNamesNZ', \n",
    "             'Close_Match_GeoNamesAnt', \n",
    "             'Close_Match',\n",
    "             'PartialMatch_NZGazAnt', \n",
    "             'PartialMatch_NZGaz', \n",
    "             'PartialMatch_ScarNZ',\n",
    "             'PartialMatch_ScarGlobal', \n",
    "             'PartialMatch_GeoNamesNZ',\n",
    "             'PartialMatch_GeoNamesAnt', \n",
    "             'PartialMatch', \n",
    "             'inNZ', \n",
    "             'inAntarctica',\n",
    "             'Found']\n",
    "\n",
    "print('COMMAND: Format allLocMatches dataframe')\n",
    "for col in asStrings:\n",
    "    allLocMatches[col] = allLocMatches[col].astype(str)\n",
    "    \n",
    "for col in asBool:\n",
    "    allLocMatches[col] = allLocMatches[col].astype(bool)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get token numbers for spcecies matches\n",
    "print('COMMAND: Get token numbers')\n",
    "for row in allSpecMatches[allSpecMatches['Found_as'].isna()].index:\n",
    "    allSpecMatches.at[row, 'Found_as']    = allSpecMatches.loc[row].Text\n",
    "    #allSpecMatches.at[row, 'Start_Token'] = get_token_num_for_char(allSpecMatches.loc[row].Text) \n",
    "    # This line wasn't working even in the original project I inherited\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get unmatched locations\n",
    "print('COMMAND: Get unmatched locations')\n",
    "unmatchedLocations = allLocMatches[allLocMatches.ExtractedLocation == 'nan']\n",
    "print(\"RESULT:  OK\")\n",
    "\n",
    "# Load gazetters\n",
    "print('COMMAND: Load gazetters')\n",
    "nzGaz            = list(np.load('NPYs\\\\nzgazetteer.npy',      allow_pickle=True))\n",
    "print('         nzgaz            OK')\n",
    "nzGazAntartica   = list(np.load('NPYs\\\\nzgazAntartica.npy',   allow_pickle=True))\n",
    "print('         nzgazAntartica   OK')\n",
    "scarGlobalNames  = list(np.load('NPYs\\\\SCARGlobalnames.npy',  allow_pickle=True))\n",
    "print('         SCARGlobalnames  OK')\n",
    "scarNzNames      = list(np.load('NPYs\\\\SCARNZnames.npy',      allow_pickle=True))\n",
    "print('         SCARNZnames      OK')\n",
    "geoNamesAnt      = list(np.load('NPYs\\\\GeoNamesAnt.npy',      allow_pickle=True))\n",
    "print('         GeoNamesAnt      OK')\n",
    "geoNamesNZ       = list(np.load('NPYs\\\\GeoNamesNZ.npy',       allow_pickle=True))\n",
    "print('         GeoNamesNZ       OK')\n",
    "geoNamesFiltered = list(np.load('NPYs\\\\GeoNamesFiltered.npy', allow_pickle=True))\n",
    "print('         GeoNamesFiltered OK')\n",
    "print('         Create list arg for function calls')\n",
    "gazetters       = [nzGaz, nzGazAntartica, scarGlobalNames, scarNzNames, geoNamesAnt, geoNamesNZ, geoNamesFiltered]\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Filter unmatched locations to actualy unmatched locations\n",
    "print('COMMAND: Apply unmatched locations')\n",
    "unmatchedLocations = populate_taggedlocations_df(unmatchedLocations, gazetters, filePairs) \n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Apply to matched locations\n",
    "print('COMMAND: Apply to as filter all matched locations dataframe')\n",
    "allLocMatches.update(unmatchedLocations, overwrite=True)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Convert columns to typeof Object\n",
    "print('COMMAND: Convert columns')\n",
    "for col in asBool:\n",
    "    unmatchedLocations[col] = unmatchedLocations[col].astype(object)    \n",
    "    allLocMatches[col]      = allLocMatches[col].astype(object)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Create tagged location dataframe\n",
    "print('COMMAND: Create tagged location dataframe from location matched')\n",
    "TaggedLocations = allLocMatches[allLocMatches.TaggedLocation.notna()]\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Clear output on success\n",
    "clear_output(wait = True)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Found_as</th>\n",
       "      <th>Full_name</th>\n",
       "      <th>Start_Token</th>\n",
       "      <th>End_Token</th>\n",
       "      <th>Start_Index</th>\n",
       "      <th>End_Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>classId</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>inAbstract300</th>\n",
       "      <th>inAbstract500</th>\n",
       "      <th>max_TFISF</th>\n",
       "      <th>Sent_Start</th>\n",
       "      <th>Document</th>\n",
       "      <th>AnnTxtFile</th>\n",
       "      <th>Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>15807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>15807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>15807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>15807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>15807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5707</td>\n",
       "      <td>5708</td>\n",
       "      <td>31687</td>\n",
       "      <td>31695</td>\n",
       "      <td>McKelvey</td>\n",
       "      <td>e_2</td>\n",
       "      <td>(Since, the, alphaproteobacterial, OTUs, inclu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.263745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5709</td>\n",
       "      <td>5710</td>\n",
       "      <td>31700</td>\n",
       "      <td>31705</td>\n",
       "      <td>Miers</td>\n",
       "      <td>e_2</td>\n",
       "      <td>(Since, the, alphaproteobacterial, OTUs, inclu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.130214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5745</td>\n",
       "      <td>5747</td>\n",
       "      <td>31966</td>\n",
       "      <td>31983</td>\n",
       "      <td>University Valley</td>\n",
       "      <td>e_2</td>\n",
       "      <td>(Lichen, associations, were, demonstrated, in,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.811760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5925</td>\n",
       "      <td>5926</td>\n",
       "      <td>33023</td>\n",
       "      <td>33033</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>e_2</td>\n",
       "      <td>(Acknowledgments, The, authors, wish, to, ackn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.724715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5925</td>\n",
       "      <td>5926</td>\n",
       "      <td>33023</td>\n",
       "      <td>33033</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>e_2</td>\n",
       "      <td>(Acknowledgments, The, authors, wish, to, ackn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.724715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Found_as               Full_name  Start_Token  End_Token  \\\n",
       "0    Hemichloris antarctica  Hemichloris antarctica         2908       2910   \n",
       "1    Hemichloris antarctica  Hemichloris antarctica         2908       2910   \n",
       "2    Hemichloris antarctica  Hemichloris antarctica         2908       2910   \n",
       "3    Hemichloris antarctica  Hemichloris antarctica         2908       2910   \n",
       "4    Hemichloris antarctica  Hemichloris antarctica         2908       2910   \n",
       "..                      ...                     ...          ...        ...   \n",
       "787                     NaN                     NaN         5707       5708   \n",
       "788                     NaN                     NaN         5709       5710   \n",
       "789                     NaN                     NaN         5745       5747   \n",
       "790                     NaN                     NaN         5925       5926   \n",
       "791                     NaN                     NaN         5925       5926   \n",
       "\n",
       "     Start_Index  End_Index               Text classId  \\\n",
       "0          15785      15807                NaN     NaN   \n",
       "1          15785      15807                NaN     NaN   \n",
       "2          15785      15807                NaN     NaN   \n",
       "3          15785      15807                NaN     NaN   \n",
       "4          15785      15807                NaN     NaN   \n",
       "..           ...        ...                ...     ...   \n",
       "787        31687      31695           McKelvey     e_2   \n",
       "788        31700      31705              Miers     e_2   \n",
       "789        31966      31983  University Valley     e_2   \n",
       "790        33023      33033         Antarctica     e_2   \n",
       "791        33023      33033         Antarctica     e_2   \n",
       "\n",
       "                                              Sentence  inAbstract300  \\\n",
       "0                                                  NaN            NaN   \n",
       "1                                                  NaN            NaN   \n",
       "2                                                  NaN            NaN   \n",
       "3                                                  NaN            NaN   \n",
       "4                                                  NaN            NaN   \n",
       "..                                                 ...            ...   \n",
       "787  (Since, the, alphaproteobacterial, OTUs, inclu...            0.0   \n",
       "788  (Since, the, alphaproteobacterial, OTUs, inclu...            0.0   \n",
       "789  (Lichen, associations, were, demonstrated, in,...            0.0   \n",
       "790  (Acknowledgments, The, authors, wish, to, ackn...            1.0   \n",
       "791  (Acknowledgments, The, authors, wish, to, ackn...            1.0   \n",
       "\n",
       "     inAbstract500  max_TFISF  Sent_Start  \\\n",
       "0              NaN        NaN         NaN   \n",
       "1              NaN        NaN         NaN   \n",
       "2              NaN        NaN         NaN   \n",
       "3              NaN        NaN         NaN   \n",
       "4              NaN        NaN         NaN   \n",
       "..             ...        ...         ...   \n",
       "787            0.0   2.263745         0.0   \n",
       "788            0.0   2.130214         0.0   \n",
       "789            0.0   1.811760         0.0   \n",
       "790            1.0   6.724715         0.0   \n",
       "791            1.0   6.724715         0.0   \n",
       "\n",
       "                                              Document  \\\n",
       "0    Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "1    Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "2    Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "3    Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "4    Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "..                                                 ...   \n",
       "787  Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "788  Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "789  Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "790  Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "791  Archer2017_Article_EndolithicMicrobialDiversit...   \n",
       "\n",
       "                                            AnnTxtFile  Match  \n",
       "0                                                  NaN  False  \n",
       "1                                                  NaN  False  \n",
       "2                                                  NaN  False  \n",
       "3                                                  NaN  False  \n",
       "4                                                  NaN  False  \n",
       "..                                                 ...    ...  \n",
       "787  ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...  False  \n",
       "788  ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...  False  \n",
       "789  ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...  False  \n",
       "790  ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...  False  \n",
       "791  ExtractedAnnotatedData\\OldSetTxt\\Archer2017_Ar...  False  \n",
       "\n",
       "[792 rows x 16 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This data frame has all the extracted species matched to all the locations in the annotated data sets\n",
    "allSpecMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Found as</th>\n",
       "      <th>Full name</th>\n",
       "      <th>InDefinitiveList</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>PositionOfFirstToken</th>\n",
       "      <th>Sentence_species</th>\n",
       "      <th>InReferenceList</th>\n",
       "      <th>ExtractedLocation</th>\n",
       "      <th>Original Tokenised Text</th>\n",
       "      <th>...</th>\n",
       "      <th>Found</th>\n",
       "      <th>TaggedLocation</th>\n",
       "      <th>classId</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>inAbstract300</th>\n",
       "      <th>inAbstract500</th>\n",
       "      <th>max_TFISF</th>\n",
       "      <th>Sent_Start</th>\n",
       "      <th>Document</th>\n",
       "      <th>AnnTxtFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>Free-living algal cells resembling Hemichloris...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>3949</td>\n",
       "      <td>3950</td>\n",
       "      <td>21628</td>\n",
       "      <td> Chasmoendolithic microbial community from Mi...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>3989</td>\n",
       "      <td>3991</td>\n",
       "      <td>21838</td>\n",
       "      <td> Chasmoendolithic microbial community from Mi...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trebouxia</td>\n",
       "      <td>Trebouxia</td>\n",
       "      <td>True</td>\n",
       "      <td>4003</td>\n",
       "      <td>4004</td>\n",
       "      <td>21918</td>\n",
       "      <td>Whilst Trebouxia-like algal cells were observe...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lecidea</td>\n",
       "      <td>Lecidea</td>\n",
       "      <td>False</td>\n",
       "      <td>4046</td>\n",
       "      <td>4047</td>\n",
       "      <td>22161</td>\n",
       "      <td>Lichen-forming ascomycetes (Lecidea genus) wer...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>5419</td>\n",
       "      <td>5421</td>\n",
       "      <td>29696</td>\n",
       "      <td>Among the eukaryotes, the free-living alga Hem...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>5443</td>\n",
       "      <td>5444</td>\n",
       "      <td>29840</td>\n",
       "      <td>Among the eukaryotes, the free-living alga Hem...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>5460</td>\n",
       "      <td>5461</td>\n",
       "      <td>29941</td>\n",
       "      <td>The genus Diplosphaera genus has been reported...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Hemichloris</td>\n",
       "      <td>Hemichloris</td>\n",
       "      <td>False</td>\n",
       "      <td>5530</td>\n",
       "      <td>5531</td>\n",
       "      <td>30357</td>\n",
       "      <td>Hemichloris cells were situated G) Springer - ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Coccomyxa</td>\n",
       "      <td>Coccomyxa</td>\n",
       "      <td>True</td>\n",
       "      <td>6399</td>\n",
       "      <td>6400</td>\n",
       "      <td>35332</td>\n",
       "      <td>1300643110 Darienko T, Gustavs L et al (2015) ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Found as               Full name  InDefinitiveList  Start  \\\n",
       "0    Hemichloris antarctica  Hemichloris antarctica              True   2908   \n",
       "1              Diplosphaera            Diplosphaera             False   3949   \n",
       "2    Hemichloris antarctica  Hemichloris antarctica              True   3989   \n",
       "3                 Trebouxia               Trebouxia              True   4003   \n",
       "4                   Lecidea                 Lecidea             False   4046   \n",
       "..                      ...                     ...               ...    ...   \n",
       "644  Hemichloris antarctica  Hemichloris antarctica              True   5419   \n",
       "645            Diplosphaera            Diplosphaera             False   5443   \n",
       "646            Diplosphaera            Diplosphaera             False   5460   \n",
       "647             Hemichloris             Hemichloris             False   5530   \n",
       "648               Coccomyxa               Coccomyxa              True   6399   \n",
       "\n",
       "      End  PositionOfFirstToken  \\\n",
       "0    2910                 15785   \n",
       "1    3950                 21628   \n",
       "2    3991                 21838   \n",
       "3    4004                 21918   \n",
       "4    4047                 22161   \n",
       "..    ...                   ...   \n",
       "644  5421                 29696   \n",
       "645  5444                 29840   \n",
       "646  5461                 29941   \n",
       "647  5531                 30357   \n",
       "648  6400                 35332   \n",
       "\n",
       "                                      Sentence_species  InReferenceList  \\\n",
       "0    Free-living algal cells resembling Hemichloris...             True   \n",
       "1     Chasmoendolithic microbial community from Mi...            False   \n",
       "2     Chasmoendolithic microbial community from Mi...             True   \n",
       "3    Whilst Trebouxia-like algal cells were observe...            False   \n",
       "4    Lichen-forming ascomycetes (Lecidea genus) wer...            False   \n",
       "..                                                 ...              ...   \n",
       "644  Among the eukaryotes, the free-living alga Hem...             True   \n",
       "645  Among the eukaryotes, the free-living alga Hem...            False   \n",
       "646  The genus Diplosphaera genus has been reported...            False   \n",
       "647  Hemichloris cells were situated G) Springer - ...            False   \n",
       "648  1300643110 Darienko T, Gustavs L et al (2015) ...            False   \n",
       "\n",
       "    ExtractedLocation Original Tokenised Text  ...  Found  TaggedLocation  \\\n",
       "0          Antarctica              Antarctica  ...   True             NaN   \n",
       "1          Antarctica              Antarctica  ...   True             NaN   \n",
       "2          Antarctica              Antarctica  ...   True             NaN   \n",
       "3          Antarctica              Antarctica  ...   True             NaN   \n",
       "4          Antarctica              Antarctica  ...   True             NaN   \n",
       "..                ...                     ...  ...    ...             ...   \n",
       "644        Antarctica              Antarctica  ...   True             NaN   \n",
       "645        Antarctica              Antarctica  ...   True             NaN   \n",
       "646        Antarctica              Antarctica  ...   True             NaN   \n",
       "647        Antarctica              Antarctica  ...   True             NaN   \n",
       "648        Antarctica              Antarctica  ...   True             NaN   \n",
       "\n",
       "     classId  Sentence inAbstract300  inAbstract500  max_TFISF  Sent_Start  \\\n",
       "0        NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "1        NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "2        NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "3        NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "4        NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "..       ...       ...           ...            ...        ...         ...   \n",
       "644      NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "645      NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "646      NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "647      NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "648      NaN       NaN           NaN            NaN        NaN         NaN   \n",
       "\n",
       "                                              Document  AnnTxtFile  \n",
       "0    Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "1    Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "2    Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "3    Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "4    Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "..                                                 ...         ...  \n",
       "644  Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "645  Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "646  Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "647  Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "648  Archer2017_Article_EndolithicMicrobialDiversit...         NaN  \n",
       "\n",
       "[649 rows x 49 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataframe has all the extracted locations matched to all the locations in the annotated data sets\n",
    "allLocMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Start_Token_Species</th>\n",
       "      <th>End_Token_Species</th>\n",
       "      <th>Start_Index_Species</th>\n",
       "      <th>End_Index_Species</th>\n",
       "      <th>Location</th>\n",
       "      <th>Start_Token_Location</th>\n",
       "      <th>End_Token_Location</th>\n",
       "      <th>Start_Index_Location</th>\n",
       "      <th>End_Index_Location</th>\n",
       "      <th>Tagged_Relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hemichloris antarctic</td>\n",
       "      <td>2885</td>\n",
       "      <td>2887</td>\n",
       "      <td>15742</td>\n",
       "      <td>15763</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>2819.0</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>15357</td>\n",
       "      <td>15373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scytonematacea</td>\n",
       "      <td>3362</td>\n",
       "      <td>3363</td>\n",
       "      <td>18312</td>\n",
       "      <td>18326</td>\n",
       "      <td>McKelvey Valle</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>18367</td>\n",
       "      <td>18381</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scytonematacea</td>\n",
       "      <td>3362</td>\n",
       "      <td>3363</td>\n",
       "      <td>18312</td>\n",
       "      <td>18326</td>\n",
       "      <td>Miers Valle</td>\n",
       "      <td>3382.0</td>\n",
       "      <td>3384.0</td>\n",
       "      <td>18428</td>\n",
       "      <td>18439</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scytonematacea</td>\n",
       "      <td>3362</td>\n",
       "      <td>3363</td>\n",
       "      <td>18312</td>\n",
       "      <td>18326</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>3397.0</td>\n",
       "      <td>3399.0</td>\n",
       "      <td>18499</td>\n",
       "      <td>18515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actinobacteri</td>\n",
       "      <td>3624</td>\n",
       "      <td>3625</td>\n",
       "      <td>19809</td>\n",
       "      <td>19822</td>\n",
       "      <td>Miers Valle</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>3642.0</td>\n",
       "      <td>19922</td>\n",
       "      <td>19933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bacteriodete</td>\n",
       "      <td>3626</td>\n",
       "      <td>3627</td>\n",
       "      <td>19825</td>\n",
       "      <td>19837</td>\n",
       "      <td>Miers Valle</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>3642.0</td>\n",
       "      <td>19922</td>\n",
       "      <td>19933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chloroflex</td>\n",
       "      <td>3628</td>\n",
       "      <td>3629</td>\n",
       "      <td>19843</td>\n",
       "      <td>19853</td>\n",
       "      <td>Miers Valle</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>3642.0</td>\n",
       "      <td>19922</td>\n",
       "      <td>19933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lentisphaera</td>\n",
       "      <td>3727</td>\n",
       "      <td>3728</td>\n",
       "      <td>20460</td>\n",
       "      <td>20472</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>3731.0</td>\n",
       "      <td>3733.0</td>\n",
       "      <td>20486</td>\n",
       "      <td>20502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hemichloris antarctic</td>\n",
       "      <td>3948</td>\n",
       "      <td>3950</td>\n",
       "      <td>21706</td>\n",
       "      <td>21727</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>3924.0</td>\n",
       "      <td>3926.0</td>\n",
       "      <td>21576</td>\n",
       "      <td>21592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hemichloris antarctic</td>\n",
       "      <td>3948</td>\n",
       "      <td>3950</td>\n",
       "      <td>21706</td>\n",
       "      <td>21727</td>\n",
       "      <td>McKelve</td>\n",
       "      <td>3956.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>21751</td>\n",
       "      <td>21758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hemichloris antarctic</td>\n",
       "      <td>3948</td>\n",
       "      <td>3950</td>\n",
       "      <td>21706</td>\n",
       "      <td>21727</td>\n",
       "      <td>Mier</td>\n",
       "      <td>3958.0</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>21764</td>\n",
       "      <td>21768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lecide</td>\n",
       "      <td>4005</td>\n",
       "      <td>4006</td>\n",
       "      <td>22029</td>\n",
       "      <td>22035</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>4012.0</td>\n",
       "      <td>4014.0</td>\n",
       "      <td>22066</td>\n",
       "      <td>22082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thaumarchaeot</td>\n",
       "      <td>4130</td>\n",
       "      <td>4131</td>\n",
       "      <td>22826</td>\n",
       "      <td>22839</td>\n",
       "      <td>McKelve</td>\n",
       "      <td>4133.0</td>\n",
       "      <td>4134.0</td>\n",
       "      <td>22852</td>\n",
       "      <td>22859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thaumarchaeot</td>\n",
       "      <td>4130</td>\n",
       "      <td>4131</td>\n",
       "      <td>22826</td>\n",
       "      <td>22839</td>\n",
       "      <td>Mier</td>\n",
       "      <td>4135.0</td>\n",
       "      <td>4136.0</td>\n",
       "      <td>22865</td>\n",
       "      <td>22869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Thaumarchaeot</td>\n",
       "      <td>4130</td>\n",
       "      <td>4131</td>\n",
       "      <td>22826</td>\n",
       "      <td>22839</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>4140.0</td>\n",
       "      <td>4142.0</td>\n",
       "      <td>22893</td>\n",
       "      <td>22909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Scytonematacea</td>\n",
       "      <td>4981</td>\n",
       "      <td>4982</td>\n",
       "      <td>27421</td>\n",
       "      <td>27435</td>\n",
       "      <td>Mier</td>\n",
       "      <td>4986.0</td>\n",
       "      <td>4987.0</td>\n",
       "      <td>27462</td>\n",
       "      <td>27466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Scytonematacea</td>\n",
       "      <td>4981</td>\n",
       "      <td>4982</td>\n",
       "      <td>27421</td>\n",
       "      <td>27435</td>\n",
       "      <td>McKelve</td>\n",
       "      <td>4988.0</td>\n",
       "      <td>4989.0</td>\n",
       "      <td>27472</td>\n",
       "      <td>27479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hemichloris antarctic</td>\n",
       "      <td>5353</td>\n",
       "      <td>5355</td>\n",
       "      <td>29448</td>\n",
       "      <td>29469</td>\n",
       "      <td>University Valle</td>\n",
       "      <td>5365.0</td>\n",
       "      <td>5367.0</td>\n",
       "      <td>29511</td>\n",
       "      <td>29527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Diplosphaer</td>\n",
       "      <td>5377</td>\n",
       "      <td>5378</td>\n",
       "      <td>29592</td>\n",
       "      <td>29603</td>\n",
       "      <td>Mier</td>\n",
       "      <td>5387.0</td>\n",
       "      <td>5388.0</td>\n",
       "      <td>29655</td>\n",
       "      <td>29659</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Diplosphaer</td>\n",
       "      <td>5377</td>\n",
       "      <td>5378</td>\n",
       "      <td>29592</td>\n",
       "      <td>29603</td>\n",
       "      <td>McKelve</td>\n",
       "      <td>5389.0</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>29665</td>\n",
       "      <td>29672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Species  Start_Token_Species  End_Token_Species  \\\n",
       "0   Hemichloris antarctic                 2885               2887   \n",
       "1          Scytonematacea                 3362               3363   \n",
       "2          Scytonematacea                 3362               3363   \n",
       "3          Scytonematacea                 3362               3363   \n",
       "4           Actinobacteri                 3624               3625   \n",
       "5            Bacteriodete                 3626               3627   \n",
       "6              Chloroflex                 3628               3629   \n",
       "7            Lentisphaera                 3727               3728   \n",
       "8   Hemichloris antarctic                 3948               3950   \n",
       "9   Hemichloris antarctic                 3948               3950   \n",
       "10  Hemichloris antarctic                 3948               3950   \n",
       "11                 Lecide                 4005               4006   \n",
       "12          Thaumarchaeot                 4130               4131   \n",
       "13          Thaumarchaeot                 4130               4131   \n",
       "14          Thaumarchaeot                 4130               4131   \n",
       "15         Scytonematacea                 4981               4982   \n",
       "16         Scytonematacea                 4981               4982   \n",
       "17  Hemichloris antarctic                 5353               5355   \n",
       "18            Diplosphaer                 5377               5378   \n",
       "19            Diplosphaer                 5377               5378   \n",
       "\n",
       "    Start_Index_Species  End_Index_Species          Location  \\\n",
       "0                 15742              15763  University Valle   \n",
       "1                 18312              18326    McKelvey Valle   \n",
       "2                 18312              18326       Miers Valle   \n",
       "3                 18312              18326  University Valle   \n",
       "4                 19809              19822       Miers Valle   \n",
       "5                 19825              19837       Miers Valle   \n",
       "6                 19843              19853       Miers Valle   \n",
       "7                 20460              20472  University Valle   \n",
       "8                 21706              21727  University Valle   \n",
       "9                 21706              21727           McKelve   \n",
       "10                21706              21727              Mier   \n",
       "11                22029              22035  University Valle   \n",
       "12                22826              22839           McKelve   \n",
       "13                22826              22839              Mier   \n",
       "14                22826              22839  University Valle   \n",
       "15                27421              27435              Mier   \n",
       "16                27421              27435           McKelve   \n",
       "17                29448              29469  University Valle   \n",
       "18                29592              29603              Mier   \n",
       "19                29592              29603           McKelve   \n",
       "\n",
       "    Start_Token_Location  End_Token_Location  Start_Index_Location  \\\n",
       "0                 2819.0              2821.0                 15357   \n",
       "1                 3371.0              3373.0                 18367   \n",
       "2                 3382.0              3384.0                 18428   \n",
       "3                 3397.0              3399.0                 18499   \n",
       "4                 3640.0              3642.0                 19922   \n",
       "5                 3640.0              3642.0                 19922   \n",
       "6                 3640.0              3642.0                 19922   \n",
       "7                 3731.0              3733.0                 20486   \n",
       "8                 3924.0              3926.0                 21576   \n",
       "9                 3956.0              3957.0                 21751   \n",
       "10                3958.0              3959.0                 21764   \n",
       "11                4012.0              4014.0                 22066   \n",
       "12                4133.0              4134.0                 22852   \n",
       "13                4135.0              4136.0                 22865   \n",
       "14                4140.0              4142.0                 22893   \n",
       "15                4986.0              4987.0                 27462   \n",
       "16                4988.0              4989.0                 27472   \n",
       "17                5365.0              5367.0                 29511   \n",
       "18                5387.0              5388.0                 29655   \n",
       "19                5389.0              5390.0                 29665   \n",
       "\n",
       "    End_Index_Location  Tagged_Relationship  \n",
       "0                15373                    1  \n",
       "1                18381                    1  \n",
       "2                18439                    1  \n",
       "3                18515                    1  \n",
       "4                19933                    1  \n",
       "5                19933                    1  \n",
       "6                19933                    1  \n",
       "7                20502                    1  \n",
       "8                21592                    1  \n",
       "9                21758                    1  \n",
       "10               21768                    1  \n",
       "11               22082                    1  \n",
       "12               22859                    1  \n",
       "13               22869                    1  \n",
       "14               22909                    1  \n",
       "15               27466                    1  \n",
       "16               27479                    1  \n",
       "17               29527                    1  \n",
       "18               29659                    1  \n",
       "19               29672                    1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This data frame has all the relationships found in the manually annotated pdfs\n",
    "allRelAnnotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classId_Species</th>\n",
       "      <th>Sentence_Species</th>\n",
       "      <th>inAbstract300_Species</th>\n",
       "      <th>inAbstract500_Species</th>\n",
       "      <th>max_TFISF_Species</th>\n",
       "      <th>Sent_Start_Species</th>\n",
       "      <th>Document</th>\n",
       "      <th>AnnTxtFile</th>\n",
       "      <th>classId_Location</th>\n",
       "      <th>Sentence_Location</th>\n",
       "      <th>...</th>\n",
       "      <th>Start_Token_Species</th>\n",
       "      <th>End_Token_Species</th>\n",
       "      <th>Start_Index_Species</th>\n",
       "      <th>End_Index_Species</th>\n",
       "      <th>Location</th>\n",
       "      <th>Start_Token_Location</th>\n",
       "      <th>End_Token_Location</th>\n",
       "      <th>Start_Index_Location</th>\n",
       "      <th>End_Index_Location</th>\n",
       "      <th>Tagged_Relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [classId_Species, Sentence_Species, inAbstract300_Species, inAbstract500_Species, max_TFISF_Species, Sent_Start_Species, Document, AnnTxtFile, classId_Location, Sentence_Location, inAbstract300_Location, inAbstract500_Location, max_TFISF_Location, Sent_Start_Location, AnnTxtFile_Location, Species, Start_Token_Species, End_Token_Species, Start_Index_Species, End_Index_Species, Location, Start_Token_Location, End_Token_Location, Start_Index_Location, End_Index_Location, Tagged_Relationship]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 26 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows all the possible annotated relationships\n",
    "allPossibleRelationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Tagged</th>\n",
       "      <th>Extracted</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Partial Matches</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Adj. Precision</th>\n",
       "      <th>Adj. Recall</th>\n",
       "      <th>Adj. F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>143</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total</td>\n",
       "      <td>143</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Tagged  Extracted  \\\n",
       "0  Archer2017_Article_EndolithicMicrobialDiversit...     143        649   \n",
       "1                                              Total     143        649   \n",
       "\n",
       "   True Positives  False Positives  False Negatives  Partial Matches  \\\n",
       "0               0              649              143                0   \n",
       "1               0              649              143                0   \n",
       "\n",
       "   Precision  Recall  F1  Adj. Precision  Adj. Recall  Adj. F1  \n",
       "0        0.0     0.0 NaN             0.0          0.0      NaN  \n",
       "1        0.0     0.0 NaN             0.0          0.0      NaN  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the metrics for species\n",
    "speciesMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Tagged</th>\n",
       "      <th>Extracted</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Partial Matches</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Adj. Precision</th>\n",
       "      <th>Adj. Recall</th>\n",
       "      <th>Adj. F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Tagged  Extracted  \\\n",
       "0  Archer2017_Article_EndolithicMicrobialDiversit...       0        649   \n",
       "1                                              Total       0        649   \n",
       "\n",
       "   True Positives  False Positives  False Negatives  Partial Matches  \\\n",
       "0               0              649                0                0   \n",
       "1               0              649                0                0   \n",
       "\n",
       "   Precision  Recall  F1  Adj. Precision  Adj. Recall  Adj. F1  \n",
       "0        0.0     NaN NaN             0.0          NaN      NaN  \n",
       "1        0.0     NaN NaN             0.0          NaN      NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the metrics for locations\n",
    "locationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Found as</th>\n",
       "      <th>Full name</th>\n",
       "      <th>InDefinitiveList</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>PositionOfFirstToken</th>\n",
       "      <th>Sentence_species</th>\n",
       "      <th>InReferenceList</th>\n",
       "      <th>ExtractedLocation</th>\n",
       "      <th>Original Tokenised Text</th>\n",
       "      <th>...</th>\n",
       "      <th>inAbstract300</th>\n",
       "      <th>inAbstract500</th>\n",
       "      <th>max_TFISF</th>\n",
       "      <th>Sent_Start</th>\n",
       "      <th>Document</th>\n",
       "      <th>AnnTxtFile</th>\n",
       "      <th>id</th>\n",
       "      <th>Location</th>\n",
       "      <th>inNZ</th>\n",
       "      <th>inAntarctica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Found as, Full name, InDefinitiveList, Start, End, PositionOfFirstToken, Sentence_species, InReferenceList, ExtractedLocation, Original Tokenised Text, Start_Token, End_Token, Start_Index, End_Index, Sentence_location, NZGazAnt, NZGaz, ScarNZ, ScarGlobal, GeoNamesNZ, GeoNamesAnt, GeoNames, exactMatch, Close_Match_NZGazAnt, Close_Match_NZGaz, Close_Match_ScarNZ, Close_Match_ScarGlobal, Close_Match_GeoNamesNZ, Close_Match_GeoNamesAnt, Close_Match, PartialMatch_NZGazAnt, PartialMatch_NZGaz, PartialMatch_ScarNZ, PartialMatch_ScarGlobal, PartialMatch_GeoNamesNZ, PartialMatch_GeoNamesAnt, PartialMatch, Found, TaggedLocation, classId, Sentence, inAbstract300, inAbstract500, max_TFISF, Sent_Start, Document, AnnTxtFile, id, Location, inNZ, inAntarctica]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 51 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all the unmatched locations\n",
    "unmatchedLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Found as</th>\n",
       "      <th>Full name</th>\n",
       "      <th>InDefinitiveList</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>PositionOfFirstToken</th>\n",
       "      <th>Sentence_species</th>\n",
       "      <th>InReferenceList</th>\n",
       "      <th>ExtractedLocation</th>\n",
       "      <th>Original Tokenised Text</th>\n",
       "      <th>...</th>\n",
       "      <th>Found</th>\n",
       "      <th>TaggedLocation</th>\n",
       "      <th>classId</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>inAbstract300</th>\n",
       "      <th>inAbstract500</th>\n",
       "      <th>max_TFISF</th>\n",
       "      <th>Sent_Start</th>\n",
       "      <th>Document</th>\n",
       "      <th>AnnTxtFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>2908</td>\n",
       "      <td>2910</td>\n",
       "      <td>15785</td>\n",
       "      <td>Free-living algal cells resembling Hemichloris...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>3949</td>\n",
       "      <td>3950</td>\n",
       "      <td>21628</td>\n",
       "      <td> Chasmoendolithic microbial community from Mi...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>3989</td>\n",
       "      <td>3991</td>\n",
       "      <td>21838</td>\n",
       "      <td> Chasmoendolithic microbial community from Mi...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trebouxia</td>\n",
       "      <td>Trebouxia</td>\n",
       "      <td>True</td>\n",
       "      <td>4003</td>\n",
       "      <td>4004</td>\n",
       "      <td>21918</td>\n",
       "      <td>Whilst Trebouxia-like algal cells were observe...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lecidea</td>\n",
       "      <td>Lecidea</td>\n",
       "      <td>False</td>\n",
       "      <td>4046</td>\n",
       "      <td>4047</td>\n",
       "      <td>22161</td>\n",
       "      <td>Lichen-forming ascomycetes (Lecidea genus) wer...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>Hemichloris antarctica</td>\n",
       "      <td>True</td>\n",
       "      <td>5419</td>\n",
       "      <td>5421</td>\n",
       "      <td>29696</td>\n",
       "      <td>Among the eukaryotes, the free-living alga Hem...</td>\n",
       "      <td>True</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>5443</td>\n",
       "      <td>5444</td>\n",
       "      <td>29840</td>\n",
       "      <td>Among the eukaryotes, the free-living alga Hem...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>False</td>\n",
       "      <td>5460</td>\n",
       "      <td>5461</td>\n",
       "      <td>29941</td>\n",
       "      <td>The genus Diplosphaera genus has been reported...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Hemichloris</td>\n",
       "      <td>Hemichloris</td>\n",
       "      <td>False</td>\n",
       "      <td>5530</td>\n",
       "      <td>5531</td>\n",
       "      <td>30357</td>\n",
       "      <td>Hemichloris cells were situated G) Springer - ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Coccomyxa</td>\n",
       "      <td>Coccomyxa</td>\n",
       "      <td>True</td>\n",
       "      <td>6399</td>\n",
       "      <td>6400</td>\n",
       "      <td>35332</td>\n",
       "      <td>1300643110 Darienko T, Gustavs L et al (2015) ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Archer2017_Article_EndolithicMicrobialDiversit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Found as               Full name  InDefinitiveList  Start  \\\n",
       "0    Hemichloris antarctica  Hemichloris antarctica              True   2908   \n",
       "1              Diplosphaera            Diplosphaera             False   3949   \n",
       "2    Hemichloris antarctica  Hemichloris antarctica              True   3989   \n",
       "3                 Trebouxia               Trebouxia              True   4003   \n",
       "4                   Lecidea                 Lecidea             False   4046   \n",
       "..                      ...                     ...               ...    ...   \n",
       "644  Hemichloris antarctica  Hemichloris antarctica              True   5419   \n",
       "645            Diplosphaera            Diplosphaera             False   5443   \n",
       "646            Diplosphaera            Diplosphaera             False   5460   \n",
       "647             Hemichloris             Hemichloris             False   5530   \n",
       "648               Coccomyxa               Coccomyxa              True   6399   \n",
       "\n",
       "      End  PositionOfFirstToken  \\\n",
       "0    2910                 15785   \n",
       "1    3950                 21628   \n",
       "2    3991                 21838   \n",
       "3    4004                 21918   \n",
       "4    4047                 22161   \n",
       "..    ...                   ...   \n",
       "644  5421                 29696   \n",
       "645  5444                 29840   \n",
       "646  5461                 29941   \n",
       "647  5531                 30357   \n",
       "648  6400                 35332   \n",
       "\n",
       "                                      Sentence_species  InReferenceList  \\\n",
       "0    Free-living algal cells resembling Hemichloris...             True   \n",
       "1     Chasmoendolithic microbial community from Mi...            False   \n",
       "2     Chasmoendolithic microbial community from Mi...             True   \n",
       "3    Whilst Trebouxia-like algal cells were observe...            False   \n",
       "4    Lichen-forming ascomycetes (Lecidea genus) wer...            False   \n",
       "..                                                 ...              ...   \n",
       "644  Among the eukaryotes, the free-living alga Hem...             True   \n",
       "645  Among the eukaryotes, the free-living alga Hem...            False   \n",
       "646  The genus Diplosphaera genus has been reported...            False   \n",
       "647  Hemichloris cells were situated G) Springer - ...            False   \n",
       "648  1300643110 Darienko T, Gustavs L et al (2015) ...            False   \n",
       "\n",
       "    ExtractedLocation Original Tokenised Text  ...  Found  TaggedLocation  \\\n",
       "0          Antarctica              Antarctica  ...   True             nan   \n",
       "1          Antarctica              Antarctica  ...   True             nan   \n",
       "2          Antarctica              Antarctica  ...   True             nan   \n",
       "3          Antarctica              Antarctica  ...   True             nan   \n",
       "4          Antarctica              Antarctica  ...   True             nan   \n",
       "..                ...                     ...  ...    ...             ...   \n",
       "644        Antarctica              Antarctica  ...   True             nan   \n",
       "645        Antarctica              Antarctica  ...   True             nan   \n",
       "646        Antarctica              Antarctica  ...   True             nan   \n",
       "647        Antarctica              Antarctica  ...   True             nan   \n",
       "648        Antarctica              Antarctica  ...   True             nan   \n",
       "\n",
       "     classId  Sentence inAbstract300 inAbstract500 max_TFISF Sent_Start  \\\n",
       "0        NaN       nan           NaN           NaN       NaN        NaN   \n",
       "1        NaN       nan           NaN           NaN       NaN        NaN   \n",
       "2        NaN       nan           NaN           NaN       NaN        NaN   \n",
       "3        NaN       nan           NaN           NaN       NaN        NaN   \n",
       "4        NaN       nan           NaN           NaN       NaN        NaN   \n",
       "..       ...       ...           ...           ...       ...        ...   \n",
       "644      NaN       nan           NaN           NaN       NaN        NaN   \n",
       "645      NaN       nan           NaN           NaN       NaN        NaN   \n",
       "646      NaN       nan           NaN           NaN       NaN        NaN   \n",
       "647      NaN       nan           NaN           NaN       NaN        NaN   \n",
       "648      NaN       nan           NaN           NaN       NaN        NaN   \n",
       "\n",
       "                                              Document AnnTxtFile  \n",
       "0    Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "1    Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "2    Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "3    Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "4    Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "..                                                 ...        ...  \n",
       "644  Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "645  Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "646  Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "647  Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "648  Archer2017_Article_EndolithicMicrobialDiversit...        NaN  \n",
       "\n",
       "[649 rows x 49 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all the tagged locations\n",
    "TaggedLocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:36:49.613714Z",
     "start_time": "2021-02-28T04:36:49.610118Z"
    }
   },
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1350px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
