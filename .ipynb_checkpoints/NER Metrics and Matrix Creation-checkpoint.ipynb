{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics on everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:30:56.753705Z",
     "start_time": "2021-03-13T07:30:56.743885Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import difflib\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "from spacy.matcher   import Matcher\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of lists of files contained within folders\n",
    "# Pass the root folder as a string eg: 'folder\\\\'\n",
    "def listAllFilePairs(rootFolder):\n",
    "    \n",
    "    # Start with empty list\n",
    "    returnList = list()\n",
    "    \n",
    "    # Walk down from the source folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(rootFolder):\n",
    "        \n",
    "        # If there are files in the folder\n",
    "        if (len(filenames) > 0):\n",
    "            \n",
    "            # New list\n",
    "            files = []\n",
    "            \n",
    "            # Add the list of filenames to the return list\n",
    "            for fileName in filenames:\n",
    "                \n",
    "                # Add to the list of files\n",
    "                files += [os.path.join(dirpath, fileName)]\n",
    "                                       \n",
    "            # Add to the return list\n",
    "            returnList += [files]\n",
    "                                       \n",
    "    # Finally, return the list\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of tuples contains all paths and files of a specific type in a directory and its sub directory\n",
    "# Pass an extension to override .json\n",
    "def listAllFiles(folder, extension = '.json'):\n",
    "    files = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        files += [(dirpath, file) for file in filenames if file.endswith(extension)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:30.601571Z",
     "start_time": "2021-03-13T07:31:30.550444Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEntitiesandRelations(annotationsFile, txtFile):\n",
    "\n",
    "    # Open JSON file with annotations\n",
    "    file                      = open(annotationsFile, encoding = \"utf-8\")\n",
    "    annotationsDoc            = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # Load the annotations\n",
    "    annotations               = json.loads(annotationsDoc)\n",
    "\n",
    "    # Read the matching .txt file\n",
    "    file                      = open(txtFile, encoding = \"utf-8\")\n",
    "    plainText                 = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # tokenize the .txt file\n",
    "    doc                       = nlp_lg(plainText)\n",
    "\n",
    "    # Extract relevant information from JSON files for entities\n",
    "    entities                  = extractEntities(annotations)\n",
    "\n",
    "    # Offset the entities\n",
    "    entities                  = offsetEntities(entities, plainText)\n",
    "#\n",
    "    # Add the start token to the dataframe\n",
    "    entities['Start_Token']   = entities.adj_Start.apply(lambda x: get_token_num_for_char(doc, x))\n",
    "\n",
    "    # Add the end token to the dataframe\n",
    "    entities['End_Token']     = [get_token_num_for_end_char(doc, entities.loc[X]['Start_Token'], entities.loc[X]['adj_doctext']) for X in entities.index]\n",
    "\n",
    "    #Create a dictionary for relationship indices\n",
    "    positionDict              = dict(zip(entities.Start_Index, entities.adj_Start))\n",
    "\n",
    "    # Filter to specific columns \n",
    "    entities                  = entities[['adj_doctext', 'Start_Token','End_Token','adj_Start', 'adj_End', 'classId']]\n",
    "       \n",
    "    # Add columns back in\n",
    "    entities.columns          = ['Text', 'Start_Token','End_Token','Start_Index','End_Index','classId']\n",
    "   \n",
    "    # Get rid of the junk rows\n",
    "    if (annotationsFile.split('\\\\')[1] == 'OldSet'):\n",
    "        entities = filterOld(entities)\n",
    "    else:\n",
    "        entities = filterNew(entities)\n",
    "  \n",
    "    # Set that start token\n",
    "    entities['Sentence']      = [doc[entities.loc[X]['Start_Token']].sent for X in entities.index]\n",
    "  \n",
    "    # Find abstract entities\n",
    "    entities                  = findAbstractEntities(entities, doc)\n",
    "     \n",
    "    # Calculate the TFSIF\n",
    "    entities['max_TFISF']     = calculateTFISF(zip(entities.Start_Token, entities.End_Token), doc) \n",
    "  \n",
    "    # I think this is treated as a boolean value, based on what the value os is_sent_start is but I don't know what is_sent_start is\n",
    "    entities['Sent_Start']    = [ # List comprehension\n",
    "                                  1 if   doc[entities.loc[index]['Start_Token']].is_sent_start == True \n",
    "                                  else 0 \n",
    "                                  for index in entities.index\n",
    "                                ]\n",
    "\n",
    "    # Add the name of the document to a document column in the entities dataframe\n",
    "    entities['Document']      = txtFile.split('\\\\')[-1][:-4] \n",
    "    entities['AnnTxtFile']    = txtFile\n",
    "\n",
    "    # Make a lit of columns to downcast\n",
    "    cols_to_downcast          = ['Start_Token', \n",
    "                                 'End_Token', \n",
    "                                 'Start_Index',\n",
    "                                 'End_Index', \n",
    "                                 'inAbstract300', \n",
    "                                 'inAbstract500', \n",
    "                                 'Sent_Start']\n",
    "\n",
    "    # Downcast the columns\n",
    "    entities[cols_to_downcast] = entities[cols_to_downcast].apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "    # Create dataframes of annotated species and locations \n",
    "    if (annotationsFile.split('\\\\')[1] == 'OldSet'):\n",
    "        species   = filterSpeciesOld(entities)\n",
    "        locations = filterLocationsOld(entities)\n",
    "    else:\n",
    "        species   = filterSpeciesNew(entities)\n",
    "        locations = filterLocationsNew(entities)\n",
    "        \n",
    "    species.drop(columns   = ['classId'])  \n",
    "    locations.drop(columns = ['classId'])\n",
    "    \n",
    "    # Extract the relationships from the Annotations file\n",
    "    relationships              = extractRelationships(annotations)\n",
    "    \n",
    "    #Rearrange order of entities so species always appears on left hand side of the pair \n",
    "    relationships[['entity1-type','entity1-start','entity1-end','entity2-type','entity2-start','entity2-end']] = relationships[['entity1-type','entity1-start','entity1-end','entity2-type','entity2-start','entity2-end']].mask(\n",
    "        relationships['entity1-type'] == 'location', relationships[['entity2-type','entity2-start','entity2-end','entity1-type','entity1-start','entity1-end']].values)\n",
    "    \n",
    "    # Adjust the relationships indices\n",
    "    relationships = adjustRelationshipEntityIndices(relationships, positionDict)\n",
    "\n",
    "    # Insert names into dataframe\n",
    "    relationships = insertEntityNames(relationships, plainText)\n",
    "\n",
    "    # Insert token positions to the dataframe\n",
    "    relationships = insertTokenNumbers(relationships, doc)\n",
    "\n",
    "    # Add Ground Truth Value for predictions\n",
    "    relationships['Tagged_Relationship'] = 1\n",
    "\n",
    "    # Tidy the whole thing up\n",
    "    relationships.drop(columns = ['entity1-class',\n",
    "                                  'entity2-class',\n",
    "                                  'entity1-type', \n",
    "                                  'entity2-type',\n",
    "                                  'entity1-start',\n",
    "                                  'entity1-end',\n",
    "                                  'entity2-start',\n",
    "                                  'entity2-end'], inplace = True)\n",
    "\n",
    "    relationships.columns      = ['Start_Index_Species', \n",
    "                                  'End_Index_Species',\n",
    "                                  'Start_Index_Location', \n",
    "                                  'End_Index_Location', \n",
    "                                  'Species', \n",
    "                                  'Location', \n",
    "                                  'Start_Token_Species',\n",
    "                                  'End_Token_Species',\n",
    "                                  'Start_Token_Location',\n",
    "                                  'End_Token_Location',\n",
    "                                  'Tagged_Relationship']\n",
    "\n",
    "    relationships               = relationships[['Species', \n",
    "                                   'Start_Token_Species',\n",
    "                                   'End_Token_Species',\n",
    "                                   'Start_Index_Species', \n",
    "                                   'End_Index_Species', \n",
    "                                   'Location',\n",
    "                                   'Start_Token_Location',\n",
    "                                   'End_Token_Location',\n",
    "                                   'Start_Index_Location', \n",
    "                                   'End_Index_Location',\n",
    "                                   'Tagged_Relationship']]\n",
    "\n",
    "    # return three dataframes\n",
    "    return species, locations, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:33.097271Z",
     "start_time": "2021-03-13T07:31:33.091849Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculateTFISF(tokens, doc):\n",
    "    results       = []\n",
    "    numSentsinDoc = len(list(doc.sents))\n",
    "    \n",
    "    for startToken,endToken in tokens:\n",
    "        maxtfisf = 0\n",
    "        for tkn in range(startToken,endToken):\n",
    "            if doc[tkn].is_alpha:\n",
    "                tf       = len([1 for word in doc[tkn].sent if doc[tkn].text == str(word)])\n",
    "                isf      = np.log(numSentsinDoc / len([1 for sent in doc.sents if doc[tkn].text in str(sent)]))\n",
    "                tfisf    = tf*isf\n",
    "                maxtfisf = max(maxtfisf, tfisf)\n",
    "        results.append(maxtfisf)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:34.221396Z",
     "start_time": "2021-03-13T07:31:34.217779Z"
    }
   },
   "outputs": [],
   "source": [
    "def findAbstractStart(doc):\n",
    "    matcher = Matcher(nlp_lg.vocab)\n",
    "    pattern = [\n",
    "        [{\"LOWER\": \"abstract\"}]\n",
    "    ]\n",
    "    matcher.add(\"abstract_pattern\", pattern)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    if len(matches) >0:\n",
    "        AbstractToken = matches[0][1]\n",
    "    else: AbstractToken = -1\n",
    "        \n",
    "    return AbstractToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:35.032921Z",
     "start_time": "2021-03-13T07:31:35.027214Z"
    }
   },
   "outputs": [],
   "source": [
    "def inAbstract(tokens, AbstractStart, doc, limit):\n",
    "    if AbstractStart == -1:\n",
    "        return np.zeros(len(list(tokens)))\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    for startToken,endToken in tokens:\n",
    "        matcher = Matcher(nlp_lg.vocab)\n",
    "        pattern = []\n",
    "        \n",
    "        for tkn in range (startToken,endToken):\n",
    "            pattern.append({\"TEXT\": doc[tkn].text})\n",
    "            \n",
    "        patterns = [pattern]\n",
    "        matcher.add(\"species_pattern\", patterns)\n",
    "        matches  = matcher(doc)\n",
    "        idx      = [match[1] for match in matches if match[1] in range(AbstractStart,AbstractStart+limit)]\n",
    "        \n",
    "        if len(idx)>0:\n",
    "            results.append(1)\n",
    "        else: results.append(0)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:36.114104Z",
     "start_time": "2021-03-13T07:31:36.110547Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_num_for_char(doc, start_idx):\n",
    "    for i, token in enumerate(doc):\n",
    "        if start_idx > token.idx:\n",
    "            continue\n",
    "        if start_idx == token.idx:\n",
    "            return i\n",
    "        if start_idx < token.idx:\n",
    "            return i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T07:31:37.798619Z",
     "start_time": "2021-03-13T07:31:37.794946Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_num_for_end_char(doc, start_token, text):\n",
    "    end_token = start_token+1\n",
    "    span      = doc[start_token:end_token]\n",
    "    \n",
    "    while len(span.text)< len(text):\n",
    "        end_token += 1\n",
    "        span       = doc[start_token:end_token]\n",
    "        \n",
    "    return end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts entities from the parsed annotations JSON file\n",
    "# Must be passed the annotations in parsed JSON format\n",
    "def extractEntities(annotations):\n",
    "    \n",
    "    # Create data frame from Entities in JSON\n",
    "    entities                  = pd.DataFrame(annotations['entities'], columns = ['classId', 'offsets'])\n",
    "    \n",
    "    # Define columns in the dataframe\n",
    "    entities['Start_Index']   = [X[0].get('start') for X in entities['offsets']]\n",
    "    entities['Text']          = [X[0].get('text') for X in entities['offsets']]\n",
    "    entities['End_Index']     = [len(X[0].get('text'))+X[0].get('start') for X in entities['offsets']]\n",
    "    entities                  = entities[['Text', 'Start_Index', 'End_Index','classId']]\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off set entities \n",
    "def offsetEntities(entities, plainText):\n",
    "\n",
    "    # Offset the entities is a bit convoluted so I heavily commented it\n",
    "    # TagTog outputs its txt files as html. The txt is split into <pre>'s. \n",
    "    # The annotations file's start index keeps track of the character count from the start of the <pre>, \n",
    "    # Not the start of the txt, as is the way we calculate it\n",
    "    # Subsequently, each new pre resets the start index from 0. \n",
    "    # Because we work from the start of the txt file, we have to keep track of the last start/end index and work from there\n",
    "\n",
    "    # First, we need to add the working columns to the dataframe\n",
    "    entities['newStartIndex'] = 0\n",
    "    entities['newEndIndex']   = 0\n",
    "    entities['lastListedSI']  = 0\n",
    "    entities['offsetR']       = 0\n",
    "    entities['offsetL']       = 0\n",
    "    entities['adj_Start']     = 0\n",
    "    entities['adj_End']       = 0\n",
    "    entities['adj_doctext']   = \"\"\n",
    "\n",
    "    # We also need to keep track of the last items Start Index\n",
    "    lastSI                    = 0 # Stored per index\n",
    "    startIndex                = 0 # Calculated per index\n",
    "    lastListedSI              = 0 # Required for calculations\n",
    "\n",
    "    # And the last items End Index\n",
    "    lastEI                    = 0\n",
    "    endIndex                  = 0\n",
    "\n",
    "    # This boolean lets us know if we've gone past the first <pre>\n",
    "    reset                     = False\n",
    "\n",
    "    # Iterate through the entities\n",
    "    for index in entities.index:\n",
    "\n",
    "        # Check if the start index has reset\n",
    "        listedSI = entities.at[index, 'Start_Index']\n",
    "\n",
    "        # Most of the entries for longer pdfs will be outside of the first <pre> so we start with the reset check as\n",
    "        # it will speed up the process\n",
    "        if reset == True:\n",
    "\n",
    "            # If it's reset but we've reached a new <pre> we don't need to do anything fancy\n",
    "            if listedSI < lastListedSI:\n",
    "\n",
    "                # So we add the set startIndex to the last known start index (stored in lastSI)\n",
    "                startIndex = lastSI + listedSI \n",
    "\n",
    "            # Otherwise...\n",
    "            else:\n",
    "\n",
    "                # We need to manipulate the startIndex to find the new accurate start index in the text document\n",
    "                startIndex = listedSI - lastListedSI + lastSI\n",
    "\n",
    "        # If we are still in the first <pre> we need to check for the first entry of the next <pre>\n",
    "        elif listedSI < lastSI:\n",
    "\n",
    "            # We've now entered the next <pre> so we add the listed startIndex to the last known start index (stored in lastSI)\n",
    "            startIndex = lastSI + listedSI \n",
    "\n",
    "            # And set the reset flag as true to indicate we have gone past the first pre\n",
    "            reset      = True\n",
    "\n",
    "        # Otherwise\n",
    "        else:\n",
    "\n",
    "            # We are still in the first <pre> so we just use the accurate index\n",
    "            startIndex = listedSI\n",
    "\n",
    "        # Here we keep track of the index variables needed on the next iteration\n",
    "        lastSI       = startIndex\n",
    "        lastListedSI = listedSI\n",
    "        endIndex     = startIndex + len(entities.at[index, 'Text']) \n",
    "        lastEI       = endIndex\n",
    "\n",
    "        # Now the offset work begins\n",
    "        # Find the offset on the right\n",
    "        offsetR = startIndex - plainText[:endIndex].rfind(entities.loc[index]['Text'])\n",
    "\n",
    "        # Set it to a valid high integer if it went off the text\n",
    "        if offsetR == -1:\n",
    "\n",
    "            entities.at[index, 'offsetR'] = 99999999\n",
    "\n",
    "        else:\n",
    "\n",
    "            entities.at[index, 'offsetR'] = offsetR        \n",
    "\n",
    "        # Find the offset on the left\n",
    "        offsetL = plainText[startIndex:].find(entities.loc[index]['Text'])\n",
    "\n",
    "        # Set it to a valid high integer if it went off the text\n",
    "        if offsetL == -1:\n",
    "\n",
    "            entities.at[index, 'offsetL'] = 99999999\n",
    "\n",
    "        else:\n",
    "            entities.at[index, 'offsetL'] = offsetL\n",
    "\n",
    "        # Check which side we need to offset on and do it\n",
    "        if offsetR < offsetL:\n",
    "\n",
    "            # Adjust it left \n",
    "            adjustedStart = startIndex - offsetR\n",
    "\n",
    "        else: \n",
    "\n",
    "            # Adjust it right\n",
    "            adjustedStart = startIndex + offsetL\n",
    "\n",
    "        # Set the adjusted start\n",
    "        entities.at[index, 'adj_Start']      = adjustedStart\n",
    "\n",
    "        # Set the adjusted end\n",
    "        adjustedEnd                          = endIndex - startIndex + adjustedStart\n",
    "        entities.at[index, 'adj_End']        = adjustedEnd - 1\n",
    "\n",
    "        # Set the adjusted text\n",
    "        entities.at[index, 'adj_doctext']    = plainText[adjustedStart: adjustedEnd]\n",
    "\n",
    "    # Return the data frame\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only the relevant TagTog annotations in the old data\n",
    "# We need to find a way to standardise the annotations both between trees but also between projects\n",
    "def filterNew(entities):\n",
    "    return entities[(entities.classId == 'e_1' )    #Bryophytes\n",
    "                  | (entities.classId == 'e_2' )    #Nematoda\n",
    "                  | (entities.classId == 'e_4' )    #Location\n",
    "                  | (entities.classId == 'e_41')    #Cyanobacteria\n",
    "                  | (entities.classId == 'e_49')    #Rotifers\n",
    "                  | (entities.classId == 'e_75')    #Algae\n",
    "                  | (entities.classId == 'e_38')    #Acari\n",
    "                  | (entities.classId == 'e_55')    #Cyanobacteria\n",
    "                  | (entities.classId == 'e_43')    #Mites\n",
    "                  | (entities.classId == 'e_32')    #Moss\n",
    "                  | (entities.classId == 'e_53')    #Rotier\n",
    "                  | (entities.classId == 'e_54')    #Protist\n",
    "                  | (entities.classId == 'e_57')    #Micro_Molecular\n",
    "                  | (entities.classId == 'e_7' )    #Lichen\n",
    "                  | (entities.classId == 'e_33')    #Nematode\n",
    "                  | (entities.classId == 'e_44')    #SpingTail\n",
    "                  | (entities.classId == 'e_48')    #Tardigrades\n",
    "                  | (entities.classId == 'e_39')    #Collembola\n",
    "                  | (entities.classId == 'e_40')    #Algae\n",
    "                  | (entities.classId == 'e_51')    #Tardigrade\n",
    "                  | (entities.classId == 'e_31')    #Lichen\n",
    "                  | (entities.classId == 'e_50')]   #Protists   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as species in the new documents\n",
    "def filterSpeciesNew(entities):\n",
    "    return entities[(entities.classId != 'e_4' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as locations in the new documents\n",
    "def filterLocationsNew(entities):\n",
    "    return entities[(entities.classId == 'e_4' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only the relevant TagTog annotations in the old data\n",
    "def filterOld(entities):\n",
    "    return entities[(entities.classId == 'e_1' )    #Species\n",
    "                  | (entities.classId == 'e_6' )    #Taxa\n",
    "                  | (entities.classId == 'e_2' )]   #Locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as species in the old documents\n",
    "def filterSpeciesOld(entities):\n",
    "    return entities[(entities.classId != 'e_2' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns only entries annotated as locations in the old documents\n",
    "def filterLocationsOld(entities):\n",
    "    return entities[(entities.classId == 'e_2' )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAbstractEntities(entities, doc):\n",
    "    \n",
    "    # Get the abstract\n",
    "    AbstractStart   = findAbstractStart(doc)\n",
    "\n",
    "    # Make two lists of entities within the abstract\n",
    "    entities['inAbstract300'] = inAbstract(zip(entities.Start_Token, entities.End_Token), AbstractStart, doc, 300) \n",
    "    entities['inAbstract500'] = inAbstract(zip(entities.Start_Token, entities.End_Token), AbstractStart, doc, 500) \n",
    "    \n",
    "    # Return\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from JSON file for relationships\n",
    "# Requires you to pass in the parsed annotations\n",
    "def extractRelationships(annotations):\n",
    "\n",
    "    # Create a data frame using current data\n",
    "    relationships                  = pd.DataFrame(annotations['relations'], columns = ['entities'])\n",
    "\n",
    "    # Add desire columns to the data frame\n",
    "    # for entity one\n",
    "    relationships['entity1-class']  = \"\"\n",
    "    relationships['entity1-type']   = \"\"\n",
    "    relationships['entity1-start']  = 0\n",
    "    relationships['entity1-end']    = 0\n",
    "\n",
    "    # for entity two\n",
    "    relationships['entity2-class']  = \"\"\n",
    "    relationships['entity2-type']   = \"\"\n",
    "    relationships['entity2-start']  = 0\n",
    "    relationships['entity2-end']    = 0\n",
    "\n",
    "    # Iterate through the current data\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Extract entities\n",
    "        entityOne, entityTwo                         = relationships.at[index, 'entities']\n",
    "\n",
    "        # Extract relevant details\n",
    "        # Entity one\n",
    "        throwAway, entityOneClass, entityOneStartEnd = entityOne.split('|')\n",
    "        entityOneStart, entityOneEnd                 = entityOneStartEnd.split(',')\n",
    "\n",
    "        relationships.at[index, 'entity1-class']     = entityOneClass\n",
    "        relationships.at[index, 'entity1-type']      = \"location\" if entityOneClass == 'e_4' else \"species\"\n",
    "        relationships.at[index, 'entity1-start']     = entityOneStart\n",
    "        relationships.at[index, 'entity1-end']       = entityOneEnd\n",
    "\n",
    "        # Entity two\n",
    "        throwAway, entityTwoClass, entityTwoStartEnd = entityTwo.split('|')\n",
    "        entityTwoStart, entityTwoEnd                 = entityTwoStartEnd.split(',')\n",
    "\n",
    "        relationships.at[index, 'entity2-class']     = entityTwoClass \n",
    "        relationships.at[index, 'entity2-type']      = \"location\" if entityTwoClass == 'e_4' else \"species\"\n",
    "        relationships.at[index, 'entity2-start']     = entityTwoStart\n",
    "        relationships.at[index, 'entity2-end']       = entityTwoEnd\n",
    "\n",
    "    # Dopr entities column    \n",
    "    relationships.drop(columns = 'entities', inplace=True)\n",
    "    \n",
    "    # Return\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert actual text of the entities into the relationships dataframe\n",
    "def insertEntityNames(relationships, document):\n",
    "\n",
    "    # Iterate through the relationships\n",
    "    for index in relationships.index:\n",
    "        \n",
    "        # Entity One\n",
    "        relationships.at[index, 'entity1'] = document[relationships.at[index, 'entity1-adjstart']: \n",
    "                                                      relationships.at[index, 'entity1-adjend'] + 1] \n",
    "\n",
    "        # Entity Two\n",
    "        relationships.at[index, 'entity2'] = document[relationships.at[index, 'entity2-adjstart']: \n",
    "                                                      relationships.at[index, 'entity2-adjend'] + 1] \n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust entity indices to match document\n",
    "def adjustRelationshipEntityIndices(relationships, positionDict):\n",
    "\n",
    "    # Create the desired columns\n",
    "    relationships['entity1-adjstart'] = 0\n",
    "    relationships['entity1-adjend']   = 0\n",
    "    relationships['entity2-adjstart'] = 0\n",
    "    relationships['entity2-adjend']   = 0\n",
    "\n",
    "    # Iterate through relationships\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Get adjusted start for Entity One\n",
    "        relationships.at[index, 'entity1-adjstart'] = positionDict.get(relationships.at[index, 'entity1-start'])\n",
    "\n",
    "        # Calculate adjusted end for Entity Two\n",
    "        entity1End    = relationships.at[index, 'entity1-end']\n",
    "        entity1Start  = relationships.at[index, 'entity1-start']\n",
    "        entity1AdjEnd = relationships.at[index, 'entity1-adjstart']\n",
    "        relationships.at[index, 'entity1-adjend']   =  entity1End - entity1Start + entity1AdjEnd\n",
    "\n",
    "        # Get adjusted start for Entity One\n",
    "        relationships.at[index, 'entity2-adjstart'] = positionDict.get(relationships.at[index, 'entity2-start'])\n",
    "\n",
    "        # Calculate adjusted end for Entity Two\n",
    "        entity2End    = relationships.at[index, 'entity2-end']\n",
    "        entity2Start  = relationships.at[index, 'entity2-start']\n",
    "        entity2AdjEnd = relationships.at[index, 'entity2-adjstart']\n",
    "        relationships.at[index, 'entity2-adjend']   =  entity2End - entity2Start + entity2AdjEnd\n",
    "        \n",
    "    # Return\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add token numbers to the relationships tables\n",
    "def insertTokenNumbers(relationships, doc):\n",
    "\n",
    "    # Add desires columns\n",
    "    relationships['Start_Token_Species'] = 0\n",
    "    relationships['End_Token_Species']   = 0\n",
    "\n",
    "    # Iterate through the relationships\n",
    "    for index in relationships.index:\n",
    "\n",
    "        # Get the starting tokens\n",
    "        # Species\n",
    "        tokenNum = relationships.at[index, 'entity1-adjstart']\n",
    "        relationships.at[index, 'Start_Token_Species']  = get_token_num_for_char(doc, tokenNum)\n",
    "\n",
    "        # Location\n",
    "        tokenNum = relationships.at[index, 'entity2-adjstart']\n",
    "        relationships.at[index, 'Start_Token_Location'] = get_token_num_for_char(doc, tokenNum)\n",
    "\n",
    "        # Get end tokens\n",
    "        # Species\n",
    "        relationships.at[index, 'End_Token_Species']    = get_token_num_for_end_char(\n",
    "             doc, relationships.at[index, 'Start_Token_Species'], relationships.at[index, 'entity1'])\n",
    "\n",
    "        # Location\n",
    "        relationships.at[index, 'End_Token_Location']    = get_token_num_for_end_char(\n",
    "         doc, relationships.at[index, 'Start_Token_Location'], relationships.at[index, 'entity2'])\n",
    "        \n",
    "    # Return relationships\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the Extracted CSV folder and find the CSV\n",
    "def findExtractedCSV(txtFile, CSVFolder, fileTag):\n",
    "    \n",
    "    # Check if we are using oldset or new set\n",
    "    # Then get the CSV name\n",
    "    if txtFile.split('\\\\')[1] == 'OldSet':\n",
    "        speciesCSVFileName = re.sub('.txt.txt', fileTag, txtFile)\n",
    "    else:\n",
    "        speciesCSVFileName = re.sub('.pdf.txt', fileTag, txtFile)\n",
    "    speciesCSVFileName     = speciesCSVFileName.split('\\\\')[-1]\n",
    "    \n",
    "    # Iterate through the folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(CSVFolder):\n",
    "        for filename in filenames:\n",
    "            if (filename == speciesCSVFileName):\n",
    "                return True, os.path.join(dirpath, filename)\n",
    "        \n",
    "    # return -1 on fail\n",
    "    return False, speciesCSVFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeciesMetrics(speciesMatchesDF):\n",
    "    Document       = []\n",
    "    Tagged         = []\n",
    "    Extracted      = []\n",
    "    TruePositive   = []\n",
    "    FalsePositive  = []\n",
    "    FalseNegative  = []\n",
    "    Precision      = []\n",
    "    Recall         = []\n",
    "    F1             = []\n",
    "    partialMatches = []\n",
    "\n",
    "    for doc in speciesMatchesDF.Document.unique():\n",
    "        Document.append(doc)\n",
    "        \n",
    "        ActualResults    = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Text'].notna())])\n",
    "        \n",
    "        PredictedResults = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Found_as'].notna())])\n",
    "        \n",
    "        Tagged.append(ActualResults)\n",
    "        \n",
    "        Extracted.append(PredictedResults)\n",
    "        \n",
    "        TP               = len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& (speciesMatchesDF['Found_as'].notna()) & (speciesMatchesDF['Text'].notna())])\n",
    "        \n",
    "        TruePositive.append(TP)\n",
    "        \n",
    "        FalsePositive.append(len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].isna()]))\n",
    "        \n",
    "        FalseNegative.append(len(speciesMatchesDF[(speciesMatchesDF.Document==doc)& speciesMatchesDF['Found_as'].isna() & speciesMatchesDF['Text'].notna()]))\n",
    "        \n",
    "        notTagged        = speciesMatchesDF[(speciesMatchesDF.Document ==doc) & speciesMatchesDF.Text.isna()]\n",
    "        \n",
    "        notExtracted     = speciesMatchesDF[(speciesMatchesDF.Document==doc) & speciesMatchesDF.Found_as.isna()]\n",
    "        \n",
    "        closeMatches     = 0\n",
    "        \n",
    "        for i in notTagged.index:\n",
    "            \n",
    "            for j in notExtracted.index:\n",
    "                \n",
    "                if notTagged.loc[i]['Start_Index']>=notExtracted.loc[j]['Start_Index'] and notTagged.loc[i]['End_Index']<=notExtracted.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "                    \n",
    "        for i in notExtracted.index:\n",
    "            \n",
    "            for j in notTagged.index:\n",
    "                \n",
    "                if notExtracted.loc[i]['Start_Index']>=notTagged.loc[j]['Start_Index'] and notExtracted.loc[i]['End_Index']<=notTagged.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "                    \n",
    "        partialMatches.append(closeMatches)\n",
    "\n",
    "    Document.append('Total')\n",
    "    \n",
    "    ActualResults    = len(speciesMatchesDF[speciesMatchesDF['Text'].notna()])\n",
    "    \n",
    "    PredictedResults = len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna()])\n",
    "    \n",
    "    Tagged.append(ActualResults)\n",
    "    \n",
    "    Extracted.append(PredictedResults)\n",
    "    \n",
    "    TP               =len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].notna()])\n",
    "    \n",
    "    TruePositive.append(TP)\n",
    "    \n",
    "    FalsePositive.append(len(speciesMatchesDF[speciesMatchesDF['Found_as'].notna() & speciesMatchesDF['Text'].isna()]))\n",
    "    \n",
    "    FalseNegative.append(len(speciesMatchesDF[speciesMatchesDF['Found_as'].isna() & speciesMatchesDF['Text'].notna()]))\n",
    "    \n",
    "    partialMatches.append(sum(partialMatches))\n",
    "\n",
    "    results                   = pd.DataFrame({'Document': Document, 'Tagged':Tagged, 'Extracted':Extracted, 'True Positives':TruePositive,\n",
    "                                              'False Positives':FalsePositive, 'False Negatives':FalseNegative, 'Partial Matches':partialMatches,\n",
    "                                             })\n",
    "    \n",
    "    results['Precision']      = np.round(results['True Positives']/results['Extracted'], 4)\n",
    "    \n",
    "    results['Recall']         = np.round(results['True Positives']/results['Tagged'], 4)\n",
    "    \n",
    "    results['F1']             = np.round(2* ((results['Precision']*results['Recall'])/(results['Precision']+results['Recall'])), 4)\n",
    "    \n",
    "    results['Adj. Precision'] = np.round((results['True Positives']+results['Partial Matches'])/results['Extracted'], 4)\n",
    "    \n",
    "    results['Adj. Recall']    = np.round((results['True Positives']+results['Partial Matches'])/results['Tagged'], 4)\n",
    "    \n",
    "    results['Adj. F1']        = np.round(2* ((results['Adj. Precision']*results['Adj. Recall'])/(results['Adj. Precision']+results['Adj. Recall'])), 4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocationsMetrics(locationsMatchesDF):\n",
    "    \n",
    "    Document       = []\n",
    "    Tagged         = []\n",
    "    Extracted      = []\n",
    "    TruePositive   = []\n",
    "    FalsePositive  = []\n",
    "    FalseNegative  = []\n",
    "    Precision      = []\n",
    "    Recall         = []\n",
    "    F1             = []\n",
    "    partialMatches = []\n",
    "\n",
    "    for doc in locationsMatchesDF.Document.unique():\n",
    "        Document.append(doc)\n",
    "        \n",
    "        ActualResults    = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['TaggedLocation'].notna())])\n",
    "        \n",
    "        PredictedResults = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['ExtractedLocation'].notna())])\n",
    "        \n",
    "        Tagged.append(ActualResults)\n",
    "        \n",
    "        Extracted.append(PredictedResults)\n",
    "        \n",
    "        TP               = len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& (locationsMatchesDF['ExtractedLocation'].notna()) & (locationsMatchesDF['TaggedLocation'].notna())])\n",
    "        \n",
    "        TruePositive.append(TP)\n",
    "        \n",
    "        FalsePositive.append(len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].isna()]))\n",
    "        \n",
    "        FalseNegative.append(len(locationsMatchesDF[(locationsMatchesDF.Document==doc)& locationsMatchesDF['ExtractedLocation'].isna() & locationsMatchesDF['TaggedLocation'].notna()]))\n",
    "        \n",
    "        notTagged        = locationsMatchesDF[(locationsMatchesDF.Document ==doc) & locationsMatchesDF.TaggedLocation.isna()]\n",
    "        \n",
    "        notExtracted     = locationsMatchesDF[(locationsMatchesDF.Document==doc) & locationsMatchesDF.ExtractedLocation.isna()]\n",
    "        \n",
    "        closeMatches     = 0\n",
    "        \n",
    "        for i in notTagged.index:\n",
    "        \n",
    "            for j in notExtracted.index:\n",
    "        \n",
    "                if notTagged.loc[i]['Start_Index']>=notExtracted.loc[j]['Start_Index'] and notTagged.loc[i]['End_Index']<=notExtracted.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "        \n",
    "        for i in notExtracted.index:\n",
    "        \n",
    "            for j in notTagged.index:\n",
    "        \n",
    "                if notExtracted.loc[i]['Start_Index']>=notTagged.loc[j]['Start_Index'] and notExtracted.loc[i]['End_Index']<=notTagged.loc[j]['End_Index']:\n",
    "                    closeMatches += 1\n",
    "        \n",
    "        partialMatches.append(closeMatches)\n",
    "\n",
    "    Document.append('Total')\n",
    "    \n",
    "    ActualResults    = len(locationsMatchesDF[locationsMatchesDF['TaggedLocation'].notna()])\n",
    "    \n",
    "    PredictedResults = len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna()])\n",
    "    \n",
    "    Tagged.append(ActualResults)\n",
    "    \n",
    "    Extracted.append(PredictedResults)\n",
    "    \n",
    "    TP               = len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].notna()])\n",
    "    \n",
    "    TruePositive.append(TP)\n",
    "    \n",
    "    FalsePositive.append(len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].notna() & locationsMatchesDF['TaggedLocation'].isna()]))\n",
    "    \n",
    "    FalseNegative.append(len(locationsMatchesDF[locationsMatchesDF['ExtractedLocation'].isna() & locationsMatchesDF['TaggedLocation'].notna()]))\n",
    "    \n",
    "    partialMatches.append(sum(partialMatches))\n",
    "\n",
    "    results                   = pd.DataFrame({'Document': Document, 'Tagged':Tagged, 'Extracted':Extracted, 'True Positives':TruePositive,\n",
    "                                              'False Positives':FalsePositive, 'False Negatives':FalseNegative, 'Partial Matches':partialMatches,\n",
    "                                             })\n",
    "    \n",
    "    results['Precision']      = np.round(results['True Positives']/results['Extracted'], 4)\n",
    "    \n",
    "    results['Recall']         = np.round(results['True Positives']/results['Tagged'], 4)\n",
    "    \n",
    "    results['F1']             = np.round(2* ((results['Precision']*results['Recall'])/(results['Precision']+results['Recall'])), 4)\n",
    "    \n",
    "    results['Adj. Precision'] = np.round((results['True Positives']+results['Partial Matches'])/results['Extracted'], 4)\n",
    "    \n",
    "    results['Adj. Recall']    = np.round((results['True Positives']+results['Partial Matches'])/results['Tagged'], 4)\n",
    "    \n",
    "    results['Adj. F1']        = np.round(2* ((results['Adj. Precision']*results['Adj. Recall'])/(results['Adj. Precision']+results['Adj. Recall'])), 4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_taggedlocations_df(df, gazetters, filePairs):\n",
    "    places           = []\n",
    "\n",
    "    # Gazetter separation\n",
    "    nzGaz            = gazetters[0]\n",
    "    nzGazAntarctica  = gazetters[1]\n",
    "    scarGlobalNames  = gazetters[2]\n",
    "    scarNzNames      = gazetters[3]\n",
    "    geoNamesAnt      = gazetters[4]\n",
    "    geoNamesNZ       = gazetters[5]\n",
    "    geoNamesFiltered = gazetters[6]\n",
    "    \n",
    "    # Create columns on the dataframes\n",
    "    data               = []\n",
    "    columns            = ['id', 'Location']\n",
    "    toConCat           = pd.DataFrame(data=data,columns=columns)\n",
    "    df                 = pd.concat([df,toConCat], axis=1)\n",
    "\n",
    "    # We need to do a stupid iteration here because the list comprehension doesn't grab all the rows\n",
    "    # Iterate through locations\n",
    "    \n",
    "    i = 0\n",
    "    for row in df.index:\n",
    "        \n",
    "        # Give them all a unique id to later be used for indexing\n",
    "        df.at[row, 'id'] = i\n",
    "        i += 1\n",
    "        \n",
    "        # Set some booleans first up\n",
    "        df.at[row, 'inNZ']          = False\n",
    "        df.at[row, 'inAntarctica']  = False\n",
    "        df.at[row, 'exactMatch']    = False\n",
    "            \n",
    "        # Get location and leading four letters\n",
    "        location = str(df.loc[row, 'TaggedLocation'])\n",
    "        \n",
    "        # Add place but drop leading words 'The'/'the' and replace leading words 'Mt.'/'mt' with 'Mount'\n",
    "        if location.startswith('The '):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "            \n",
    "        elif location.startswith('the '):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "            \n",
    "        elif location.startswith('Mt.'):\n",
    "            df.at[row, 'Location'] = location[4:]\n",
    "        \n",
    "        elif location.startswith('Mt '):\n",
    "            df.at[row, 'Location'] = location[3:]\n",
    "\n",
    "        # Check possible locations in gazetteers for exact matches\n",
    "        if location in nzGazAntarctica:\n",
    "            df.at[row, 'NZGazAnt']    = True\n",
    "            df.at[row, 'inNZ'    ]    = True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "        else:\n",
    "            df.at[row, 'NZGazAnt']    = False\n",
    "            \n",
    "        if location in nzGaz:\n",
    "            df.at[row, 'NZGaz']       = True\n",
    "            df.at[row, 'inNZ' ]       = True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'NZGaz']       = False\n",
    "            \n",
    "        if location in scarNzNames:\n",
    "            df.at[row, 'ScarNZ']      = True\n",
    "        else:\n",
    "            df.at[row, 'ScarNZ']      = False\n",
    "                 \n",
    "        if location in scarGlobalNames:\n",
    "            df.at[row, 'ScarGlobal'  ]= True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'ScarGlobal']  = False\n",
    "\n",
    "        if location in geoNamesNZ:\n",
    "            df.at[row, 'GeoNamesNZ']  = True\n",
    "            df.at[row, 'inNZ'      ]  = True\n",
    "            df.at[row, 'inAntarctica']= True\n",
    "        else:\n",
    "            df.at[row, 'GeoNamesNZ']  = False\n",
    "                 \n",
    "        if location in geoNamesAnt:\n",
    "            df.at[row, 'GeoNamesAnt'] = True\n",
    "        else:\n",
    "            df.at[row, 'GeoNamesAnt'] = False\n",
    "                 \n",
    "        if location in geoNamesFiltered:\n",
    "            df.at[row, 'GeoNames']    = True\n",
    "            df.at[row, 'exactMatch']  = True\n",
    "        else:\n",
    "            df.at[row, 'GeoNames']    = False\n",
    "\n",
    "    # filter locations by those not found in Antarctica or New Zealand gazeteers \n",
    "    df2                = df[(df['inAntarctica'] == False) & (df['inNZ']== False)].copy()\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # look through these remaining locations (including those found only in GeoNames) for close matches \n",
    "    # eg. McMurdo Dry Valley v McMurdo Dry Valley or for partial matches eg. Ross Sea Region == Ross Sea\n",
    "    for row in df2.index:\n",
    "        \n",
    "        # This section is for close matches\n",
    "        # Set to not found\n",
    "        df2.at[row, 'Close_Match'] = False\n",
    "        \n",
    "        # Then iterate for close matches on the location as a string\n",
    "        location = str(df2.loc[row, 'Location'])\n",
    "            \n",
    "        # In Antarctica\n",
    "        matches = difflib.get_close_matches(location, nzGazAntarctica, cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_NZGazAnt']   = True #matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "            \n",
    "        # In NZ\n",
    "        matches = difflib.get_close_matches(location, nzGaz,           cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_NZGaz']      = True #matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "        \n",
    "        # In SCARNZ\n",
    "        matches = difflib.get_close_matches(location, scarNzNames,     cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_ScarNZ']     = True #matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    " \n",
    "        # In ScarGlobal\n",
    "        matches = difflib.get_close_matches(location, scarGlobalNames, cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_ScarGlobal'] = True #matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "    \n",
    "        # In GeoNamesNZ\n",
    "        matches = difflib.get_close_matches(location, geoNamesNZ,     cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_GeoNamesNZ'] = True #matches[0]\n",
    "            df2.at[row, 'Close_Match']            = True\n",
    "            \n",
    "        # In GeoNamesAntarctica\n",
    "        matches = difflib.get_close_matches(location, geoNamesAnt,    cutoff = 0.9)\n",
    "        if len(matches) > 0:\n",
    "            \n",
    "            # If more than zero, add best match to dataframe\n",
    "            df2.at[row, 'Close_Match_GeoNamesAnt'] = True #matches[0] \n",
    "            df2.at[row, 'Close_Match']             = True\n",
    "    \n",
    "        # This section is for partial matches\n",
    "        # Set the not found boolean\n",
    "        df2.at[row, 'PartialMatch'] = False\n",
    "        \n",
    "        # Get the tokenised document for that row\n",
    "        fileString = str(df2.loc[row, 'Document'])\n",
    "        doc        = getTokenisedDocument(fileString.replace('.txt', ''), filePairs)\n",
    "    \n",
    "        # Get the location, start and end tokens\n",
    "        startToken = df2.loc[row, 'Start_Token']\n",
    "        endToken   = df2.loc[row, 'End_Token']\n",
    "        \n",
    "        # Check the partial matches\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, nzGazAntartica)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_NZGazAnt']    = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc, startToken, endToken, nzGaz)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_NZGaz']       = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "            \n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, scarNzNames)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_ScarNZ']      = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, scarGlobalNames)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_ScarGlobal']  = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "            \n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, geoNamesNZ)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_GeoNamesNZ']  = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "\n",
    "        result = getBiggestSubStringMatch(doc,  startToken, endToken, geoNamesAnt)\n",
    "        if not result == 'NaN':\n",
    "            print(result)\n",
    "            df2.at[row, 'PartialMatch_GeoNamesAnt'] = True #result\n",
    "            df2.at[row, 'PartialMatch']             = True\n",
    "            \n",
    "    # merge the filtered dataframe (now with close and partial matches) with the unfiltered dataframe\n",
    "    df_unified                 = df.merge(df2, how = 'left')\n",
    "    \n",
    "    # redo the inNZ,inAntarctica and Found columns to include close and partial matches\n",
    "    df_unified.drop(columns    = ['inNZ', 'inAntarctica'], inplace = True)\n",
    "    \n",
    "    df_unified['inNZ']         = (df_unified.NZGaz \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesNZ \n",
    "                                | df_unified.Close_Match_NZGaz \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesNZ \n",
    "                                | df_unified.PartialMatch_NZGaz \n",
    "                                | df_unified.PartialMatch_NZGazAnt \n",
    "                                | df_unified.PartialMatch_GeoNamesNZ)\n",
    "    \n",
    "    df_unified['inAntarctica'] = (df_unified.ScarGlobal \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesAnt \n",
    "                                | df_unified.Close_Match_ScarGlobal \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesAnt \n",
    "                                | df_unified.PartialMatch_ScarGlobal \n",
    "                                | df_unified.PartialMatch_NZGazAnt \n",
    "                                | df_unified.PartialMatch_GeoNamesAnt)\n",
    "    \n",
    "    df_unified['Found']        = (df_unified.exactMatch \n",
    "                                | df_unified.Close_Match\n",
    "                                | df_unified.PartialMatch)\n",
    "    \n",
    "    return df_unified[df_unified.Found==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the annotated text file and tokenises it\n",
    "def getTokenisedDocument(document, filePairs):\n",
    "\n",
    "    # Find text file in filePairs\n",
    "    document  = filterFilePairs(fileCount, document, filePairs)\n",
    "    \n",
    "    # Filter to .txt\n",
    "    txtFile   = [file for file in filePair if file.endswith('.txt')][0]\n",
    "    \n",
    "    # Open .txt\n",
    "    file      = open(txtFile, encoding = \"utf-8\")\n",
    "    plainText = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    # tokenize the .txt file\n",
    "    doc       = nlp_lg(plainText)\n",
    "    \n",
    "    # Return\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestSubStringMatch(document, fullStart, fullEnd, gazetteer):\n",
    "       \n",
    "    subLength = fullEnd - fullStart - 1\n",
    "\n",
    "    while subLength > 1:\n",
    "\n",
    "        subStart = fullStart\n",
    "\n",
    "        subEnd   = fullStart + subLength\n",
    "\n",
    "        while subEnd <= fullEnd:\n",
    "\n",
    "            subString = document[subStart:subEnd].text\n",
    "\n",
    "            if subString in gazetteer:\n",
    "\n",
    "                return subString\n",
    "\n",
    "            subStart += 1\n",
    "\n",
    "            subEnd   += 1\n",
    "\n",
    "        subLength -= 1\n",
    "            \n",
    "    return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_num_for_char(doc, start_idx):\n",
    "    for i, token in enumerate(doc):\n",
    "        if start_idx > token.idx:\n",
    "            continue\n",
    "        if start_idx == token.idx:\n",
    "            return i\n",
    "        if start_idx < token.idx:\n",
    "            return i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterFilePairs(count, file, filePairs):\n",
    "    \n",
    "    # If no specific file is passed\n",
    "    if file == None:\n",
    "        \n",
    "        # Check if specific count required\n",
    "        if count > 0:\n",
    "\n",
    "            # Filter down to count\n",
    "            return filePairs[:count]\n",
    "        else:\n",
    "                \n",
    "            # Return all of them    \n",
    "            return filePairs\n",
    "        \n",
    "    # Use a specific file\n",
    "    for filePair in filePairs:\n",
    "\n",
    "        # This gets the foler of the filepairs and matches it to the filename you put it\n",
    "        fileName = [file for file in filePair if file.endswith('.txt')][0]\n",
    "        fileName = fileName.split('\\\\')[-1]\n",
    "        if fileName.replace('.txt', '') == file:\n",
    "\n",
    "            return [filePair]\n",
    "\n",
    "    print('ERROR: File not found')\n",
    "    print('File requested: specificFile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:** Gets the results of all the extracted files and annotated files and merges them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T03:48:26.922127Z",
     "start_time": "2021-03-14T03:48:21.598584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This notebook is really intensive so I've done a lot of print outs in very ordered, specific formatting \n",
    "# to make it easier for the human brain to follow what is happening\n",
    "\n",
    "# The output will clear on success, it is really only there incase you hit errors\n",
    "\n",
    "# Any errors you do hit should only be problems with input data\n",
    "\n",
    "# I have also broken it into two parts to help with running it\n",
    "\n",
    "# God I wish Python supported multi line comments\n",
    "\n",
    "# Working with all the files takes a ball-bustingly long time, so I've built in filters:\n",
    "\n",
    "# To work with a specific file, change this value to the fileName of the PDF \n",
    "# This should match the folder the files are sitting in\n",
    "# Example: 10.1007_BF00238925.pdf\n",
    "specificFile    = None#'Archer2017_Article_EndolithicMicrobialDiversityIn'\n",
    "\n",
    "# To work with all files, set this to zero\n",
    "fileCount       = 0\n",
    "\n",
    "# Load source files\n",
    "print('STAGE: Find source files')\n",
    "srcFolder       = 'ExtractedAnnotatedData\\\\'\n",
    "filePairs       = listAllFilePairs(srcFolder)\n",
    "\n",
    "# Filter to file count or file name\n",
    "print('STAGE: Filter files')\n",
    "filePairs       = filterFilePairs(fileCount, specificFile, filePairs)\n",
    "\n",
    "# Create data frames to append stuff to\n",
    "print('STAGE: Create metrics dataframes')\n",
    "allSpecMatches           = pd.DataFrame()\n",
    "allLocMatches            = pd.DataFrame()\n",
    "allRelAnnotated          = pd.DataFrame()\n",
    "allPossibleRelationships = pd.DataFrame()\n",
    "\n",
    "# Iterate through selected files\n",
    "print ('STAGE: Iterate files')\n",
    "for filePair in filePairs:\n",
    "    print('')\n",
    "    \n",
    "    # Split types\n",
    "    annotationsFile = [file for file in filePair if file.endswith('.json')][0]\n",
    "    txtFile         = [file for file in filePair if file.endswith('.txt')][0]\n",
    "    fileName        = txtFile.split('\\\\')[-1]\n",
    "    folder          = '\\\\'.join(txtFile.split('\\\\')[:-2])\n",
    "    print('FILE:    ' + fileName)\n",
    "    print('FOLDER:  ' + folder)\n",
    "    \n",
    "    # Pass files tuple to extract species, locations and relations from the annotations\n",
    "    print('COMMAND: Get annotated entities and relations')\n",
    "    speciesAnnotated, locationsAnnotated, relationsAnnotated = getEntitiesandRelations(annotationsFile, txtFile)\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Load the Extracted Species CSV file\n",
    "    print('COMMAND: Find extracted species CSV')\n",
    "    result, csv = findExtractedCSV(txtFile, 'ExtractedSpecies\\\\', '-Species.csv')\n",
    "    \n",
    "    if result == False:\n",
    "        print('RESULT:  Could not find Extracted Species CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    else:\n",
    "        print('RESULT:  Found extracted Species CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    \n",
    "    # Parse the csv\n",
    "    print('COMMAND: Parse extraction data')\n",
    "    speciesExtracted              = pd.read_csv(csv,index_col=0 )\n",
    "    speciesExtracted              = speciesExtracted[['Found as', 'Full name', 'Start', 'End','PositionOfFirstToken']]\n",
    "    speciesExtracted.columns      = ['Found_as', 'Full_name', 'Start_Token', 'End_Token', 'Start_Index']\n",
    "    speciesExtracted['End_Index'] = [len(speciesExtracted.loc[X]['Found_as'])+speciesExtracted.loc[X]['Start_Index'] for X in speciesExtracted.index]\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Match Species from annotated data to extracted data\n",
    "    print('COMMAND: Merge extracted and annotated species data')\n",
    "    matchesSpecies                = pd.merge(speciesExtracted, speciesAnnotated, how = 'outer')\n",
    "    matchesSpecies['Match']       = matchesSpecies.Found_as == matchesSpecies.Text\n",
    "    matchesSpecies['Document']    = fileName   \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Load the Extracted Locations CSV file\n",
    "    print('COMMAND: Find extracted locations csv')\n",
    "    result, csv = findExtractedCSV(txtFile, 'ExtractedLocations\\\\', '-Locations.csv')\n",
    "    \n",
    "    if result == False:\n",
    "        print('RESULT:  Could not find extracted locations CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "    else:\n",
    "        print('RESULT:  Found extracted locations CSV for ' + fileName + ' as below')\n",
    "        print('         ' + csv)\n",
    "        \n",
    "    # Parse the CSV\n",
    "    print('COMMAND: Parse extracted data')\n",
    "    locationsExtracted              = pd.read_csv(csv,index_col=0 )  \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Match locations from annotated data to extracted data\n",
    "    print('COMMAND: Match extracted and annotated location data')\n",
    "    matchesLocations                = pd.merge(locationsExtracted, locationsAnnotated, how='outer')\n",
    "    matchesLocations.rename(columns = {'Location':'ExtractedLocation','Text':'TaggedLocation'}, inplace=True)\n",
    "    matchesLocations['Document']    = fileName\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Append to over all dataframes\n",
    "    print('COMMAND: Append to over all metrics dataframe')\n",
    "    allSpecMatches                  = pd.concat([allSpecMatches,  matchesSpecies])\n",
    "    allLocMatches                   = pd.concat([allLocMatches,   matchesLocations])\n",
    "    allRelAnnotated                 = pd.concat([allRelAnnotated, relationsAnnotated])\n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Reset indexes\n",
    "    print('COMMAND: Reset indexes')\n",
    "    allSpecMatches.reset_index (drop=True, inplace=True)\n",
    "    allLocMatches.reset_index  (drop=True, inplace=True)\n",
    "    allRelAnnotated.reset_index(drop=True, inplace=True)  \n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Start identifying relationships\n",
    "    print('COMMAND: Merge species and locations to possible relationships')\n",
    "    possibleRelationships = pd.merge(speciesAnnotated.assign(key   = 0), \n",
    "                                     locationsAnnotated.assign(key = 0),\n",
    "                                     on       = ['key','Document'], \n",
    "                                     suffixes = ('_Species', \n",
    "                                                 '_Location')).drop('key', axis = 1)\n",
    "\n",
    "    possibleRelationships.rename(columns = {'Text_Species' :'Species', \n",
    "                                            'Text_Location':'Location',\n",
    "                                            'AnnTxtFile_Species':'AnnTxtFile'}, \n",
    "                                 inplace = True) \n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Create a dataframe of the actual relationships in the file\n",
    "    print('COMMAND: Create dataframe of actual relationships')\n",
    "    oneDocRelationships                     = pd.merge(possibleRelationships, relationsAnnotated, how = 'left')\n",
    "    oneDocRelationships.Tagged_Relationship.fillna(0, inplace=True)\n",
    "    oneDocRelationships.Tagged_Relationship = oneDocRelationships.Tagged_Relationship.astype('int8')\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "    # Create the out folder if it's not there\n",
    "    outFolder = 'Metrics\\\\Relationships\\\\DocumentSpecific\\\\' + folder.split('\\\\')[-1]\n",
    "    if not os.path.exists(outFolder):\n",
    "        os.makedirs(outFolder)   \n",
    "    \n",
    "    # Out the single documents relationships\n",
    "    print('COMMAND: Out oneDocRelationships.csv')\n",
    "    oneDocRelationships.to_csv(outFolder + '\\\\' + fileName + '.csv')\n",
    "    print('RESULT:  OK')\n",
    "\n",
    "    # Concat bot down to all possible relationships\n",
    "    print('COMMAND: Concat all relationship dataframes')\n",
    "    allPossibleRelationships = pd.concat([allPossibleRelationships, oneDocRelationships])\n",
    "    allPossibleRelationships.reset_index(drop=True, inplace=True)\n",
    "    print('RESULT:  OK')\n",
    "    \n",
    "# Print to allRelationships CSVs\n",
    "print('COMMAND: Out allPossibleRelationships.csv')\n",
    "allPossibleRelationships.to_csv('Metrics\\\\Relationships\\\\allPossible.csv')\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Clear output on success\n",
    "clear_output(wait = True)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** Generates the metrics from all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND: Get the species matches\n"
     ]
    }
   ],
   "source": [
    "# This will also clear the output on success\n",
    "\n",
    "# Get species matches\n",
    "print('COMMAND: Get the species matches' )\n",
    "speciesMetrics = getSpeciesMetrics(allSpecMatches)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get location matches\n",
    "print('COMMAND: Get the location matches')\n",
    "locationMetrics = getLocationsMetrics(allLocMatches)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Format allLocMatches\n",
    "asStrings = ['ExtractedLocation', \n",
    "             'Original Tokenised Text',\n",
    "             'Sentence',\n",
    "             'TaggedLocation', \n",
    "             'Document']\n",
    "\n",
    "asInts    = ['Start_Token',\n",
    "             'End_Token', \n",
    "             'Start_Index', \n",
    "             'End_Index']\n",
    "\n",
    "asBool    = ['NZGazAnt',\n",
    "             'NZGaz', \n",
    "             'ScarNZ', \n",
    "             'ScarGlobal', \n",
    "             'GeoNamesNZ', \n",
    "             'GeoNamesAnt',\n",
    "             'GeoNames',\n",
    "             'exactMatch', \n",
    "             'Close_Match_NZGazAnt', \n",
    "             'Close_Match_NZGaz',\n",
    "             'Close_Match_ScarNZ', \n",
    "             'Close_Match_ScarGlobal',\n",
    "             'Close_Match_GeoNamesNZ', \n",
    "             'Close_Match_GeoNamesAnt', \n",
    "             'Close_Match',\n",
    "             'PartialMatch_NZGazAnt', \n",
    "             'PartialMatch_NZGaz', \n",
    "             'PartialMatch_ScarNZ',\n",
    "             'PartialMatch_ScarGlobal', \n",
    "             'PartialMatch_GeoNamesNZ',\n",
    "             'PartialMatch_GeoNamesAnt', \n",
    "             'PartialMatch', \n",
    "             'inNZ', \n",
    "             'inAntarctica',\n",
    "             'Found']\n",
    "\n",
    "print('COMMAND: Format allLocMatches dataframe')\n",
    "for col in asStrings:\n",
    "    allLocMatches[col] = allLocMatches[col].astype(str)\n",
    "    \n",
    "for col in asBool:\n",
    "    allLocMatches[col] = allLocMatches[col].astype(bool)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get token numbers for spcecies matches\n",
    "print('COMMAND: Get token numbers')\n",
    "for row in allSpecMatches[allSpecMatches['Found_as'].isna()].index:\n",
    "    allSpecMatches.at[row, 'Found_as']    = allSpecMatches.loc[row].Text\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Get unmatched locations\n",
    "print('COMMAND: Get unmatched locations')\n",
    "unmatchedLocations = allLocMatches[allLocMatches.ExtractedLocation == 'nan']\n",
    "print(\"RESULT:  OK\")\n",
    "\n",
    "# Load gazetters\n",
    "print('COMMAND: Load gazetters')\n",
    "nzGaz            = list(np.load('NPYs\\\\nzgazetteer.npy',      allow_pickle=True))\n",
    "print('         nzgaz            OK')\n",
    "nzGazAntartica   = list(np.load('NPYs\\\\nzgazAntartica.npy',   allow_pickle=True))\n",
    "print('         nzgazAntartica   OK')\n",
    "scarGlobalNames  = list(np.load('NPYs\\\\SCARGlobalnames.npy',  allow_pickle=True))\n",
    "print('         SCARGlobalnames  OK')\n",
    "scarNzNames      = list(np.load('NPYs\\\\SCARNZnames.npy',      allow_pickle=True))\n",
    "print('         SCARNZnames      OK')\n",
    "geoNamesAnt      = list(np.load('NPYs\\\\GeoNamesAnt.npy',      allow_pickle=True))\n",
    "print('         GeoNamesAnt      OK')\n",
    "geoNamesNZ       = list(np.load('NPYs\\\\GeoNamesNZ.npy',       allow_pickle=True))\n",
    "print('         GeoNamesNZ       OK')\n",
    "geoNamesFiltered = list(np.load('NPYs\\\\GeoNamesFiltered.npy', allow_pickle=True))\n",
    "print('         GeoNamesFiltered OK')\n",
    "print('         Create list arg for function calls')\n",
    "gazetters       = [nzGaz, nzGazAntartica, scarGlobalNames, scarNzNames, geoNamesAnt, geoNamesNZ, geoNamesFiltered]\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Filter unmatched locations to actualy unmatched locations\n",
    "print('COMMAND: Apply unmatched locations')\n",
    "unmatchedLocations = populate_taggedlocations_df(unmatchedLocations, gazetters, filePairs) \n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Apply to matched locations\n",
    "print('COMMAND: Apply to as filter all matched locations dataframe')\n",
    "allLocMatches.update(unmatchedLocations, overwrite=True)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Convert columns to typeof Object\n",
    "print('COMMAND: Convert columns')\n",
    "for col in asBool:\n",
    "    unmatchedLocations[col] = unmatchedLocations[col].astype(object)    \n",
    "    allLocMatches[col]      = allLocMatches[col].astype(object)\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Create tagged location dataframe\n",
    "print('COMMAND: Create tagged location dataframe from location matched')\n",
    "TaggedLocations = allLocMatches[allLocMatches.TaggedLocation.notna()]\n",
    "print('RESULT:  OK')\n",
    "\n",
    "# Clear output on success\n",
    "clear_output(wait = True)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data frame has all the extracted species matched to all the locations in the annotated data sets\n",
    "allSpecMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe has all the extracted locations matched to all the locations in the annotated data sets\n",
    "allLocMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data frame has all the relationships found in the manually annotated pdfs\n",
    "allRelAnnotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the CSV of the relationships in the last pdf to be accessed\n",
    "oneDocRelationships.loc[oneDocRelationships['Tagged_Relationship'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows all the possible annotated relationships\n",
    "allPossibleRelationships.loc[allPossibleRelationships['Tagged_Relationship'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the metrics for species\n",
    "speciesMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the metrics for locations\n",
    "locationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the unmatched locations\n",
    "unmatchedLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the found locations that matched a tagged location\n",
    "TaggedLocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:36:49.613714Z",
     "start_time": "2021-02-28T04:36:49.610118Z"
    }
   },
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1350px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
