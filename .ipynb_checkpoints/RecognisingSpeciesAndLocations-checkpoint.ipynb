{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognising Species and Locations from Extracted Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-10T22:38:44.950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; } p, ul {max-width:␣\n",
       ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
       ",→display: flex; justify-content: center;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Markdown as md\n",
    "display(HTML(\"\"\"<style>.container { width:80% !important; } p, ul {max-width:␣\n",
    ",→40em;} .rendered_html table { margin-left: 0; } .output_subarea.output_png {␣\n",
    ",→display: flex; justify-content: center;}</style>\"\"\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import difflib\n",
    "from   spacy         import displacy\n",
    "from   collections   import Counter\n",
    "from   spacy.matcher import Matcher\n",
    "\n",
    "# Spacy Small\n",
    "import en_core_web_sm\n",
    "nlp =  spacy.load('en_core_web_sm')\n",
    "\n",
    "# Spacy Medium\n",
    "import en_core_web_md\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "# Spacey Large\n",
    "import en_core_web_lg\n",
    "nlp_lg = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T08:20:08.588486Z",
     "start_time": "2021-02-26T08:20:08.525590Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_locations(doc):\n",
    "    places = []\n",
    "    for x in doc.ents:\n",
    "        if x.label_ == 'LOC':\n",
    "            if doc[x.start].text == 'the' or doc[x.start].text == 'The':\n",
    "                places.append([x.text[4:], x.text, x.start+1, x.end, doc[x.start+1].idx, doc[x.end-1].idx + len(doc[x.end-1]), x.sent])\n",
    "            elif doc[x.start].text== 'Mt':\n",
    "                places.append(['Mount' + x.text[2:], x.text, x.start+1, x.end, doc[x.start].idx, doc[x.end-1].idx + len(doc[x.end-1]), x.sent])\n",
    "            elif doc[x.start].text== 'Mt.':\n",
    "                places.append(['Mount' + x.text[3:], x.text, x.start+1, x.end, doc[x.start].idx, doc[x.end-1].idx + len(doc[x.end-1]), x.sent])              \n",
    "            else:\n",
    "                places.append([x.text, x. text, x.start, x.end, doc[x.start].idx, doc[x.end-1].idx + len(doc[x.end-1]), x.sent])\n",
    "    \n",
    "    df = pd.DataFrame(places,columns=['Location', 'Original Tokenised Text','Start_Token', 'End_Token','Start_Index','End_Index','Sentence']\n",
    "             )\n",
    "    # Check possible locations in gazetteers for exact matches\n",
    "    df['NZGazAnt']     = [True if X in nzgazAntartica else False for X in df.Location ]\n",
    "    df['NZGaz']        = [True if (df['NZGazAnt'] is True or X in nzgaz) else False for X in df.Location ]\n",
    "    df['ScarNZ']       = [True if X in SCARNZnames else False for X in df.Location ]\n",
    "    df['ScarGlobal']   = [True if X in SCARGlobalnames else False for X in df.Location]\n",
    "    df['GeoNamesNZ']   = [True if X in GeoNamesNZ else False for X in df.Location]\n",
    "    df['GeoNamesAnt']  = [True if X in GeoNamesAnt else False for X in df.Location]\n",
    "    df['GeoNames']     = [True if (df['GeoNamesNZ'] is True or df['GeoNamesAnt'] is True or X in GeoNamesFiltered) else False for X in df.Location ]\n",
    "    df['inNZ']         = df.NZGaz      | df.NZGazAnt   | df.GeoNamesNZ\n",
    "    df['inAntarctica'] = df.ScarGlobal | df.NZGazAnt   | df.GeoNamesAnt\n",
    "    df['exactMatch']   = df.NZGaz      | df.ScarGlobal | df.GeoNames\n",
    "    \n",
    "    \n",
    "    # filter locations by those not found in Antarctica or New Zealand gazeteers \n",
    "    df2                = df[(df['inAntarctica'] == False) & (df['inNZ'] == False)].copy()\n",
    "    \n",
    "\n",
    "    # look through these remaining locations (including those found only in GeoNames) for close matches \n",
    "    # eg. McMurdo Dry Valley v McMurdo Dry Valley or for partial matches eg. Ross Sea Region == Ross Sea \n",
    "   \n",
    "    df2['Close_Match_NZGazAnt']    =  [difflib.get_close_matches(X, nzgazAntartica, cutoff=0.9)[0]\n",
    "                                                      if    len(difflib.get_close_matches(X, nzgazAntartica, cutoff=0.9))>0\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']] \n",
    "    \n",
    "    df2['Close_Match_NZGaz']       =  [difflib.get_close_matches(X, nzgaz, cutoff=0.9)[0]\n",
    "                                                      if    len(difflib.get_close_matches(X, nzgaz, cutoff=0.9))>0\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']] \n",
    "    \n",
    "    df2['Close_Match_ScarNZ']      =  [difflib.get_close_matches(X, SCARNZnames, cutoff=0.9)[0] \n",
    "                                                      if    len(difflib.get_close_matches(X, SCARNZnames, cutoff=0.9))>0\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']] \n",
    "    \n",
    "    df2['Close_Match_ScarGlobal']  =  [difflib.get_close_matches(X, SCARGlobalnames, cutoff=0.9)[0]\n",
    "                                                      if   (X !='Antarctic' and len(difflib.get_close_matches(X, SCARGlobalnames, cutoff=0.9))>0)\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']] \n",
    "    \n",
    "    df2['Close_Match_GeoNamesNZ']  = [difflib.get_close_matches(X, GeoNamesNZ, cutoff=0.9)[0] \n",
    "                                                      if    len(difflib.get_close_matches(X, GeoNamesNZ, cutoff=0.9))>0\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']] \n",
    "    \n",
    "    df2['Close_Match_GeoNamesAnt'] =  [difflib.get_close_matches(X, GeoNamesAnt, cutoff=0.9)[0]\n",
    "                                                      if   (X !='Antarctic' and len(difflib.get_close_matches(X, GeoNamesAnt, cutoff=0.9))>0)\n",
    "                                                      else  np.nan \n",
    "                                                      for   X in df2['Location']]\n",
    "    \n",
    "    df2['Close_Match']             = (df2.Close_Match_NZGazAnt.notna() \n",
    "                                   | df2.Close_Match_NZGaz.notna() \n",
    "                                   | df2.Close_Match_ScarNZ.notna() \n",
    "                                   | df2.Close_Match_ScarGlobal.notna() \n",
    "                                   | df2.Close_Match_GeoNamesAnt.notna())\n",
    "    \n",
    "    df2['PartialMatch_NZGazAnt']    = getBiggestSubStringMatch(doc, df2, nzgazAntartica)\n",
    "    df2['PartialMatch_NZGaz']       = getBiggestSubStringMatch(doc, df2, nzgaz)\n",
    "    df2['PartialMatch_ScarNZ']      = getBiggestSubStringMatch(doc, df2, SCARNZnames)\n",
    "    df2['PartialMatch_ScarGlobal']  = getBiggestSubStringMatch(doc, df2, SCARGlobalnames)\n",
    "    df2['PartialMatch_GeoNamesNZ']  = getBiggestSubStringMatch(doc, df2, GeoNamesNZ)\n",
    "    df2['PartialMatch_GeoNamesAnt'] = getBiggestSubStringMatch(doc, df2, GeoNamesAnt)\n",
    "    df2['PartialMatch']             = (df2['PartialMatch_NZGazAnt'].notna() \n",
    "                                     | df2['PartialMatch_NZGaz'].notna() \n",
    "                                     | df2['PartialMatch_ScarNZ'].notna() \n",
    "                                     | df2['PartialMatch_ScarGlobal'].notna() \n",
    "                                     | df2['PartialMatch_GeoNamesAnt'].notna())\n",
    "    \n",
    "    # merge the filtered dataframe (now with close and partial matches) with the unfiltered dataframe\n",
    "    df_unified = pd.merge(df, df2, how = 'left')\n",
    "    \n",
    "    # redo the inNZ,inAntarctica and Found columns to include close and partial matches\n",
    "    df_unified.drop(columns    = ['inNZ', 'inAntarctica'], inplace = True)\n",
    "    \n",
    "    df_unified['inNZ']         = (df_unified.NZGaz \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesNZ \n",
    "                                | df_unified.Close_Match_NZGaz \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesNZ \n",
    "                                | df_unified.PartialMatch_NZGaz \n",
    "                                | df_unified.PartialMatch_NZGazAnt\n",
    "                                | df_unified.PartialMatch_GeoNamesNZ)\n",
    "    \n",
    "    df_unified['inAntarctica'] = (df_unified.ScarGlobal \n",
    "                                | df_unified.NZGazAnt \n",
    "                                | df_unified.GeoNamesAnt \n",
    "                                | df_unified.Close_Match_ScarGlobal \n",
    "                                | df_unified.Close_Match_NZGazAnt \n",
    "                                | df_unified.Close_Match_GeoNamesAnt\n",
    "                                | df_unified.PartialMatch_ScarGlobal \n",
    "                                | df_unified.PartialMatch_NZGazAnt \n",
    "                                | df_unified.PartialMatch_GeoNamesAnt)\n",
    "    \n",
    "    df_unified['Found']        = df_unified.exactMatch | df_unified.Close_Match | df_unified.PartialMatch\n",
    "    \n",
    "    return df_unified[df_unified.Found == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T04:29:48.477852Z",
     "start_time": "2021-02-26T04:29:48.427765Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBiggestSubStringMatch(document, df, gazetteer):\n",
    "    matches = []\n",
    "    for row in df.index:\n",
    "        fullStart = df.loc[row,'Start_Token']\n",
    "        fullEnd   = df.loc[row,'End_Token']\n",
    "        subLength = fullEnd-fullStart-1\n",
    "        Found     = False\n",
    "        \n",
    "        while subLength >1:\n",
    "            subStart = fullStart\n",
    "            subEnd   = fullStart + subLength\n",
    "            \n",
    "            while subEnd <= fullEnd:\n",
    "                subString = document[subStart:subEnd].text\n",
    "                if subString in gazetteer:\n",
    "                    matches.append(subString)\n",
    "                    Found = True,\n",
    "                    break\n",
    "                subStart += 1\n",
    "                subEnd   += 1\n",
    "            if Found:\n",
    "                    break\n",
    "            else:\n",
    "                subLength -= 1\n",
    "        if Found == False:\n",
    "            matches.append(np.nan)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T01:27:45.101155Z",
     "start_time": "2021-02-27T01:27:43.948481Z"
    }
   },
   "outputs": [],
   "source": [
    "def findSpecies(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Dictionary approach.\n",
    "    \n",
    "    This code runs through token by token looking to match Genus + species, \n",
    "    Genus + species + species, G. species etc based on words found in the genus \n",
    "    and species columns of the supplied list of species.\n",
    "    \n",
    "    Also finds examples where a known genus is followed by an unrecognised species \n",
    "    if that example also appears in an abbreviated form elsewhere. \n",
    "    \"\"\"\n",
    "\n",
    "    foundSpeciesList   = []\n",
    "    foundSpecies       = ''\n",
    "    foundSpeciestokens = []\n",
    "    foundSpeciesDict   = {}\n",
    "    possiblesNames     = []\n",
    "    possiblesTokens    = []\n",
    "\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        if str(token) in uniqueGenus:\n",
    "            foundSpeciestokens.append(token)\n",
    "            foundSpecies = foundSpecies + str(token)\n",
    "            j            = 1\n",
    "            onlyGenus    = True\n",
    "            \n",
    "            while (doc[token.i + j].text in uniqueSpecies):\n",
    "                foundSpecies += ' ' + str(doc[token.i + j])\n",
    "                foundSpeciestokens.append(doc[token.i + j])\n",
    "                onlyGenus = False\n",
    "                j        += 1\n",
    "                \n",
    "            key = ''\n",
    "            \n",
    "            if onlyGenus == False:\n",
    "                for tkn in foundSpeciestokens[:-1]:\n",
    "                    key += tkn.text[0] + '. '\n",
    "                    \n",
    "                key += foundSpeciestokens[-1].text\n",
    "                if key not in foundSpeciesDict:\n",
    "                    foundSpeciesDict[key] = [foundSpecies, True]\n",
    "\n",
    "                foundSpeciesList.append([foundSpecies, \n",
    "                                         foundSpecies, \n",
    "                                         True, \n",
    "                                         token.i, \n",
    "                                         token.i + j, \n",
    "                                         token.idx, \n",
    "                                         token.sent])\n",
    "          \n",
    "            else:\n",
    "                if doc[token.i + 1].is_alpha:\n",
    "                    candidate = token.text + ' ' + doc[token.i+1].text\n",
    "                    possiblesNames.append(candidate)\n",
    "                    possiblesTokens.append((token, doc[token.i+1]))\n",
    "\n",
    "                else:\n",
    "                    foundSpeciesList.append([foundSpecies, \n",
    "                                             foundSpecies, \n",
    "                                             True, \n",
    "                                             token.i, \n",
    "                                             token.i + 1, \n",
    "                                             token.idx, \n",
    "                                             token.sent])\n",
    "\n",
    "            foundSpeciestokens = []\n",
    "            foundSpecies       = ''\n",
    "\n",
    "        elif token.shape_ == 'X.':\n",
    "            foundSpeciestokens.append(token)\n",
    "            foundSpeciestokens = []\n",
    "            foundSpecies       = foundSpecies + str(token)\n",
    "            j                  = 1\n",
    "            \n",
    "            while ((doc[token.i+j].is_alpha and doc[token.i + j].text in uniqueSpecies) or doc[token.i+j].shape_ == 'x.'):\n",
    "                foundSpeciestokens.append(doc[token.i + j])\n",
    "                foundSpecies += ' ' + str(doc[token.i + j])\n",
    "                j            += 1\n",
    "                \n",
    "            if doc[token.i + j - 1].text in uniqueSpecies:\n",
    "                if foundSpecies in foundSpeciesDict:\n",
    "                    foundSpeciesList.append([foundSpecies, \n",
    "                                             foundSpeciesDict.get(foundSpecies)[0], \n",
    "                                             foundSpeciesDict.get(foundSpecies)[1], \n",
    "                                             token.i, \n",
    "                                             token.i + j, \n",
    "                                             token.idx, \n",
    "                                             token.sent])\n",
    "                    \n",
    "                else:\n",
    "                    # Build pattern\n",
    "                    pattern = []\n",
    "                    for tkn in foundSpeciestokens[1:-1]:\n",
    "                        pattern.append([{\"SHAPE\"   : \"xxxx\"}, \n",
    "                                        {\"TEXT\"    : tkn[0].text[0]}]) \n",
    "                    pattern.append({\"TEXT\": foundSpeciestokens[-1].text})  \n",
    "                    \n",
    "                    # Create matcher\n",
    "                    matcher = Matcher(nlp.vocab)\n",
    "                    matcher.add(\"Species_Pattern\", [pattern])\n",
    "                    matches = matcher(doc)\n",
    "                    \n",
    "                    # Deal with matches\n",
    "                    if len(matches) > 0:\n",
    "                        start        = matches[0][1]\n",
    "                        end          = matches[0][2]\n",
    "                        matched_span = doc[start:end]\n",
    "                        inList       = False\n",
    "                        \n",
    "                        if doc[start].text in uniqueGenus:\n",
    "                            inList = True\n",
    "\n",
    "                        foundSpeciesList.append([foundSpecies, \n",
    "                                                 matched_span.text, \n",
    "                                                 inList, token.i, \n",
    "                                                 token.i + j, \n",
    "                                                 token.idx, \n",
    "                                                 token.sent])\n",
    "\n",
    "                        foundSpeciesDict[foundSpecies] = [matched_span.text, inList]\n",
    "\n",
    "                        if inList == False:\n",
    "                            for match_id, start, end in matches:\n",
    "                                matched_span = doc[start:end]\n",
    "                                foundSpeciesList.append([matched_span.text, \n",
    "                                                         matched_span.text, \n",
    "                                                         inList, \n",
    "                                                         start,\n",
    "                                                         end, \n",
    "                                                         doc[start].idx, \n",
    "                                                         doc[start].sent])\n",
    "            foundSpecies       = ''\n",
    "            foundSpeciestokens = []\n",
    "                \n",
    "    for tkns in possiblesTokens:\n",
    "        # Generate possible patterns\n",
    "        shortPattern = [{\"SHAPE\"   : \"X.\"}, \n",
    "                        {\"TEXT\"    : tkns[0].text[0]},\n",
    "                        {\"TEXT\"    : doc[tkns[0].i+1].text}]\n",
    "        \n",
    "        fullPattern = [{\"SHAPE\" : \"Xxxxx\"},\n",
    "                       {\"TEXT\"  : tkns[0].text},\n",
    "                       {\"TEXT\"  : doc[tkns[0].i+1].text}]\n",
    "        \n",
    "        # Create matcher\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        matcher.add(\"abbreviated_Species_Pattern\", [shortPattern])\n",
    "        matches = matcher(doc)\n",
    "        \n",
    "        # Deal with matches\n",
    "        if len(matches) > 0:\n",
    "            matcher.add(\"full_Species_Pattern\", [fullPattern])   \n",
    "            matches = matcher(doc)\n",
    "            for match_id, start, end in matches:\n",
    "                matched_span = doc[start:end]\n",
    "                foundSpeciesList.append([matched_span.text, \n",
    "                                         tkns[0].text + ' '+ tkns[1].text, \n",
    "                                         False, \n",
    "                                         start, \n",
    "                                         end, \n",
    "                                         doc[start].idx, \n",
    "                                         doc[start].sent])\n",
    "\n",
    "        else:\n",
    "            foundSpeciesList.append([tkns[0].text, \n",
    "                                     tkns[0].text, \n",
    "                                     False, \n",
    "                                     tkns[0].i, \n",
    "                                     tkns[0].i + 1, \n",
    "                                     doc[tkns[0].i].idx, \n",
    "                                     doc[tkns[0].i].sent])\n",
    "\n",
    "\n",
    "\n",
    "    FoundSpeciesDF = pd.DataFrame(foundSpeciesList,columns=['Found as', \n",
    "                                                            'Full name', \n",
    "                                                            'InDefinitiveList', \n",
    "                                                            'Start', \n",
    "                                                            'End',\n",
    "                                                            'PositionOfFirstToken',\n",
    "                                                            'Sentence'])\n",
    "    \n",
    "    FoundSpeciesDF.sort_values('Start', inplace = True)\n",
    "    FoundSpeciesDF.reset_index(inplace = True,drop = True)\n",
    "    FoundSpeciesDF['InReferenceList']  = [True if x in uniqueSpeciesStrings else False for x in FoundSpeciesDF['Full name']]\n",
    "    FoundSpeciesDF.drop_duplicates(keep='first', inplace=True)\n",
    "    return FoundSpeciesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T00:02:39.079093Z",
     "start_time": "2021-02-11T00:02:39.072690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load gazetters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gorilla Rig\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (28,29,32,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save gazetters as NPYs\n",
      "Load species list\n",
      "Parse species list\n",
      "Set source and dest. folders\n",
      "Iterate folders in source folder\n",
      "\n",
      "Check destination folder exits\n",
      "Dest: ExtractedLSMatrices\\\n",
      "Dest: ExtractedSpecies\\\n",
      "Dest: ExtractedLocations\\\n",
      "Iterate files\n",
      "\n",
      "Check destination folder exits\n",
      "Dest: ExtractedLSMatrices\\OldSet\n",
      "Dest: ExtractedSpecies\\OldSet\n",
      "Dest: ExtractedLocations\\OldSet\n",
      "Iterate files\n",
      "\n",
      "File: Archer2017_Article_EndolithicMicrobialDiversityIn.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n",
      "\n",
      "File: fmicb_10_01018.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n",
      "\n",
      "File: Fraser2018_Article_EvidenceOfPlantAndAnimalCommun.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n",
      "\n",
      "File: s42003_018_0260_y.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n",
      "\n",
      "File: source.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n",
      "\n",
      "File: summer_activity_patterns_for_mosses_and_lichens_in_maritime_antarctica.txt.ann.txt\n",
      "Read text to memory\n",
      "Tokenise doc\n",
      "Extract species\n",
      "Extract location\n",
      "Out to CSV\n"
     ]
    }
   ],
   "source": [
    "# Change the SpaCY model here\n",
    "# 1 small\n",
    "# 2 medium\n",
    "# 3 large\n",
    "NLPmodel         = 1\n",
    "\n",
    "# Start by importing location gazetters\n",
    "print('Load gazetters')\n",
    "nzgaz            = pd.read_csv(\"SpreadSheets/Jamies Original/gaz_names.csv\")\n",
    "nzgazAntartica   = nzgaz[nzgaz.land_district.isna()].name.unique()\n",
    "nzgaz            = nzgaz.name.unique()\n",
    "\n",
    "SCARGlobal       = pd.read_csv(\"SpreadSheets/Jamies Original/SCAR_CGA_PLACE_NAMES_GLOBAL_SIMPLIFIED.csv\")\n",
    "SCARGlobalnames  = SCARGlobal.place_name_mapping.unique()\n",
    "del SCARGlobal\n",
    "\n",
    "SCARNZ           = pd.read_csv(\"SpreadSheets/Jamies Original/SCAR_CGA_PLACE_NAMES_NZ_SIMPLIFIED.csv\")\n",
    "SCARNZnames      = SCARNZ.place_name_mapping.unique()\n",
    "del SCARNZ\n",
    "\n",
    "GeoNames         = pd.read_csv('SpreadSheets/Jamies Original/GeoNamesAnt.csv', index_col=0)\n",
    "GeoNamesUnique   = GeoNames.name.unique()\n",
    "GeoNamesAnt      = GeoNames[GeoNames['country code']=='AQ'].name.unique()\n",
    "GeoNamesNZ       = GeoNames[GeoNames['country code']=='NZ'].name.unique()\n",
    "dropWords        = ['Inner', 'Harbour', 'Lake', 'Fig', 'Valleys', 'Soil', \n",
    "                    'Lakes', 'South', 'North', 'Inner', 'Mount', 'Frozen', 'Oceans',\n",
    "                    'Upper', 'Contour']\n",
    "GeoNamesFiltered = [place for place in GeoNamesUnique if place not in dropWords]\n",
    "del GeoNames\n",
    "\n",
    "# Save them as NPYs\n",
    "print('Save gazetters as NPYs')\n",
    "np.save('NPYs/nzgazetteer.npy', nzgaz)\n",
    "np.save('NPYs/nzgazAntartica.npy', nzgazAntartica)\n",
    "np.save('NPYs/SCARGlobalnames.npy', SCARGlobalnames)\n",
    "np.save('NPYs/SCARNZnames.npy', SCARNZnames)\n",
    "np.save('NPYs/GeoNamesAnt.npy', GeoNamesAnt)\n",
    "np.save('NPYs/GeoNamesNZ.npy', GeoNamesNZ)\n",
    "np.save('NPYs/GeoNamesFiltered.npy', GeoNamesFiltered)\n",
    "\n",
    "# Import the species reference list\n",
    "print('Load species list')\n",
    "SpeciesReferenceList                  = pd.read_excel('SpreadSheets/Jamies Original/Antarctic_Species_List.xlsx')\n",
    "SpeciesReferenceList['SpeciesString'] = SpeciesReferenceList[['GENUS', 'SPECIES']].agg(' '.join, axis = 1)\n",
    "\n",
    "print('Parse species list')\n",
    "uniqueGenus          = SpeciesReferenceList.GENUS.unique()\n",
    "uniqueSpecies        = SpeciesReferenceList.SPECIES.unique()\n",
    "uniqueSpeciesStrings = SpeciesReferenceList.SpeciesString.unique()\n",
    "\n",
    "# Set source and destination folders\n",
    "print('Set source and dest. folders')\n",
    "root_src_dir          = 'ExtractedText\\\\'\n",
    "root_dst_dir_matrix   = 'ExtractedLSMatrices\\\\'\n",
    "root_dst_dir_species  = 'ExtractedSpecies\\\\'\n",
    "root_dst_dir_location = 'ExtractedLocations\\\\'\n",
    "\n",
    "# Walk through source folder\n",
    "print('Iterate folders in source folder')\n",
    "for src_dir, dirs, files in os.walk(root_src_dir):\n",
    "    print('')\n",
    "    dst_dir_matrix = src_dir.replace(root_src_dir, root_dst_dir_matrix, 1)\n",
    "\n",
    "    # Check destination folder exists\n",
    "    print('Check destination folder exits')\n",
    "    print('Dest: ' + dst_dir_matrix)\n",
    "    if not os.path.exists(dst_dir_matrix):\n",
    "        os.makedirs(dst_dir_matrix)   \n",
    "\n",
    "    dst_dir_species = src_dir.replace(root_src_dir, root_dst_dir_species, 1)\n",
    "    print('Dest: ' + dst_dir_species)\n",
    "    if not os.path.exists(dst_dir_species):\n",
    "        os.makedirs(dst_dir_species)   \n",
    "        \n",
    "    dst_dir_location = src_dir.replace(root_src_dir, root_dst_dir_location, 1)\n",
    "    print('Dest: ' + dst_dir_location)\n",
    "    if not os.path.exists(dst_dir_location):\n",
    "        os.makedirs(dst_dir_location)  \n",
    "        \n",
    "    # Iterate through the folders files\n",
    "    print('Iterate files')\n",
    "    for file in files:\n",
    "        print('')\n",
    "        \n",
    "        # Create file paths\n",
    "        # Species.csv\n",
    "        print('File: ' + file)\n",
    "        src_file         = os.path.join(src_dir, file)\n",
    "\n",
    "        # Species\n",
    "        dst_file_species = os.path.join(dst_dir_species, file)\n",
    "        if os.path.exists(dst_file_species):\n",
    "            # in case of the src and dst are the same file\n",
    "            if os.path.samefile(src_file, dst_file_species):\n",
    "                continue\n",
    "            os.remove(dst_file_species)   \n",
    "        \n",
    "        # Location.csv\n",
    "        dst_file_location = os.path.join(dst_dir_location, file)\n",
    "        if os.path.exists(dst_file_location):\n",
    "            # in case of the src and dst are the same file\n",
    "            if os.path.samefile(src_file, dst_file_location):\n",
    "                continue\n",
    "            os.remove(dst_file_location)   \n",
    "\n",
    "        # Matrix.csv\n",
    "        dst_file_matrix = os.path.join(dst_dir_matrix, file)\n",
    "        if os.path.exists(dst_file_matrix):\n",
    "            # in case of the src and dst are the same file\n",
    "            if os.path.samefile(src_file, dst_file_matrix):\n",
    "                continue\n",
    "            os.remove(dst_file_matrix)   \n",
    "            \n",
    "        # Read the .txt to memory\n",
    "        print('Read text to memory')\n",
    "        file = open(src_file, encoding = \"utf8\")\n",
    "        doc  = file.read()\n",
    "        file.close()\n",
    "\n",
    "        # Tokenize the document\n",
    "        print('Tokenise doc')\n",
    "        if NLPmodel   == 1:\n",
    "            document = nlp(doc)\n",
    "        elif NLPmodel == 2:\n",
    "            document = nlp_md(doc)\n",
    "        elif NLPmodel == 3:\n",
    "            document = nlp_lg(doc)\n",
    "\n",
    "        # Extract the species \n",
    "        print('Extract species')\n",
    "        species          = findSpecies(document)\n",
    "\n",
    "        # And locations with filter\n",
    "        print ('Extract location')\n",
    "        locations        = find_locations(document)\n",
    "        locations        = locations[locations.Found == True] \n",
    "\n",
    "        # Merge the results\n",
    "        speciesXlocation = pd.merge(species.assign(key=0), \n",
    "                                    locations.assign(key=0), \n",
    "                                    on       = 'key', \n",
    "                                    suffixes = ('_species', \n",
    "                                                '_location')).drop('key', axis=1)\n",
    "\n",
    "        # Out to CSVs\n",
    "        print('Out to CSV') \n",
    "        \n",
    "        # Matrix\n",
    "        dst_file = dst_file_matrix.replace('.txt', '-SLMatrix.csv')\n",
    "        speciesXlocation.to_csv(dst_file)\n",
    "        \n",
    "        # Species\n",
    "        dst_file = dst_file_species.replace('.txt', '-Species.csv')\n",
    "        speciesXlocation.to_csv(dst_file)\n",
    "        \n",
    "        # Location\n",
    "        dst_file = dst_file_location.replace('.txt', '-Locations.csv')\n",
    "        speciesXlocation.to_csv(dst_file)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1350px",
    "right": "20px",
    "top": "118px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
